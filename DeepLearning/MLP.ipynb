{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = datasets.load_iris()\n",
    "# df = pd.DataFrame(df.data, columns=df.feature_names)\n",
    "# df.values[:, -1]\n",
    "X, y = df.data, df.target\n",
    "LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.9, 3.1, 4.9, 1.5], dtype=float32), 1.0)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IRISDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        df = datasets.load_iris()\n",
    "        self.X = df.data.astype(\"float32\")\n",
    "        self.y = df.target.astype(\"float32\")\n",
    "        # self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def get_splits(self, n_test=0.3):\n",
    "        test_size = round(len(self.X) * n_test)\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "test = IRISDataSet()\n",
    "x1, x2 = test.get_splits()\n",
    "x1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_output):\n",
    "        super().__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_inputs, n_hidden1)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        self.hidden3 = torch.nn.Linear(n_hidden2, n_output)\n",
    "        self.act3 = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    dataset = IRISDataSet()\n",
    "    train, test = dataset.get_splits()\n",
    "    train_dl = DataLoader(train, batch_size=2, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, model, loss, trainer, n_epochs):\n",
    "    criterion = loss\n",
    "    optimizer = trainer\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            y = y.long()\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            print(f\"epoch: {epoch}, batch: {i}, loss: {loss.data}.\")\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = [], []\n",
    "    for i, (x, y) in enumerate(test_dl):\n",
    "        y_hat = model(x)\n",
    "        y_hat = y_hat.detach().numpy()\n",
    "        y = y.numpy()\n",
    "        y_hat = argmax(y_hat)\n",
    "        y = y.reshape((len(y), 1))\n",
    "        y_hat = y_hat.reshape((len(y_hat), 1))\n",
    "        predictions.append(y_hat)\n",
    "        actuals.append(y)\n",
    "    predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 1\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = prepare_data()\n",
    "print(len(train_dl), len(test_dl))\n",
    "model = MLP(4, 5, 6, 3)\n",
    "loss = CrossEntropyLoss()\n",
    "trainer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 1\n",
      "epoch: 0, batch: 0, loss: 1.1068472862243652.\n",
      "epoch: 0, batch: 1, loss: 1.1036326885223389.\n",
      "epoch: 0, batch: 2, loss: 1.0889215469360352.\n",
      "epoch: 0, batch: 3, loss: 1.1161584854125977.\n",
      "epoch: 0, batch: 4, loss: 1.0901367664337158.\n",
      "epoch: 0, batch: 5, loss: 1.0891005992889404.\n",
      "epoch: 0, batch: 6, loss: 1.0871601104736328.\n",
      "epoch: 0, batch: 7, loss: 1.0976239442825317.\n",
      "epoch: 0, batch: 8, loss: 1.0843257904052734.\n",
      "epoch: 0, batch: 9, loss: 1.0985708236694336.\n",
      "epoch: 0, batch: 10, loss: 1.099625587463379.\n",
      "epoch: 0, batch: 11, loss: 1.0734305381774902.\n",
      "epoch: 0, batch: 12, loss: 1.0941680669784546.\n",
      "epoch: 0, batch: 13, loss: 1.0712535381317139.\n",
      "epoch: 0, batch: 14, loss: 1.0845389366149902.\n",
      "epoch: 0, batch: 15, loss: 1.0704131126403809.\n",
      "epoch: 0, batch: 16, loss: 1.095430850982666.\n",
      "epoch: 0, batch: 17, loss: 1.0795096158981323.\n",
      "epoch: 0, batch: 18, loss: 1.0879490375518799.\n",
      "epoch: 0, batch: 19, loss: 1.0819432735443115.\n",
      "epoch: 0, batch: 20, loss: 1.0833990573883057.\n",
      "epoch: 0, batch: 21, loss: 1.0966304540634155.\n",
      "epoch: 0, batch: 22, loss: 1.0827796459197998.\n",
      "epoch: 0, batch: 23, loss: 1.0614440441131592.\n",
      "epoch: 0, batch: 24, loss: 1.0888036489486694.\n",
      "epoch: 0, batch: 25, loss: 1.0863925218582153.\n",
      "epoch: 0, batch: 26, loss: 1.0786699056625366.\n",
      "epoch: 0, batch: 27, loss: 1.086545467376709.\n",
      "epoch: 0, batch: 28, loss: 1.072766661643982.\n",
      "epoch: 0, batch: 29, loss: 1.0743364095687866.\n",
      "epoch: 0, batch: 30, loss: 1.1017489433288574.\n",
      "epoch: 0, batch: 31, loss: 1.0790646076202393.\n",
      "epoch: 0, batch: 32, loss: 1.0804071426391602.\n",
      "epoch: 0, batch: 33, loss: 1.0745601654052734.\n",
      "epoch: 0, batch: 34, loss: 1.0765941143035889.\n",
      "epoch: 0, batch: 35, loss: 1.0850497484207153.\n",
      "epoch: 0, batch: 36, loss: 1.0817861557006836.\n",
      "epoch: 0, batch: 37, loss: 1.072888731956482.\n",
      "epoch: 0, batch: 38, loss: 1.0691790580749512.\n",
      "epoch: 0, batch: 39, loss: 1.0769239664077759.\n",
      "epoch: 0, batch: 40, loss: 1.0714067220687866.\n",
      "epoch: 0, batch: 41, loss: 1.074585199356079.\n",
      "epoch: 0, batch: 42, loss: 1.0711708068847656.\n",
      "epoch: 0, batch: 43, loss: 1.0732524394989014.\n",
      "epoch: 0, batch: 44, loss: 1.0677852630615234.\n",
      "epoch: 0, batch: 45, loss: 1.079671859741211.\n",
      "epoch: 0, batch: 46, loss: 1.0700361728668213.\n",
      "epoch: 0, batch: 47, loss: 1.070476770401001.\n",
      "epoch: 0, batch: 48, loss: 1.0752954483032227.\n",
      "epoch: 0, batch: 49, loss: 1.0803256034851074.\n",
      "epoch: 0, batch: 50, loss: 1.0714668035507202.\n",
      "epoch: 0, batch: 51, loss: 1.0752589702606201.\n",
      "epoch: 0, batch: 52, loss: 1.071375846862793.\n",
      "epoch: 1, batch: 0, loss: 1.0929661989212036.\n",
      "epoch: 1, batch: 1, loss: 1.0964670181274414.\n",
      "epoch: 1, batch: 2, loss: 1.0705811977386475.\n",
      "epoch: 1, batch: 3, loss: 1.0666418075561523.\n",
      "epoch: 1, batch: 4, loss: 1.0724048614501953.\n",
      "epoch: 1, batch: 5, loss: 1.0703933238983154.\n",
      "epoch: 1, batch: 6, loss: 1.069413661956787.\n",
      "epoch: 1, batch: 7, loss: 1.0766913890838623.\n",
      "epoch: 1, batch: 8, loss: 1.0695545673370361.\n",
      "epoch: 1, batch: 9, loss: 1.064914345741272.\n",
      "epoch: 1, batch: 10, loss: 1.07039213180542.\n",
      "epoch: 1, batch: 11, loss: 1.0662469863891602.\n",
      "epoch: 1, batch: 12, loss: 1.0772217512130737.\n",
      "epoch: 1, batch: 13, loss: 1.0759292840957642.\n",
      "epoch: 1, batch: 14, loss: 1.0717999935150146.\n",
      "epoch: 1, batch: 15, loss: 1.0851054191589355.\n",
      "epoch: 1, batch: 16, loss: 1.0647428035736084.\n",
      "epoch: 1, batch: 17, loss: 1.0725703239440918.\n",
      "epoch: 1, batch: 18, loss: 1.0733674764633179.\n",
      "epoch: 1, batch: 19, loss: 1.0675990581512451.\n",
      "epoch: 1, batch: 20, loss: 1.067894458770752.\n",
      "epoch: 1, batch: 21, loss: 1.073163390159607.\n",
      "epoch: 1, batch: 22, loss: 1.0600926876068115.\n",
      "epoch: 1, batch: 23, loss: 1.0741779804229736.\n",
      "epoch: 1, batch: 24, loss: 1.072035789489746.\n",
      "epoch: 1, batch: 25, loss: 1.0741429328918457.\n",
      "epoch: 1, batch: 26, loss: 1.0642368793487549.\n",
      "epoch: 1, batch: 27, loss: 1.0722968578338623.\n",
      "epoch: 1, batch: 28, loss: 1.0676121711730957.\n",
      "epoch: 1, batch: 29, loss: 1.0772303342819214.\n",
      "epoch: 1, batch: 30, loss: 1.0757384300231934.\n",
      "epoch: 1, batch: 31, loss: 1.0659934282302856.\n",
      "epoch: 1, batch: 32, loss: 1.0642542839050293.\n",
      "epoch: 1, batch: 33, loss: 1.0628583431243896.\n",
      "epoch: 1, batch: 34, loss: 1.0628271102905273.\n",
      "epoch: 1, batch: 35, loss: 1.0784797668457031.\n",
      "epoch: 1, batch: 36, loss: 1.0615720748901367.\n",
      "epoch: 1, batch: 37, loss: 1.0738070011138916.\n",
      "epoch: 1, batch: 38, loss: 1.0596187114715576.\n",
      "epoch: 1, batch: 39, loss: 1.0635478496551514.\n",
      "epoch: 1, batch: 40, loss: 1.0645824670791626.\n",
      "epoch: 1, batch: 41, loss: 1.062934398651123.\n",
      "epoch: 1, batch: 42, loss: 1.0598703622817993.\n",
      "epoch: 1, batch: 43, loss: 1.0724891424179077.\n",
      "epoch: 1, batch: 44, loss: 1.0572688579559326.\n",
      "epoch: 1, batch: 45, loss: 1.062814474105835.\n",
      "epoch: 1, batch: 46, loss: 1.0718704462051392.\n",
      "epoch: 1, batch: 47, loss: 1.0577552318572998.\n",
      "epoch: 1, batch: 48, loss: 1.0643107891082764.\n",
      "epoch: 1, batch: 49, loss: 1.0509850978851318.\n",
      "epoch: 1, batch: 50, loss: 1.0657285451889038.\n",
      "epoch: 1, batch: 51, loss: 1.0626074075698853.\n",
      "epoch: 1, batch: 52, loss: 1.0534394979476929.\n",
      "epoch: 2, batch: 0, loss: 1.0511821508407593.\n",
      "epoch: 2, batch: 1, loss: 1.0617197751998901.\n",
      "epoch: 2, batch: 2, loss: 1.0612709522247314.\n",
      "epoch: 2, batch: 3, loss: 1.0513919591903687.\n",
      "epoch: 2, batch: 4, loss: 1.0480797290802002.\n",
      "epoch: 2, batch: 5, loss: 1.0840487480163574.\n",
      "epoch: 2, batch: 6, loss: 1.0649549961090088.\n",
      "epoch: 2, batch: 7, loss: 1.0632154941558838.\n",
      "epoch: 2, batch: 8, loss: 1.057572603225708.\n",
      "epoch: 2, batch: 9, loss: 1.0610153675079346.\n",
      "epoch: 2, batch: 10, loss: 1.0541470050811768.\n",
      "epoch: 2, batch: 11, loss: 1.0648984909057617.\n",
      "epoch: 2, batch: 12, loss: 1.0594055652618408.\n",
      "epoch: 2, batch: 13, loss: 1.0604878664016724.\n",
      "epoch: 2, batch: 14, loss: 1.0557379722595215.\n",
      "epoch: 2, batch: 15, loss: 1.0576140880584717.\n",
      "epoch: 2, batch: 16, loss: 1.0514799356460571.\n",
      "epoch: 2, batch: 17, loss: 1.0625461339950562.\n",
      "epoch: 2, batch: 18, loss: 1.0559046268463135.\n",
      "epoch: 2, batch: 19, loss: 1.0583577156066895.\n",
      "epoch: 2, batch: 20, loss: 1.0555732250213623.\n",
      "epoch: 2, batch: 21, loss: 1.051478624343872.\n",
      "epoch: 2, batch: 22, loss: 1.0533852577209473.\n",
      "epoch: 2, batch: 23, loss: 1.0424871444702148.\n",
      "epoch: 2, batch: 24, loss: 1.065415382385254.\n",
      "epoch: 2, batch: 25, loss: 1.0646162033081055.\n",
      "epoch: 2, batch: 26, loss: 1.0608775615692139.\n",
      "epoch: 2, batch: 27, loss: 1.0373286008834839.\n",
      "epoch: 2, batch: 28, loss: 1.0405361652374268.\n",
      "epoch: 2, batch: 29, loss: 1.0559601783752441.\n",
      "epoch: 2, batch: 30, loss: 1.0549712181091309.\n",
      "epoch: 2, batch: 31, loss: 1.0414656400680542.\n",
      "epoch: 2, batch: 32, loss: 1.0505203008651733.\n",
      "epoch: 2, batch: 33, loss: 1.04902982711792.\n",
      "epoch: 2, batch: 34, loss: 1.050323247909546.\n",
      "epoch: 2, batch: 35, loss: 1.0376954078674316.\n",
      "epoch: 2, batch: 36, loss: 1.0429933071136475.\n",
      "epoch: 2, batch: 37, loss: 1.0530781745910645.\n",
      "epoch: 2, batch: 38, loss: 1.0564906597137451.\n",
      "epoch: 2, batch: 39, loss: 1.0666186809539795.\n",
      "epoch: 2, batch: 40, loss: 1.044179916381836.\n",
      "epoch: 2, batch: 41, loss: 1.0387475490570068.\n",
      "epoch: 2, batch: 42, loss: 1.0591644048690796.\n",
      "epoch: 2, batch: 43, loss: 1.030707597732544.\n",
      "epoch: 2, batch: 44, loss: 1.0373098850250244.\n",
      "epoch: 2, batch: 45, loss: 1.0543797016143799.\n",
      "epoch: 2, batch: 46, loss: 1.0141057968139648.\n",
      "epoch: 2, batch: 47, loss: 1.0294413566589355.\n",
      "epoch: 2, batch: 48, loss: 1.0522938966751099.\n",
      "epoch: 2, batch: 49, loss: 1.0441536903381348.\n",
      "epoch: 2, batch: 50, loss: 1.030916452407837.\n",
      "epoch: 2, batch: 51, loss: 1.0574183464050293.\n",
      "epoch: 2, batch: 52, loss: 1.0614054203033447.\n",
      "epoch: 3, batch: 0, loss: 1.0252536535263062.\n",
      "epoch: 3, batch: 1, loss: 1.0465227365493774.\n",
      "epoch: 3, batch: 2, loss: 1.0560619831085205.\n",
      "epoch: 3, batch: 3, loss: 1.0462417602539062.\n",
      "epoch: 3, batch: 4, loss: 1.0483160018920898.\n",
      "epoch: 3, batch: 5, loss: 1.0436348915100098.\n",
      "epoch: 3, batch: 6, loss: 1.0346213579177856.\n",
      "epoch: 3, batch: 7, loss: 1.0200421810150146.\n",
      "epoch: 3, batch: 8, loss: 1.0414140224456787.\n",
      "epoch: 3, batch: 9, loss: 1.0457160472869873.\n",
      "epoch: 3, batch: 10, loss: 1.0095436573028564.\n",
      "epoch: 3, batch: 11, loss: 1.0187736749649048.\n",
      "epoch: 3, batch: 12, loss: 1.0294570922851562.\n",
      "epoch: 3, batch: 13, loss: 1.033510446548462.\n",
      "epoch: 3, batch: 14, loss: 1.0477020740509033.\n",
      "epoch: 3, batch: 15, loss: 1.0188828706741333.\n",
      "epoch: 3, batch: 16, loss: 1.0535942316055298.\n",
      "epoch: 3, batch: 17, loss: 1.0460193157196045.\n",
      "epoch: 3, batch: 18, loss: 1.0217561721801758.\n",
      "epoch: 3, batch: 19, loss: 1.0124473571777344.\n",
      "epoch: 3, batch: 20, loss: 1.0327752828598022.\n",
      "epoch: 3, batch: 21, loss: 0.9756156206130981.\n",
      "epoch: 3, batch: 22, loss: 1.0407793521881104.\n",
      "epoch: 3, batch: 23, loss: 1.0240676403045654.\n",
      "epoch: 3, batch: 24, loss: 1.0064935684204102.\n",
      "epoch: 3, batch: 25, loss: 0.9697328805923462.\n",
      "epoch: 3, batch: 26, loss: 1.048210620880127.\n",
      "epoch: 3, batch: 27, loss: 1.0524837970733643.\n",
      "epoch: 3, batch: 28, loss: 1.047103762626648.\n",
      "epoch: 3, batch: 29, loss: 1.017512559890747.\n",
      "epoch: 3, batch: 30, loss: 0.9916774034500122.\n",
      "epoch: 3, batch: 31, loss: 0.9708864688873291.\n",
      "epoch: 3, batch: 32, loss: 1.059041976928711.\n",
      "epoch: 3, batch: 33, loss: 0.9373774528503418.\n",
      "epoch: 3, batch: 34, loss: 1.0384156703948975.\n",
      "epoch: 3, batch: 35, loss: 1.05466628074646.\n",
      "epoch: 3, batch: 36, loss: 1.0440340042114258.\n",
      "epoch: 3, batch: 37, loss: 0.9914379119873047.\n",
      "epoch: 3, batch: 38, loss: 0.9973986148834229.\n",
      "epoch: 3, batch: 39, loss: 1.051349401473999.\n",
      "epoch: 3, batch: 40, loss: 0.9802951812744141.\n",
      "epoch: 3, batch: 41, loss: 1.0519435405731201.\n",
      "epoch: 3, batch: 42, loss: 1.0350457429885864.\n",
      "epoch: 3, batch: 43, loss: 0.9979472756385803.\n",
      "epoch: 3, batch: 44, loss: 1.047680377960205.\n",
      "epoch: 3, batch: 45, loss: 1.0399298667907715.\n",
      "epoch: 3, batch: 46, loss: 1.0449576377868652.\n",
      "epoch: 3, batch: 47, loss: 1.037588119506836.\n",
      "epoch: 3, batch: 48, loss: 1.0394313335418701.\n",
      "epoch: 3, batch: 49, loss: 1.042123556137085.\n",
      "epoch: 3, batch: 50, loss: 1.0018720626831055.\n",
      "epoch: 3, batch: 51, loss: 1.065342903137207.\n",
      "epoch: 3, batch: 52, loss: 0.9360653758049011.\n",
      "epoch: 4, batch: 0, loss: 0.9812577962875366.\n",
      "epoch: 4, batch: 1, loss: 1.0417007207870483.\n",
      "epoch: 4, batch: 2, loss: 0.9796164035797119.\n",
      "epoch: 4, batch: 3, loss: 1.037308692932129.\n",
      "epoch: 4, batch: 4, loss: 0.985862135887146.\n",
      "epoch: 4, batch: 5, loss: 0.9498662948608398.\n",
      "epoch: 4, batch: 6, loss: 1.0388537645339966.\n",
      "epoch: 4, batch: 7, loss: 1.0313310623168945.\n",
      "epoch: 4, batch: 8, loss: 1.0091732740402222.\n",
      "epoch: 4, batch: 9, loss: 0.9880985021591187.\n",
      "epoch: 4, batch: 10, loss: 1.0321037769317627.\n",
      "epoch: 4, batch: 11, loss: 1.036409854888916.\n",
      "epoch: 4, batch: 12, loss: 1.039764642715454.\n",
      "epoch: 4, batch: 13, loss: 1.0394318103790283.\n",
      "epoch: 4, batch: 14, loss: 0.9914305210113525.\n",
      "epoch: 4, batch: 15, loss: 0.9901312589645386.\n",
      "epoch: 4, batch: 16, loss: 0.9815142154693604.\n",
      "epoch: 4, batch: 17, loss: 0.9896587133407593.\n",
      "epoch: 4, batch: 18, loss: 0.966417670249939.\n",
      "epoch: 4, batch: 19, loss: 0.9033562541007996.\n",
      "epoch: 4, batch: 20, loss: 1.0300462245941162.\n",
      "epoch: 4, batch: 21, loss: 0.9644809365272522.\n",
      "epoch: 4, batch: 22, loss: 0.9622215032577515.\n",
      "epoch: 4, batch: 23, loss: 1.0336037874221802.\n",
      "epoch: 4, batch: 24, loss: 1.0327972173690796.\n",
      "epoch: 4, batch: 25, loss: 1.0285195112228394.\n",
      "epoch: 4, batch: 26, loss: 0.8987751603126526.\n",
      "epoch: 4, batch: 27, loss: 1.0264642238616943.\n",
      "epoch: 4, batch: 28, loss: 0.953400194644928.\n",
      "epoch: 4, batch: 29, loss: 0.9562180042266846.\n",
      "epoch: 4, batch: 30, loss: 1.023641586303711.\n",
      "epoch: 4, batch: 31, loss: 1.0265374183654785.\n",
      "epoch: 4, batch: 32, loss: 1.0280725955963135.\n",
      "epoch: 4, batch: 33, loss: 0.9316211938858032.\n",
      "epoch: 4, batch: 34, loss: 1.030630350112915.\n",
      "epoch: 4, batch: 35, loss: 0.9613960981369019.\n",
      "epoch: 4, batch: 36, loss: 1.025050401687622.\n",
      "epoch: 4, batch: 37, loss: 0.9692345857620239.\n",
      "epoch: 4, batch: 38, loss: 1.026167392730713.\n",
      "epoch: 4, batch: 39, loss: 0.9642819166183472.\n",
      "epoch: 4, batch: 40, loss: 1.0421371459960938.\n",
      "epoch: 4, batch: 41, loss: 0.8691050410270691.\n",
      "epoch: 4, batch: 42, loss: 0.9333034753799438.\n",
      "epoch: 4, batch: 43, loss: 1.0345699787139893.\n",
      "epoch: 4, batch: 44, loss: 1.0305800437927246.\n",
      "epoch: 4, batch: 45, loss: 1.0218048095703125.\n",
      "epoch: 4, batch: 46, loss: 0.9442934989929199.\n",
      "epoch: 4, batch: 47, loss: 0.9325393438339233.\n",
      "epoch: 4, batch: 48, loss: 0.8459938764572144.\n",
      "epoch: 4, batch: 49, loss: 1.0174083709716797.\n",
      "epoch: 4, batch: 50, loss: 1.016690731048584.\n",
      "epoch: 4, batch: 51, loss: 0.9031851291656494.\n",
      "epoch: 4, batch: 52, loss: 1.0145641565322876.\n",
      "epoch: 5, batch: 0, loss: 0.9350979328155518.\n",
      "epoch: 5, batch: 1, loss: 0.9335684180259705.\n",
      "epoch: 5, batch: 2, loss: 1.0341501235961914.\n",
      "epoch: 5, batch: 3, loss: 1.0072319507598877.\n",
      "epoch: 5, batch: 4, loss: 0.9213168025016785.\n",
      "epoch: 5, batch: 5, loss: 0.9247390627861023.\n",
      "epoch: 5, batch: 6, loss: 0.9368519186973572.\n",
      "epoch: 5, batch: 7, loss: 1.0225987434387207.\n",
      "epoch: 5, batch: 8, loss: 0.8933703899383545.\n",
      "epoch: 5, batch: 9, loss: 0.9098126292228699.\n",
      "epoch: 5, batch: 10, loss: 1.0102368593215942.\n",
      "epoch: 5, batch: 11, loss: 0.9239611625671387.\n",
      "epoch: 5, batch: 12, loss: 0.8210150003433228.\n",
      "epoch: 5, batch: 13, loss: 0.9461838006973267.\n",
      "epoch: 5, batch: 14, loss: 0.8329293131828308.\n",
      "epoch: 5, batch: 15, loss: 0.8102583885192871.\n",
      "epoch: 5, batch: 16, loss: 0.9203348159790039.\n",
      "epoch: 5, batch: 17, loss: 0.8962129354476929.\n",
      "epoch: 5, batch: 18, loss: 1.0087218284606934.\n",
      "epoch: 5, batch: 19, loss: 1.0047187805175781.\n",
      "epoch: 5, batch: 20, loss: 0.9047472476959229.\n",
      "epoch: 5, batch: 21, loss: 0.9132542014122009.\n",
      "epoch: 5, batch: 22, loss: 0.8931916952133179.\n",
      "epoch: 5, batch: 23, loss: 1.012742519378662.\n",
      "epoch: 5, batch: 24, loss: 0.9171613454818726.\n",
      "epoch: 5, batch: 25, loss: 1.0038375854492188.\n",
      "epoch: 5, batch: 26, loss: 0.9067039489746094.\n",
      "epoch: 5, batch: 27, loss: 0.9091233015060425.\n",
      "epoch: 5, batch: 28, loss: 1.0147253274917603.\n",
      "epoch: 5, batch: 29, loss: 1.016626000404358.\n",
      "epoch: 5, batch: 30, loss: 1.0072354078292847.\n",
      "epoch: 5, batch: 31, loss: 0.7740423679351807.\n",
      "epoch: 5, batch: 32, loss: 0.9996967315673828.\n",
      "epoch: 5, batch: 33, loss: 1.0252898931503296.\n",
      "epoch: 5, batch: 34, loss: 1.01774263381958.\n",
      "epoch: 5, batch: 35, loss: 0.876444935798645.\n",
      "epoch: 5, batch: 36, loss: 1.0150837898254395.\n",
      "epoch: 5, batch: 37, loss: 1.0046521425247192.\n",
      "epoch: 5, batch: 38, loss: 0.9085302352905273.\n",
      "epoch: 5, batch: 39, loss: 1.0115776062011719.\n",
      "epoch: 5, batch: 40, loss: 0.9987350702285767.\n",
      "epoch: 5, batch: 41, loss: 0.999542772769928.\n",
      "epoch: 5, batch: 42, loss: 1.0028356313705444.\n",
      "epoch: 5, batch: 43, loss: 0.9978629946708679.\n",
      "epoch: 5, batch: 44, loss: 0.7644059062004089.\n",
      "epoch: 5, batch: 45, loss: 0.9918079376220703.\n",
      "epoch: 5, batch: 46, loss: 1.0002350807189941.\n",
      "epoch: 5, batch: 47, loss: 0.9976209402084351.\n",
      "epoch: 5, batch: 48, loss: 0.9942458271980286.\n",
      "epoch: 5, batch: 49, loss: 0.8792860507965088.\n",
      "epoch: 5, batch: 50, loss: 0.9891287088394165.\n",
      "epoch: 5, batch: 51, loss: 0.9930251836776733.\n",
      "epoch: 5, batch: 52, loss: 0.7143006324768066.\n",
      "epoch: 6, batch: 0, loss: 0.9895629286766052.\n",
      "epoch: 6, batch: 1, loss: 0.7530917525291443.\n",
      "epoch: 6, batch: 2, loss: 0.7789599895477295.\n",
      "epoch: 6, batch: 3, loss: 1.0004990100860596.\n",
      "epoch: 6, batch: 4, loss: 0.9877153635025024.\n",
      "epoch: 6, batch: 5, loss: 0.9953116178512573.\n",
      "epoch: 6, batch: 6, loss: 0.9769412875175476.\n",
      "epoch: 6, batch: 7, loss: 0.8716076016426086.\n",
      "epoch: 6, batch: 8, loss: 0.9870255589485168.\n",
      "epoch: 6, batch: 9, loss: 0.7056821584701538.\n",
      "epoch: 6, batch: 10, loss: 0.9908809065818787.\n",
      "epoch: 6, batch: 11, loss: 0.994320273399353.\n",
      "epoch: 6, batch: 12, loss: 0.9870016574859619.\n",
      "epoch: 6, batch: 13, loss: 0.9709513783454895.\n",
      "epoch: 6, batch: 14, loss: 0.8597676157951355.\n",
      "epoch: 6, batch: 15, loss: 0.8939617276191711.\n",
      "epoch: 6, batch: 16, loss: 0.7523131370544434.\n",
      "epoch: 6, batch: 17, loss: 0.8826949000358582.\n",
      "epoch: 6, batch: 18, loss: 0.8753395080566406.\n",
      "epoch: 6, batch: 19, loss: 0.8783650398254395.\n",
      "epoch: 6, batch: 20, loss: 0.7410977482795715.\n",
      "epoch: 6, batch: 21, loss: 0.994522750377655.\n",
      "epoch: 6, batch: 22, loss: 0.984019935131073.\n",
      "epoch: 6, batch: 23, loss: 0.9842274188995361.\n",
      "epoch: 6, batch: 24, loss: 1.0051908493041992.\n",
      "epoch: 6, batch: 25, loss: 0.9808818101882935.\n",
      "epoch: 6, batch: 26, loss: 0.9976958632469177.\n",
      "epoch: 6, batch: 27, loss: 0.9733819961547852.\n",
      "epoch: 6, batch: 28, loss: 0.8646088242530823.\n",
      "epoch: 6, batch: 29, loss: 0.8475382328033447.\n",
      "epoch: 6, batch: 30, loss: 0.8436287641525269.\n",
      "epoch: 6, batch: 31, loss: 0.9836914539337158.\n",
      "epoch: 6, batch: 32, loss: 0.8449033498764038.\n",
      "epoch: 6, batch: 33, loss: 0.8493983149528503.\n",
      "epoch: 6, batch: 34, loss: 0.9851459860801697.\n",
      "epoch: 6, batch: 35, loss: 0.8514846563339233.\n",
      "epoch: 6, batch: 36, loss: 0.8287153244018555.\n",
      "epoch: 6, batch: 37, loss: 0.9848086833953857.\n",
      "epoch: 6, batch: 38, loss: 1.00254487991333.\n",
      "epoch: 6, batch: 39, loss: 0.9745839834213257.\n",
      "epoch: 6, batch: 40, loss: 0.9840913414955139.\n",
      "epoch: 6, batch: 41, loss: 0.832676351070404.\n",
      "epoch: 6, batch: 42, loss: 0.973525881767273.\n",
      "epoch: 6, batch: 43, loss: 0.8722096085548401.\n",
      "epoch: 6, batch: 44, loss: 0.8107168674468994.\n",
      "epoch: 6, batch: 45, loss: 0.9744158983230591.\n",
      "epoch: 6, batch: 46, loss: 0.8310989737510681.\n",
      "epoch: 6, batch: 47, loss: 0.6894400119781494.\n",
      "epoch: 6, batch: 48, loss: 0.9588372111320496.\n",
      "epoch: 6, batch: 49, loss: 0.9724622368812561.\n",
      "epoch: 6, batch: 50, loss: 0.9754084944725037.\n",
      "epoch: 6, batch: 51, loss: 0.7025776505470276.\n",
      "epoch: 6, batch: 52, loss: 0.8231204152107239.\n",
      "epoch: 7, batch: 0, loss: 0.9553236961364746.\n",
      "epoch: 7, batch: 1, loss: 0.6892428398132324.\n",
      "epoch: 7, batch: 2, loss: 0.9798437356948853.\n",
      "epoch: 7, batch: 3, loss: 0.9500603675842285.\n",
      "epoch: 7, batch: 4, loss: 0.7878704071044922.\n",
      "epoch: 7, batch: 5, loss: 0.9156593084335327.\n",
      "epoch: 7, batch: 6, loss: 0.8766974210739136.\n",
      "epoch: 7, batch: 7, loss: 0.9852541089057922.\n",
      "epoch: 7, batch: 8, loss: 0.9641033411026001.\n",
      "epoch: 7, batch: 9, loss: 0.9511656165122986.\n",
      "epoch: 7, batch: 10, loss: 0.9719923734664917.\n",
      "epoch: 7, batch: 11, loss: 0.6647651195526123.\n",
      "epoch: 7, batch: 12, loss: 0.8021092414855957.\n",
      "epoch: 7, batch: 13, loss: 0.918331503868103.\n",
      "epoch: 7, batch: 14, loss: 0.8230685591697693.\n",
      "epoch: 7, batch: 15, loss: 0.9684330224990845.\n",
      "epoch: 7, batch: 16, loss: 0.7115521430969238.\n",
      "epoch: 7, batch: 17, loss: 0.8342480659484863.\n",
      "epoch: 7, batch: 18, loss: 0.9668139219284058.\n",
      "epoch: 7, batch: 19, loss: 0.9575818777084351.\n",
      "epoch: 7, batch: 20, loss: 0.8396069407463074.\n",
      "epoch: 7, batch: 21, loss: 0.9454780220985413.\n",
      "epoch: 7, batch: 22, loss: 0.8968470096588135.\n",
      "epoch: 7, batch: 23, loss: 0.9142919778823853.\n",
      "epoch: 7, batch: 24, loss: 0.6582433581352234.\n",
      "epoch: 7, batch: 25, loss: 0.6843442916870117.\n",
      "epoch: 7, batch: 26, loss: 0.6882425546646118.\n",
      "epoch: 7, batch: 27, loss: 0.9581992030143738.\n",
      "epoch: 7, batch: 28, loss: 0.9932464361190796.\n",
      "epoch: 7, batch: 29, loss: 0.7885547876358032.\n",
      "epoch: 7, batch: 30, loss: 0.9502480030059814.\n",
      "epoch: 7, batch: 31, loss: 0.9940512180328369.\n",
      "epoch: 7, batch: 32, loss: 0.8444693088531494.\n",
      "epoch: 7, batch: 33, loss: 0.8745808601379395.\n",
      "epoch: 7, batch: 34, loss: 0.9427129030227661.\n",
      "epoch: 7, batch: 35, loss: 0.7816770076751709.\n",
      "epoch: 7, batch: 36, loss: 0.9728809595108032.\n",
      "epoch: 7, batch: 37, loss: 0.8267111778259277.\n",
      "epoch: 7, batch: 38, loss: 0.7992801666259766.\n",
      "epoch: 7, batch: 39, loss: 0.9885765314102173.\n",
      "epoch: 7, batch: 40, loss: 0.9933925271034241.\n",
      "epoch: 7, batch: 41, loss: 0.8928612470626831.\n",
      "epoch: 7, batch: 42, loss: 0.8089010119438171.\n",
      "epoch: 7, batch: 43, loss: 0.9509285688400269.\n",
      "epoch: 7, batch: 44, loss: 0.6381827592849731.\n",
      "epoch: 7, batch: 45, loss: 0.8470697402954102.\n",
      "epoch: 7, batch: 46, loss: 0.809199333190918.\n",
      "epoch: 7, batch: 47, loss: 0.7898464202880859.\n",
      "epoch: 7, batch: 48, loss: 0.8211538791656494.\n",
      "epoch: 7, batch: 49, loss: 0.9728741645812988.\n",
      "epoch: 7, batch: 50, loss: 0.822980523109436.\n",
      "epoch: 7, batch: 51, loss: 0.9776567220687866.\n",
      "epoch: 7, batch: 52, loss: 0.9267736077308655.\n",
      "epoch: 8, batch: 0, loss: 0.6641231775283813.\n",
      "epoch: 8, batch: 1, loss: 0.8080450296401978.\n",
      "epoch: 8, batch: 2, loss: 0.8237547278404236.\n",
      "epoch: 8, batch: 3, loss: 0.9459161758422852.\n",
      "epoch: 8, batch: 4, loss: 0.9531335234642029.\n",
      "epoch: 8, batch: 5, loss: 0.7661415338516235.\n",
      "epoch: 8, batch: 6, loss: 0.8067711591720581.\n",
      "epoch: 8, batch: 7, loss: 0.7819101810455322.\n",
      "epoch: 8, batch: 8, loss: 0.8072458505630493.\n",
      "epoch: 8, batch: 9, loss: 0.8798332214355469.\n",
      "epoch: 8, batch: 10, loss: 0.89287930727005.\n",
      "epoch: 8, batch: 11, loss: 0.8116501569747925.\n",
      "epoch: 8, batch: 12, loss: 0.8138123750686646.\n",
      "epoch: 8, batch: 13, loss: 0.7885372042655945.\n",
      "epoch: 8, batch: 14, loss: 0.8968027830123901.\n",
      "epoch: 8, batch: 15, loss: 0.9319971203804016.\n",
      "epoch: 8, batch: 16, loss: 0.8695955872535706.\n",
      "epoch: 8, batch: 17, loss: 0.6145110726356506.\n",
      "epoch: 8, batch: 18, loss: 0.9455735683441162.\n",
      "epoch: 8, batch: 19, loss: 0.7688413858413696.\n",
      "epoch: 8, batch: 20, loss: 0.9731878042221069.\n",
      "epoch: 8, batch: 21, loss: 0.9728937149047852.\n",
      "epoch: 8, batch: 22, loss: 0.8258570432662964.\n",
      "epoch: 8, batch: 23, loss: 0.8096648454666138.\n",
      "epoch: 8, batch: 24, loss: 0.7596545219421387.\n",
      "epoch: 8, batch: 25, loss: 0.8943065404891968.\n",
      "epoch: 8, batch: 26, loss: 0.9967533946037292.\n",
      "epoch: 8, batch: 27, loss: 0.9145904779434204.\n",
      "epoch: 8, batch: 28, loss: 0.9194319248199463.\n",
      "epoch: 8, batch: 29, loss: 0.793419361114502.\n",
      "epoch: 8, batch: 30, loss: 0.7425743341445923.\n",
      "epoch: 8, batch: 31, loss: 0.8576803207397461.\n",
      "epoch: 8, batch: 32, loss: 0.9878666400909424.\n",
      "epoch: 8, batch: 33, loss: 0.9510221481323242.\n",
      "epoch: 8, batch: 34, loss: 0.7239096760749817.\n",
      "epoch: 8, batch: 35, loss: 0.7810271978378296.\n",
      "epoch: 8, batch: 36, loss: 0.9782562255859375.\n",
      "epoch: 8, batch: 37, loss: 0.9289982914924622.\n",
      "epoch: 8, batch: 38, loss: 0.9473086595535278.\n",
      "epoch: 8, batch: 39, loss: 0.9704492688179016.\n",
      "epoch: 8, batch: 40, loss: 0.7652075290679932.\n",
      "epoch: 8, batch: 41, loss: 0.9237498044967651.\n",
      "epoch: 8, batch: 42, loss: 0.8134867548942566.\n",
      "epoch: 8, batch: 43, loss: 0.8974824547767639.\n",
      "epoch: 8, batch: 44, loss: 0.7987747192382812.\n",
      "epoch: 8, batch: 45, loss: 0.6133803725242615.\n",
      "epoch: 8, batch: 46, loss: 0.6186934113502502.\n",
      "epoch: 8, batch: 47, loss: 0.8594945669174194.\n",
      "epoch: 8, batch: 48, loss: 0.7769215106964111.\n",
      "epoch: 8, batch: 49, loss: 0.8434132933616638.\n",
      "epoch: 8, batch: 50, loss: 0.9540250897407532.\n",
      "epoch: 8, batch: 51, loss: 0.7763199210166931.\n",
      "epoch: 8, batch: 52, loss: 0.6645123958587646.\n",
      "epoch: 9, batch: 0, loss: 0.9604908227920532.\n",
      "epoch: 9, batch: 1, loss: 0.8947287797927856.\n",
      "epoch: 9, batch: 2, loss: 0.9482510685920715.\n",
      "epoch: 9, batch: 3, loss: 0.9250984191894531.\n",
      "epoch: 9, batch: 4, loss: 0.6660444736480713.\n",
      "epoch: 9, batch: 5, loss: 0.7649658918380737.\n",
      "epoch: 9, batch: 6, loss: 0.7871086597442627.\n",
      "epoch: 9, batch: 7, loss: 0.9499413967132568.\n",
      "epoch: 9, batch: 8, loss: 0.7988048791885376.\n",
      "epoch: 9, batch: 9, loss: 0.9065815210342407.\n",
      "epoch: 9, batch: 10, loss: 0.7300586700439453.\n",
      "epoch: 9, batch: 11, loss: 0.7294110059738159.\n",
      "epoch: 9, batch: 12, loss: 0.9292612075805664.\n",
      "epoch: 9, batch: 13, loss: 0.894982635974884.\n",
      "epoch: 9, batch: 14, loss: 0.9305298328399658.\n",
      "epoch: 9, batch: 15, loss: 0.9501734375953674.\n",
      "epoch: 9, batch: 16, loss: 0.9363739490509033.\n",
      "epoch: 9, batch: 17, loss: 0.931409478187561.\n",
      "epoch: 9, batch: 18, loss: 0.9370728135108948.\n",
      "epoch: 9, batch: 19, loss: 0.7561897039413452.\n",
      "epoch: 9, batch: 20, loss: 0.9038115739822388.\n",
      "epoch: 9, batch: 21, loss: 0.8869595527648926.\n",
      "epoch: 9, batch: 22, loss: 0.7469265460968018.\n",
      "epoch: 9, batch: 23, loss: 0.8004500865936279.\n",
      "epoch: 9, batch: 24, loss: 0.7323763370513916.\n",
      "epoch: 9, batch: 25, loss: 0.7406774759292603.\n",
      "epoch: 9, batch: 26, loss: 0.7816942930221558.\n",
      "epoch: 9, batch: 27, loss: 0.7599606513977051.\n",
      "epoch: 9, batch: 28, loss: 0.602295458316803.\n",
      "epoch: 9, batch: 29, loss: 0.7210638523101807.\n",
      "epoch: 9, batch: 30, loss: 0.7424346208572388.\n",
      "epoch: 9, batch: 31, loss: 0.9241153001785278.\n",
      "epoch: 9, batch: 32, loss: 0.7308497428894043.\n",
      "epoch: 9, batch: 33, loss: 0.775671124458313.\n",
      "epoch: 9, batch: 34, loss: 0.748155951499939.\n",
      "epoch: 9, batch: 35, loss: 0.8688347339630127.\n",
      "epoch: 9, batch: 36, loss: 0.7472065091133118.\n",
      "epoch: 9, batch: 37, loss: 0.9313757419586182.\n",
      "epoch: 9, batch: 38, loss: 0.7517487406730652.\n",
      "epoch: 9, batch: 39, loss: 0.7473461627960205.\n",
      "epoch: 9, batch: 40, loss: 0.7546484470367432.\n",
      "epoch: 9, batch: 41, loss: 0.7784111499786377.\n",
      "epoch: 9, batch: 42, loss: 0.6093957424163818.\n",
      "epoch: 9, batch: 43, loss: 0.8899966478347778.\n",
      "epoch: 9, batch: 44, loss: 0.7481779456138611.\n",
      "epoch: 9, batch: 45, loss: 0.7490581274032593.\n",
      "epoch: 9, batch: 46, loss: 0.8427573442459106.\n",
      "epoch: 9, batch: 47, loss: 0.8296021819114685.\n",
      "epoch: 9, batch: 48, loss: 0.8020222187042236.\n",
      "epoch: 9, batch: 49, loss: 0.8814609050750732.\n",
      "epoch: 9, batch: 50, loss: 0.8018717765808105.\n",
      "epoch: 9, batch: 51, loss: 0.868553876876831.\n",
      "epoch: 9, batch: 52, loss: 0.8468720316886902.\n",
      "epoch: 10, batch: 0, loss: 0.8278663158416748.\n",
      "epoch: 10, batch: 1, loss: 0.7598734498023987.\n",
      "epoch: 10, batch: 2, loss: 0.8398352861404419.\n",
      "epoch: 10, batch: 3, loss: 0.8385199308395386.\n",
      "epoch: 10, batch: 4, loss: 0.721555233001709.\n",
      "epoch: 10, batch: 5, loss: 0.8163572549819946.\n",
      "epoch: 10, batch: 6, loss: 0.7144452929496765.\n",
      "epoch: 10, batch: 7, loss: 0.7617313861846924.\n",
      "epoch: 10, batch: 8, loss: 0.6748833656311035.\n",
      "epoch: 10, batch: 9, loss: 0.8146333694458008.\n",
      "epoch: 10, batch: 10, loss: 0.736591100692749.\n",
      "epoch: 10, batch: 11, loss: 0.6318545937538147.\n",
      "epoch: 10, batch: 12, loss: 0.942302405834198.\n",
      "epoch: 10, batch: 13, loss: 0.8016139268875122.\n",
      "epoch: 10, batch: 14, loss: 0.9205586910247803.\n",
      "epoch: 10, batch: 15, loss: 0.6816620826721191.\n",
      "epoch: 10, batch: 16, loss: 0.8441818952560425.\n",
      "epoch: 10, batch: 17, loss: 0.9200757741928101.\n",
      "epoch: 10, batch: 18, loss: 0.8673496246337891.\n",
      "epoch: 10, batch: 19, loss: 1.0571606159210205.\n",
      "epoch: 10, batch: 20, loss: 0.7989733219146729.\n",
      "epoch: 10, batch: 21, loss: 0.8892771005630493.\n",
      "epoch: 10, batch: 22, loss: 0.6249330043792725.\n",
      "epoch: 10, batch: 23, loss: 0.6789757609367371.\n",
      "epoch: 10, batch: 24, loss: 0.777911901473999.\n",
      "epoch: 10, batch: 25, loss: 0.6854552626609802.\n",
      "epoch: 10, batch: 26, loss: 0.8796044588088989.\n",
      "epoch: 10, batch: 27, loss: 0.8674354553222656.\n",
      "epoch: 10, batch: 28, loss: 0.8914874196052551.\n",
      "epoch: 10, batch: 29, loss: 0.585902988910675.\n",
      "epoch: 10, batch: 30, loss: 0.8302281498908997.\n",
      "epoch: 10, batch: 31, loss: 0.6630715131759644.\n",
      "epoch: 10, batch: 32, loss: 0.8309670686721802.\n",
      "epoch: 10, batch: 33, loss: 0.6765075922012329.\n",
      "epoch: 10, batch: 34, loss: 0.8379145860671997.\n",
      "epoch: 10, batch: 35, loss: 0.606010377407074.\n",
      "epoch: 10, batch: 36, loss: 0.7524758577346802.\n",
      "epoch: 10, batch: 37, loss: 0.7583869695663452.\n",
      "epoch: 10, batch: 38, loss: 0.9313623905181885.\n",
      "epoch: 10, batch: 39, loss: 0.903461217880249.\n",
      "epoch: 10, batch: 40, loss: 0.7503896951675415.\n",
      "epoch: 10, batch: 41, loss: 0.7417194843292236.\n",
      "epoch: 10, batch: 42, loss: 0.9405325651168823.\n",
      "epoch: 10, batch: 43, loss: 0.7352275848388672.\n",
      "epoch: 10, batch: 44, loss: 0.7031540870666504.\n",
      "epoch: 10, batch: 45, loss: 0.7940866351127625.\n",
      "epoch: 10, batch: 46, loss: 0.6855087280273438.\n",
      "epoch: 10, batch: 47, loss: 0.7209042310714722.\n",
      "epoch: 10, batch: 48, loss: 0.8552088737487793.\n",
      "epoch: 10, batch: 49, loss: 0.7170424461364746.\n",
      "epoch: 10, batch: 50, loss: 0.925787091255188.\n",
      "epoch: 10, batch: 51, loss: 0.8412864208221436.\n",
      "epoch: 10, batch: 52, loss: 0.8919366002082825.\n",
      "epoch: 11, batch: 0, loss: 0.7296919822692871.\n",
      "epoch: 11, batch: 1, loss: 0.5673094987869263.\n",
      "epoch: 11, batch: 2, loss: 0.7117823958396912.\n",
      "epoch: 11, batch: 3, loss: 0.8124916553497314.\n",
      "epoch: 11, batch: 4, loss: 0.8277100324630737.\n",
      "epoch: 11, batch: 5, loss: 0.8835547566413879.\n",
      "epoch: 11, batch: 6, loss: 0.6918213367462158.\n",
      "epoch: 11, batch: 7, loss: 0.8699401021003723.\n",
      "epoch: 11, batch: 8, loss: 0.7285901308059692.\n",
      "epoch: 11, batch: 9, loss: 0.6027302742004395.\n",
      "epoch: 11, batch: 10, loss: 0.99946129322052.\n",
      "epoch: 11, batch: 11, loss: 0.9064537882804871.\n",
      "epoch: 11, batch: 12, loss: 0.8269141912460327.\n",
      "epoch: 11, batch: 13, loss: 0.8755159378051758.\n",
      "epoch: 11, batch: 14, loss: 0.6835029721260071.\n",
      "epoch: 11, batch: 15, loss: 0.5903148651123047.\n",
      "epoch: 11, batch: 16, loss: 0.6624537110328674.\n",
      "epoch: 11, batch: 17, loss: 0.8318632245063782.\n",
      "epoch: 11, batch: 18, loss: 0.8457604646682739.\n",
      "epoch: 11, batch: 19, loss: 0.6429986953735352.\n",
      "epoch: 11, batch: 20, loss: 0.762569785118103.\n",
      "epoch: 11, batch: 21, loss: 0.7695094347000122.\n",
      "epoch: 11, batch: 22, loss: 0.7206128835678101.\n",
      "epoch: 11, batch: 23, loss: 0.7838935256004333.\n",
      "epoch: 11, batch: 24, loss: 0.8428686857223511.\n",
      "epoch: 11, batch: 25, loss: 0.5997050404548645.\n",
      "epoch: 11, batch: 26, loss: 0.8859673142433167.\n",
      "epoch: 11, batch: 27, loss: 0.8352282047271729.\n",
      "epoch: 11, batch: 28, loss: 0.7864150404930115.\n",
      "epoch: 11, batch: 29, loss: 0.8652286529541016.\n",
      "epoch: 11, batch: 30, loss: 0.8302650451660156.\n",
      "epoch: 11, batch: 31, loss: 0.6698960065841675.\n",
      "epoch: 11, batch: 32, loss: 0.8168461322784424.\n",
      "epoch: 11, batch: 33, loss: 0.6706538200378418.\n",
      "epoch: 11, batch: 34, loss: 0.9435898065567017.\n",
      "epoch: 11, batch: 35, loss: 0.7806341052055359.\n",
      "epoch: 11, batch: 36, loss: 0.7537626028060913.\n",
      "epoch: 11, batch: 37, loss: 0.8056300282478333.\n",
      "epoch: 11, batch: 38, loss: 0.8575159311294556.\n",
      "epoch: 11, batch: 39, loss: 0.9482446908950806.\n",
      "epoch: 11, batch: 40, loss: 0.7859615087509155.\n",
      "epoch: 11, batch: 41, loss: 0.9296844005584717.\n",
      "epoch: 11, batch: 42, loss: 0.684695839881897.\n",
      "epoch: 11, batch: 43, loss: 0.7837663292884827.\n",
      "epoch: 11, batch: 44, loss: 0.8043789863586426.\n",
      "epoch: 11, batch: 45, loss: 0.7096459865570068.\n",
      "epoch: 11, batch: 46, loss: 0.714769721031189.\n",
      "epoch: 11, batch: 47, loss: 0.6915484070777893.\n",
      "epoch: 11, batch: 48, loss: 0.777624249458313.\n",
      "epoch: 11, batch: 49, loss: 0.7467895150184631.\n",
      "epoch: 11, batch: 50, loss: 0.7150384187698364.\n",
      "epoch: 11, batch: 51, loss: 0.7955851554870605.\n",
      "epoch: 11, batch: 52, loss: 0.593586802482605.\n",
      "epoch: 12, batch: 0, loss: 0.6271207928657532.\n",
      "epoch: 12, batch: 1, loss: 0.8073936700820923.\n",
      "epoch: 12, batch: 2, loss: 0.669189453125.\n",
      "epoch: 12, batch: 3, loss: 0.7575374245643616.\n",
      "epoch: 12, batch: 4, loss: 0.878686785697937.\n",
      "epoch: 12, batch: 5, loss: 0.6972917318344116.\n",
      "epoch: 12, batch: 6, loss: 0.8528314232826233.\n",
      "epoch: 12, batch: 7, loss: 0.8113068342208862.\n",
      "epoch: 12, batch: 8, loss: 0.9065238237380981.\n",
      "epoch: 12, batch: 9, loss: 0.7263144254684448.\n",
      "epoch: 12, batch: 10, loss: 0.7660180330276489.\n",
      "epoch: 12, batch: 11, loss: 0.7169009447097778.\n",
      "epoch: 12, batch: 12, loss: 0.8858746290206909.\n",
      "epoch: 12, batch: 13, loss: 0.7869917154312134.\n",
      "epoch: 12, batch: 14, loss: 0.6778813600540161.\n",
      "epoch: 12, batch: 15, loss: 0.8234556913375854.\n",
      "epoch: 12, batch: 16, loss: 0.8019344806671143.\n",
      "epoch: 12, batch: 17, loss: 0.8635135293006897.\n",
      "epoch: 12, batch: 18, loss: 0.8568246364593506.\n",
      "epoch: 12, batch: 19, loss: 0.7956914901733398.\n",
      "epoch: 12, batch: 20, loss: 0.7769644260406494.\n",
      "epoch: 12, batch: 21, loss: 0.674707293510437.\n",
      "epoch: 12, batch: 22, loss: 0.789557695388794.\n",
      "epoch: 12, batch: 23, loss: 0.6569024324417114.\n",
      "epoch: 12, batch: 24, loss: 0.7452664375305176.\n",
      "epoch: 12, batch: 25, loss: 0.5850255489349365.\n",
      "epoch: 12, batch: 26, loss: 0.7889818549156189.\n",
      "epoch: 12, batch: 27, loss: 0.7735752463340759.\n",
      "epoch: 12, batch: 28, loss: 0.8857854008674622.\n",
      "epoch: 12, batch: 29, loss: 0.7589746713638306.\n",
      "epoch: 12, batch: 30, loss: 0.6935822367668152.\n",
      "epoch: 12, batch: 31, loss: 0.791316568851471.\n",
      "epoch: 12, batch: 32, loss: 0.7557616829872131.\n",
      "epoch: 12, batch: 33, loss: 0.6643402576446533.\n",
      "epoch: 12, batch: 34, loss: 0.6966693997383118.\n",
      "epoch: 12, batch: 35, loss: 0.8098798990249634.\n",
      "epoch: 12, batch: 36, loss: 0.6006860136985779.\n",
      "epoch: 12, batch: 37, loss: 0.6463602185249329.\n",
      "epoch: 12, batch: 38, loss: 0.5718123912811279.\n",
      "epoch: 12, batch: 39, loss: 0.8093863129615784.\n",
      "epoch: 12, batch: 40, loss: 0.7505457401275635.\n",
      "epoch: 12, batch: 41, loss: 0.8208287954330444.\n",
      "epoch: 12, batch: 42, loss: 0.6992963552474976.\n",
      "epoch: 12, batch: 43, loss: 0.7197688817977905.\n",
      "epoch: 12, batch: 44, loss: 0.6599315404891968.\n",
      "epoch: 12, batch: 45, loss: 0.8148367404937744.\n",
      "epoch: 12, batch: 46, loss: 0.765267014503479.\n",
      "epoch: 12, batch: 47, loss: 0.7007743120193481.\n",
      "epoch: 12, batch: 48, loss: 0.8318860530853271.\n",
      "epoch: 12, batch: 49, loss: 0.6912808418273926.\n",
      "epoch: 12, batch: 50, loss: 0.5893813371658325.\n",
      "epoch: 12, batch: 51, loss: 0.8329752683639526.\n",
      "epoch: 12, batch: 52, loss: 0.5927018523216248.\n",
      "epoch: 13, batch: 0, loss: 0.7165974974632263.\n",
      "epoch: 13, batch: 1, loss: 0.7721017599105835.\n",
      "epoch: 13, batch: 2, loss: 0.6717237234115601.\n",
      "epoch: 13, batch: 3, loss: 0.6515465974807739.\n",
      "epoch: 13, batch: 4, loss: 0.7673352956771851.\n",
      "epoch: 13, batch: 5, loss: 0.6526792049407959.\n",
      "epoch: 13, batch: 6, loss: 0.7909103035926819.\n",
      "epoch: 13, batch: 7, loss: 0.7872803211212158.\n",
      "epoch: 13, batch: 8, loss: 0.6700973510742188.\n",
      "epoch: 13, batch: 9, loss: 0.6430909037590027.\n",
      "epoch: 13, batch: 10, loss: 0.8774497509002686.\n",
      "epoch: 13, batch: 11, loss: 0.6801532506942749.\n",
      "epoch: 13, batch: 12, loss: 0.7735670208930969.\n",
      "epoch: 13, batch: 13, loss: 0.7453698515892029.\n",
      "epoch: 13, batch: 14, loss: 0.7779362201690674.\n",
      "epoch: 13, batch: 15, loss: 0.7247170209884644.\n",
      "epoch: 13, batch: 16, loss: 0.7895861864089966.\n",
      "epoch: 13, batch: 17, loss: 0.6800947189331055.\n",
      "epoch: 13, batch: 18, loss: 0.6906252503395081.\n",
      "epoch: 13, batch: 19, loss: 0.6590512990951538.\n",
      "epoch: 13, batch: 20, loss: 0.6977916359901428.\n",
      "epoch: 13, batch: 21, loss: 0.8176411390304565.\n",
      "epoch: 13, batch: 22, loss: 0.6299136877059937.\n",
      "epoch: 13, batch: 23, loss: 0.7399501800537109.\n",
      "epoch: 13, batch: 24, loss: 0.7814697027206421.\n",
      "epoch: 13, batch: 25, loss: 0.7337807416915894.\n",
      "epoch: 13, batch: 26, loss: 0.6918994188308716.\n",
      "epoch: 13, batch: 27, loss: 0.7151545882225037.\n",
      "epoch: 13, batch: 28, loss: 0.7708641290664673.\n",
      "epoch: 13, batch: 29, loss: 0.7902973890304565.\n",
      "epoch: 13, batch: 30, loss: 0.8198157548904419.\n",
      "epoch: 13, batch: 31, loss: 0.7787908315658569.\n",
      "epoch: 13, batch: 32, loss: 0.6798236966133118.\n",
      "epoch: 13, batch: 33, loss: 0.8543552160263062.\n",
      "epoch: 13, batch: 34, loss: 0.676933228969574.\n",
      "epoch: 13, batch: 35, loss: 0.8408096432685852.\n",
      "epoch: 13, batch: 36, loss: 0.6449992060661316.\n",
      "epoch: 13, batch: 37, loss: 0.5773630738258362.\n",
      "epoch: 13, batch: 38, loss: 0.7901362180709839.\n",
      "epoch: 13, batch: 39, loss: 0.5920076370239258.\n",
      "epoch: 13, batch: 40, loss: 0.658574104309082.\n",
      "epoch: 13, batch: 41, loss: 0.9795116782188416.\n",
      "epoch: 13, batch: 42, loss: 0.77250736951828.\n",
      "epoch: 13, batch: 43, loss: 0.6110650300979614.\n",
      "epoch: 13, batch: 44, loss: 0.661944568157196.\n",
      "epoch: 13, batch: 45, loss: 0.989841103553772.\n",
      "epoch: 13, batch: 46, loss: 0.7478435635566711.\n",
      "epoch: 13, batch: 47, loss: 0.8877060413360596.\n",
      "epoch: 13, batch: 48, loss: 0.5760114192962646.\n",
      "epoch: 13, batch: 49, loss: 0.6362365484237671.\n",
      "epoch: 13, batch: 50, loss: 0.6531041860580444.\n",
      "epoch: 13, batch: 51, loss: 0.6888312697410583.\n",
      "epoch: 13, batch: 52, loss: 0.8567817807197571.\n",
      "epoch: 14, batch: 0, loss: 0.6677244305610657.\n",
      "epoch: 14, batch: 1, loss: 0.657012939453125.\n",
      "epoch: 14, batch: 2, loss: 0.8033149838447571.\n",
      "epoch: 14, batch: 3, loss: 0.7699098587036133.\n",
      "epoch: 14, batch: 4, loss: 0.8507319688796997.\n",
      "epoch: 14, batch: 5, loss: 0.6432941555976868.\n",
      "epoch: 14, batch: 6, loss: 0.637357771396637.\n",
      "epoch: 14, batch: 7, loss: 0.8199881315231323.\n",
      "epoch: 14, batch: 8, loss: 0.8108665347099304.\n",
      "epoch: 14, batch: 9, loss: 0.8882305026054382.\n",
      "epoch: 14, batch: 10, loss: 0.802790641784668.\n",
      "epoch: 14, batch: 11, loss: 0.7098573446273804.\n",
      "epoch: 14, batch: 12, loss: 0.6447303295135498.\n",
      "epoch: 14, batch: 13, loss: 0.6270698308944702.\n",
      "epoch: 14, batch: 14, loss: 0.6417964696884155.\n",
      "epoch: 14, batch: 15, loss: 0.7319743037223816.\n",
      "epoch: 14, batch: 16, loss: 0.8320120573043823.\n",
      "epoch: 14, batch: 17, loss: 0.6966109871864319.\n",
      "epoch: 14, batch: 18, loss: 0.6350367069244385.\n",
      "epoch: 14, batch: 19, loss: 0.7204025983810425.\n",
      "epoch: 14, batch: 20, loss: 0.6544997692108154.\n",
      "epoch: 14, batch: 21, loss: 0.7005822658538818.\n",
      "epoch: 14, batch: 22, loss: 0.5748394727706909.\n",
      "epoch: 14, batch: 23, loss: 0.6582549214363098.\n",
      "epoch: 14, batch: 24, loss: 0.5753478407859802.\n",
      "epoch: 14, batch: 25, loss: 0.7143579721450806.\n",
      "epoch: 14, batch: 26, loss: 0.8843435049057007.\n",
      "epoch: 14, batch: 27, loss: 0.5646107196807861.\n",
      "epoch: 14, batch: 28, loss: 0.6423307061195374.\n",
      "epoch: 14, batch: 29, loss: 0.7192106246948242.\n",
      "epoch: 14, batch: 30, loss: 0.7552671432495117.\n",
      "epoch: 14, batch: 31, loss: 0.8543846607208252.\n",
      "epoch: 14, batch: 32, loss: 0.7806893587112427.\n",
      "epoch: 14, batch: 33, loss: 0.7090195417404175.\n",
      "epoch: 14, batch: 34, loss: 0.6993293762207031.\n",
      "epoch: 14, batch: 35, loss: 0.5890630483627319.\n",
      "epoch: 14, batch: 36, loss: 0.7510590553283691.\n",
      "epoch: 14, batch: 37, loss: 0.8237241506576538.\n",
      "epoch: 14, batch: 38, loss: 0.6583117246627808.\n",
      "epoch: 14, batch: 39, loss: 0.712145209312439.\n",
      "epoch: 14, batch: 40, loss: 0.9245989322662354.\n",
      "epoch: 14, batch: 41, loss: 0.7520419359207153.\n",
      "epoch: 14, batch: 42, loss: 0.6152334213256836.\n",
      "epoch: 14, batch: 43, loss: 0.6668433547019958.\n",
      "epoch: 14, batch: 44, loss: 0.7216767072677612.\n",
      "epoch: 14, batch: 45, loss: 0.677738606929779.\n",
      "epoch: 14, batch: 46, loss: 0.8078110814094543.\n",
      "epoch: 14, batch: 47, loss: 0.8434668779373169.\n",
      "epoch: 14, batch: 48, loss: 0.6320940256118774.\n",
      "epoch: 14, batch: 49, loss: 0.6846756339073181.\n",
      "epoch: 14, batch: 50, loss: 0.6626375317573547.\n",
      "epoch: 14, batch: 51, loss: 0.6940510869026184.\n",
      "epoch: 14, batch: 52, loss: 0.5962316989898682.\n",
      "epoch: 15, batch: 0, loss: 0.778567373752594.\n",
      "epoch: 15, batch: 1, loss: 0.573723554611206.\n",
      "epoch: 15, batch: 2, loss: 0.7551544904708862.\n",
      "epoch: 15, batch: 3, loss: 0.6469782590866089.\n",
      "epoch: 15, batch: 4, loss: 0.8020145893096924.\n",
      "epoch: 15, batch: 5, loss: 0.7506734132766724.\n",
      "epoch: 15, batch: 6, loss: 0.8380658030509949.\n",
      "epoch: 15, batch: 7, loss: 0.7289614081382751.\n",
      "epoch: 15, batch: 8, loss: 0.7849077582359314.\n",
      "epoch: 15, batch: 9, loss: 0.6523505449295044.\n",
      "epoch: 15, batch: 10, loss: 0.6937505006790161.\n",
      "epoch: 15, batch: 11, loss: 0.6215330362319946.\n",
      "epoch: 15, batch: 12, loss: 0.654998242855072.\n",
      "epoch: 15, batch: 13, loss: 0.6464976072311401.\n",
      "epoch: 15, batch: 14, loss: 0.8092498779296875.\n",
      "epoch: 15, batch: 15, loss: 0.6427356600761414.\n",
      "epoch: 15, batch: 16, loss: 0.5891662240028381.\n",
      "epoch: 15, batch: 17, loss: 0.6955232620239258.\n",
      "epoch: 15, batch: 18, loss: 0.8040920495986938.\n",
      "epoch: 15, batch: 19, loss: 0.7249232530593872.\n",
      "epoch: 15, batch: 20, loss: 0.8841578960418701.\n",
      "epoch: 15, batch: 21, loss: 0.7168737649917603.\n",
      "epoch: 15, batch: 22, loss: 0.6414623260498047.\n",
      "epoch: 15, batch: 23, loss: 0.5589487552642822.\n",
      "epoch: 15, batch: 24, loss: 0.7054879665374756.\n",
      "epoch: 15, batch: 25, loss: 0.6752307415008545.\n",
      "epoch: 15, batch: 26, loss: 0.6911299228668213.\n",
      "epoch: 15, batch: 27, loss: 0.8520733118057251.\n",
      "epoch: 15, batch: 28, loss: 0.7562656402587891.\n",
      "epoch: 15, batch: 29, loss: 0.5851736664772034.\n",
      "epoch: 15, batch: 30, loss: 0.6242377758026123.\n",
      "epoch: 15, batch: 31, loss: 0.6647968292236328.\n",
      "epoch: 15, batch: 32, loss: 0.7139255404472351.\n",
      "epoch: 15, batch: 33, loss: 0.6353425979614258.\n",
      "epoch: 15, batch: 34, loss: 0.830781102180481.\n",
      "epoch: 15, batch: 35, loss: 0.7020211815834045.\n",
      "epoch: 15, batch: 36, loss: 0.6733531951904297.\n",
      "epoch: 15, batch: 37, loss: 0.6735253930091858.\n",
      "epoch: 15, batch: 38, loss: 0.9514886140823364.\n",
      "epoch: 15, batch: 39, loss: 0.7369738817214966.\n",
      "epoch: 15, batch: 40, loss: 0.6632994413375854.\n",
      "epoch: 15, batch: 41, loss: 0.701407253742218.\n",
      "epoch: 15, batch: 42, loss: 0.8083036541938782.\n",
      "epoch: 15, batch: 43, loss: 0.926028847694397.\n",
      "epoch: 15, batch: 44, loss: 0.6856511831283569.\n",
      "epoch: 15, batch: 45, loss: 0.695088267326355.\n",
      "epoch: 15, batch: 46, loss: 0.6229945421218872.\n",
      "epoch: 15, batch: 47, loss: 0.6217316389083862.\n",
      "epoch: 15, batch: 48, loss: 0.6562880277633667.\n",
      "epoch: 15, batch: 49, loss: 0.6182515025138855.\n",
      "epoch: 15, batch: 50, loss: 0.5761183500289917.\n",
      "epoch: 15, batch: 51, loss: 0.6591134071350098.\n",
      "epoch: 15, batch: 52, loss: 0.6600121855735779.\n",
      "epoch: 16, batch: 0, loss: 0.7476277351379395.\n",
      "epoch: 16, batch: 1, loss: 0.7905078530311584.\n",
      "epoch: 16, batch: 2, loss: 0.8254790306091309.\n",
      "epoch: 16, batch: 3, loss: 0.6410510540008545.\n",
      "epoch: 16, batch: 4, loss: 0.6770601868629456.\n",
      "epoch: 16, batch: 5, loss: 0.8029744625091553.\n",
      "epoch: 16, batch: 6, loss: 0.637880265712738.\n",
      "epoch: 16, batch: 7, loss: 0.6832106709480286.\n",
      "epoch: 16, batch: 8, loss: 0.8544224500656128.\n",
      "epoch: 16, batch: 9, loss: 0.6766003370285034.\n",
      "epoch: 16, batch: 10, loss: 0.8383762836456299.\n",
      "epoch: 16, batch: 11, loss: 0.7179114818572998.\n",
      "epoch: 16, batch: 12, loss: 0.643075704574585.\n",
      "epoch: 16, batch: 13, loss: 0.6389610767364502.\n",
      "epoch: 16, batch: 14, loss: 0.6431730389595032.\n",
      "epoch: 16, batch: 15, loss: 0.6560717821121216.\n",
      "epoch: 16, batch: 16, loss: 0.6841872930526733.\n",
      "epoch: 16, batch: 17, loss: 0.6463559865951538.\n",
      "epoch: 16, batch: 18, loss: 0.6571956872940063.\n",
      "epoch: 16, batch: 19, loss: 0.6237322688102722.\n",
      "epoch: 16, batch: 20, loss: 0.6343928575515747.\n",
      "epoch: 16, batch: 21, loss: 0.6578543186187744.\n",
      "epoch: 16, batch: 22, loss: 0.7269479036331177.\n",
      "epoch: 16, batch: 23, loss: 0.6672829985618591.\n",
      "epoch: 16, batch: 24, loss: 0.8015786409378052.\n",
      "epoch: 16, batch: 25, loss: 0.6500238180160522.\n",
      "epoch: 16, batch: 26, loss: 0.7224081754684448.\n",
      "epoch: 16, batch: 27, loss: 0.6103174686431885.\n",
      "epoch: 16, batch: 28, loss: 0.6146553754806519.\n",
      "epoch: 16, batch: 29, loss: 0.6749061346054077.\n",
      "epoch: 16, batch: 30, loss: 0.8417553901672363.\n",
      "epoch: 16, batch: 31, loss: 0.5654683113098145.\n",
      "epoch: 16, batch: 32, loss: 0.6319711804389954.\n",
      "epoch: 16, batch: 33, loss: 0.6503415703773499.\n",
      "epoch: 16, batch: 34, loss: 0.7426779866218567.\n",
      "epoch: 16, batch: 35, loss: 0.6834863424301147.\n",
      "epoch: 16, batch: 36, loss: 0.7156244516372681.\n",
      "epoch: 16, batch: 37, loss: 0.6872373819351196.\n",
      "epoch: 16, batch: 38, loss: 0.6332906484603882.\n",
      "epoch: 16, batch: 39, loss: 0.7095627784729004.\n",
      "epoch: 16, batch: 40, loss: 0.7133673429489136.\n",
      "epoch: 16, batch: 41, loss: 0.5891552567481995.\n",
      "epoch: 16, batch: 42, loss: 0.7909573316574097.\n",
      "epoch: 16, batch: 43, loss: 0.5996742248535156.\n",
      "epoch: 16, batch: 44, loss: 0.6774561405181885.\n",
      "epoch: 16, batch: 45, loss: 0.6374870538711548.\n",
      "epoch: 16, batch: 46, loss: 0.8318111896514893.\n",
      "epoch: 16, batch: 47, loss: 0.6260998249053955.\n",
      "epoch: 16, batch: 48, loss: 0.8604837656021118.\n",
      "epoch: 16, batch: 49, loss: 0.6614532470703125.\n",
      "epoch: 16, batch: 50, loss: 0.6514847278594971.\n",
      "epoch: 16, batch: 51, loss: 0.6053363084793091.\n",
      "epoch: 16, batch: 52, loss: 0.5937188267707825.\n",
      "epoch: 17, batch: 0, loss: 0.6783697605133057.\n",
      "epoch: 17, batch: 1, loss: 0.7770986557006836.\n",
      "epoch: 17, batch: 2, loss: 0.6114603281021118.\n",
      "epoch: 17, batch: 3, loss: 0.6140605211257935.\n",
      "epoch: 17, batch: 4, loss: 0.7666187286376953.\n",
      "epoch: 17, batch: 5, loss: 0.6644196510314941.\n",
      "epoch: 17, batch: 6, loss: 0.5585955381393433.\n",
      "epoch: 17, batch: 7, loss: 0.7358465194702148.\n",
      "epoch: 17, batch: 8, loss: 0.7752186059951782.\n",
      "epoch: 17, batch: 9, loss: 0.7937304973602295.\n",
      "epoch: 17, batch: 10, loss: 0.8963040113449097.\n",
      "epoch: 17, batch: 11, loss: 0.6025863885879517.\n",
      "epoch: 17, batch: 12, loss: 0.6953712105751038.\n",
      "epoch: 17, batch: 13, loss: 0.6866356134414673.\n",
      "epoch: 17, batch: 14, loss: 0.5998386144638062.\n",
      "epoch: 17, batch: 15, loss: 0.6903887987136841.\n",
      "epoch: 17, batch: 16, loss: 0.5971488952636719.\n",
      "epoch: 17, batch: 17, loss: 0.6483559608459473.\n",
      "epoch: 17, batch: 18, loss: 0.7178045511245728.\n",
      "epoch: 17, batch: 19, loss: 0.7279883027076721.\n",
      "epoch: 17, batch: 20, loss: 0.6070517301559448.\n",
      "epoch: 17, batch: 21, loss: 0.7428571581840515.\n",
      "epoch: 17, batch: 22, loss: 0.6108112335205078.\n",
      "epoch: 17, batch: 23, loss: 0.6481361389160156.\n",
      "epoch: 17, batch: 24, loss: 0.5835855007171631.\n",
      "epoch: 17, batch: 25, loss: 0.8097262978553772.\n",
      "epoch: 17, batch: 26, loss: 0.6074426770210266.\n",
      "epoch: 17, batch: 27, loss: 0.6118143796920776.\n",
      "epoch: 17, batch: 28, loss: 0.6420687437057495.\n",
      "epoch: 17, batch: 29, loss: 0.6706531047821045.\n",
      "epoch: 17, batch: 30, loss: 0.6514430046081543.\n",
      "epoch: 17, batch: 31, loss: 0.602910041809082.\n",
      "epoch: 17, batch: 32, loss: 0.5754371881484985.\n",
      "epoch: 17, batch: 33, loss: 0.718384861946106.\n",
      "epoch: 17, batch: 34, loss: 0.6434272527694702.\n",
      "epoch: 17, batch: 35, loss: 0.8057988286018372.\n",
      "epoch: 17, batch: 36, loss: 0.6320494413375854.\n",
      "epoch: 17, batch: 37, loss: 0.6838979721069336.\n",
      "epoch: 17, batch: 38, loss: 0.6866245269775391.\n",
      "epoch: 17, batch: 39, loss: 0.6661417484283447.\n",
      "epoch: 17, batch: 40, loss: 0.727584719657898.\n",
      "epoch: 17, batch: 41, loss: 0.6292520761489868.\n",
      "epoch: 17, batch: 42, loss: 0.6185665130615234.\n",
      "epoch: 17, batch: 43, loss: 0.6649291515350342.\n",
      "epoch: 17, batch: 44, loss: 0.6041407585144043.\n",
      "epoch: 17, batch: 45, loss: 0.844692587852478.\n",
      "epoch: 17, batch: 46, loss: 0.7316310405731201.\n",
      "epoch: 17, batch: 47, loss: 0.8104766011238098.\n",
      "epoch: 17, batch: 48, loss: 0.6140064597129822.\n",
      "epoch: 17, batch: 49, loss: 0.62959885597229.\n",
      "epoch: 17, batch: 50, loss: 0.631787896156311.\n",
      "epoch: 17, batch: 51, loss: 0.6697013974189758.\n",
      "epoch: 17, batch: 52, loss: 0.666049063205719.\n",
      "epoch: 18, batch: 0, loss: 0.7700549364089966.\n",
      "epoch: 18, batch: 1, loss: 0.7340699434280396.\n",
      "epoch: 18, batch: 2, loss: 0.6027791500091553.\n",
      "epoch: 18, batch: 3, loss: 0.6782058477401733.\n",
      "epoch: 18, batch: 4, loss: 0.7447246313095093.\n",
      "epoch: 18, batch: 5, loss: 0.5838874578475952.\n",
      "epoch: 18, batch: 6, loss: 0.6115511655807495.\n",
      "epoch: 18, batch: 7, loss: 0.632845938205719.\n",
      "epoch: 18, batch: 8, loss: 0.7036336660385132.\n",
      "epoch: 18, batch: 9, loss: 0.6415901184082031.\n",
      "epoch: 18, batch: 10, loss: 0.6356561183929443.\n",
      "epoch: 18, batch: 11, loss: 0.5842752456665039.\n",
      "epoch: 18, batch: 12, loss: 0.6304000616073608.\n",
      "epoch: 18, batch: 13, loss: 0.652190089225769.\n",
      "epoch: 18, batch: 14, loss: 0.7308937311172485.\n",
      "epoch: 18, batch: 15, loss: 0.6940785646438599.\n",
      "epoch: 18, batch: 16, loss: 0.6493202447891235.\n",
      "epoch: 18, batch: 17, loss: 0.77541184425354.\n",
      "epoch: 18, batch: 18, loss: 0.5787914991378784.\n",
      "epoch: 18, batch: 19, loss: 0.803990364074707.\n",
      "epoch: 18, batch: 20, loss: 0.6598643064498901.\n",
      "epoch: 18, batch: 21, loss: 0.6454014182090759.\n",
      "epoch: 18, batch: 22, loss: 0.5910134315490723.\n",
      "epoch: 18, batch: 23, loss: 0.5931605100631714.\n",
      "epoch: 18, batch: 24, loss: 0.6816892623901367.\n",
      "epoch: 18, batch: 25, loss: 0.6121244430541992.\n",
      "epoch: 18, batch: 26, loss: 0.6652044057846069.\n",
      "epoch: 18, batch: 27, loss: 0.6334154605865479.\n",
      "epoch: 18, batch: 28, loss: 0.6640154123306274.\n",
      "epoch: 18, batch: 29, loss: 0.6123584508895874.\n",
      "epoch: 18, batch: 30, loss: 0.6050276756286621.\n",
      "epoch: 18, batch: 31, loss: 0.5661900639533997.\n",
      "epoch: 18, batch: 32, loss: 0.675521731376648.\n",
      "epoch: 18, batch: 33, loss: 0.9223995208740234.\n",
      "epoch: 18, batch: 34, loss: 0.5658446550369263.\n",
      "epoch: 18, batch: 35, loss: 0.6299580335617065.\n",
      "epoch: 18, batch: 36, loss: 0.8260698914527893.\n",
      "epoch: 18, batch: 37, loss: 0.6669266223907471.\n",
      "epoch: 18, batch: 38, loss: 0.5859310626983643.\n",
      "epoch: 18, batch: 39, loss: 0.8155355453491211.\n",
      "epoch: 18, batch: 40, loss: 0.7767833471298218.\n",
      "epoch: 18, batch: 41, loss: 0.6232731342315674.\n",
      "epoch: 18, batch: 42, loss: 0.7011820077896118.\n",
      "epoch: 18, batch: 43, loss: 0.6123311519622803.\n",
      "epoch: 18, batch: 44, loss: 0.5746159553527832.\n",
      "epoch: 18, batch: 45, loss: 0.788580060005188.\n",
      "epoch: 18, batch: 46, loss: 0.6649073362350464.\n",
      "epoch: 18, batch: 47, loss: 0.6075390577316284.\n",
      "epoch: 18, batch: 48, loss: 0.6871657967567444.\n",
      "epoch: 18, batch: 49, loss: 0.8063825964927673.\n",
      "epoch: 18, batch: 50, loss: 0.8439691066741943.\n",
      "epoch: 18, batch: 51, loss: 0.5856109261512756.\n",
      "epoch: 18, batch: 52, loss: 0.7066559791564941.\n",
      "epoch: 19, batch: 0, loss: 0.6594586968421936.\n",
      "epoch: 19, batch: 1, loss: 0.7078962326049805.\n",
      "epoch: 19, batch: 2, loss: 0.8529093861579895.\n",
      "epoch: 19, batch: 3, loss: 0.5713726878166199.\n",
      "epoch: 19, batch: 4, loss: 0.603581428527832.\n",
      "epoch: 19, batch: 5, loss: 0.6494327783584595.\n",
      "epoch: 19, batch: 6, loss: 0.6950952410697937.\n",
      "epoch: 19, batch: 7, loss: 0.5954908132553101.\n",
      "epoch: 19, batch: 8, loss: 0.6333522796630859.\n",
      "epoch: 19, batch: 9, loss: 0.6348671317100525.\n",
      "epoch: 19, batch: 10, loss: 0.636831521987915.\n",
      "epoch: 19, batch: 11, loss: 0.7091498374938965.\n",
      "epoch: 19, batch: 12, loss: 0.7139784097671509.\n",
      "epoch: 19, batch: 13, loss: 0.679295539855957.\n",
      "epoch: 19, batch: 14, loss: 0.8177295327186584.\n",
      "epoch: 19, batch: 15, loss: 0.6284126043319702.\n",
      "epoch: 19, batch: 16, loss: 0.6578941345214844.\n",
      "epoch: 19, batch: 17, loss: 0.5791728496551514.\n",
      "epoch: 19, batch: 18, loss: 0.5875663757324219.\n",
      "epoch: 19, batch: 19, loss: 0.6722593903541565.\n",
      "epoch: 19, batch: 20, loss: 0.9208673238754272.\n",
      "epoch: 19, batch: 21, loss: 0.5915954113006592.\n",
      "epoch: 19, batch: 22, loss: 0.607275664806366.\n",
      "epoch: 19, batch: 23, loss: 0.6120211482048035.\n",
      "epoch: 19, batch: 24, loss: 0.7938704490661621.\n",
      "epoch: 19, batch: 25, loss: 0.700231671333313.\n",
      "epoch: 19, batch: 26, loss: 0.646645188331604.\n",
      "epoch: 19, batch: 27, loss: 0.7625594139099121.\n",
      "epoch: 19, batch: 28, loss: 0.5896898508071899.\n",
      "epoch: 19, batch: 29, loss: 0.5598167181015015.\n",
      "epoch: 19, batch: 30, loss: 0.7221789360046387.\n",
      "epoch: 19, batch: 31, loss: 0.6747461557388306.\n",
      "epoch: 19, batch: 32, loss: 0.6283689141273499.\n",
      "epoch: 19, batch: 33, loss: 0.6350476741790771.\n",
      "epoch: 19, batch: 34, loss: 0.8379783630371094.\n",
      "epoch: 19, batch: 35, loss: 0.6189677119255066.\n",
      "epoch: 19, batch: 36, loss: 0.6175401210784912.\n",
      "epoch: 19, batch: 37, loss: 0.5947661995887756.\n",
      "epoch: 19, batch: 38, loss: 0.6576653718948364.\n",
      "epoch: 19, batch: 39, loss: 0.6506755352020264.\n",
      "epoch: 19, batch: 40, loss: 0.585290789604187.\n",
      "epoch: 19, batch: 41, loss: 0.6369778513908386.\n",
      "epoch: 19, batch: 42, loss: 0.7106794714927673.\n",
      "epoch: 19, batch: 43, loss: 0.6563720107078552.\n",
      "epoch: 19, batch: 44, loss: 0.5654990077018738.\n",
      "epoch: 19, batch: 45, loss: 0.5868005752563477.\n",
      "epoch: 19, batch: 46, loss: 0.6649163365364075.\n",
      "epoch: 19, batch: 47, loss: 0.6316641569137573.\n",
      "epoch: 19, batch: 48, loss: 0.6210776567459106.\n",
      "epoch: 19, batch: 49, loss: 0.5817922949790955.\n",
      "epoch: 19, batch: 50, loss: 0.7055109739303589.\n",
      "epoch: 19, batch: 51, loss: 0.7438052892684937.\n",
      "epoch: 19, batch: 52, loss: 0.698576807975769.\n",
      "epoch: 20, batch: 0, loss: 0.7141140699386597.\n",
      "epoch: 20, batch: 1, loss: 0.609967827796936.\n",
      "epoch: 20, batch: 2, loss: 0.5961959362030029.\n",
      "epoch: 20, batch: 3, loss: 0.6906614303588867.\n",
      "epoch: 20, batch: 4, loss: 0.5883958339691162.\n",
      "epoch: 20, batch: 5, loss: 0.5941506624221802.\n",
      "epoch: 20, batch: 6, loss: 0.6785528659820557.\n",
      "epoch: 20, batch: 7, loss: 0.5777761340141296.\n",
      "epoch: 20, batch: 8, loss: 0.6052199602127075.\n",
      "epoch: 20, batch: 9, loss: 0.6327203512191772.\n",
      "epoch: 20, batch: 10, loss: 0.6924691200256348.\n",
      "epoch: 20, batch: 11, loss: 0.7329821586608887.\n",
      "epoch: 20, batch: 12, loss: 0.6312572956085205.\n",
      "epoch: 20, batch: 13, loss: 0.5991256237030029.\n",
      "epoch: 20, batch: 14, loss: 0.5755903720855713.\n",
      "epoch: 20, batch: 15, loss: 0.6172463893890381.\n",
      "epoch: 20, batch: 16, loss: 0.6099987030029297.\n",
      "epoch: 20, batch: 17, loss: 0.575173020362854.\n",
      "epoch: 20, batch: 18, loss: 0.7668150663375854.\n",
      "epoch: 20, batch: 19, loss: 0.6296363472938538.\n",
      "epoch: 20, batch: 20, loss: 0.6873831152915955.\n",
      "epoch: 20, batch: 21, loss: 0.5637804865837097.\n",
      "epoch: 20, batch: 22, loss: 0.7194108963012695.\n",
      "epoch: 20, batch: 23, loss: 0.5999802350997925.\n",
      "epoch: 20, batch: 24, loss: 0.6669363975524902.\n",
      "epoch: 20, batch: 25, loss: 0.5595757961273193.\n",
      "epoch: 20, batch: 26, loss: 0.6525730490684509.\n",
      "epoch: 20, batch: 27, loss: 0.6567243337631226.\n",
      "epoch: 20, batch: 28, loss: 0.5924876928329468.\n",
      "epoch: 20, batch: 29, loss: 0.6541348099708557.\n",
      "epoch: 20, batch: 30, loss: 0.7880253195762634.\n",
      "epoch: 20, batch: 31, loss: 0.6617100238800049.\n",
      "epoch: 20, batch: 32, loss: 0.6282788515090942.\n",
      "epoch: 20, batch: 33, loss: 0.6343691349029541.\n",
      "epoch: 20, batch: 34, loss: 0.6399866342544556.\n",
      "epoch: 20, batch: 35, loss: 0.6188437938690186.\n",
      "epoch: 20, batch: 36, loss: 0.6213072538375854.\n",
      "epoch: 20, batch: 37, loss: 0.6355683207511902.\n",
      "epoch: 20, batch: 38, loss: 0.831567645072937.\n",
      "epoch: 20, batch: 39, loss: 0.6461905837059021.\n",
      "epoch: 20, batch: 40, loss: 0.6632046699523926.\n",
      "epoch: 20, batch: 41, loss: 0.7251443862915039.\n",
      "epoch: 20, batch: 42, loss: 0.6006656885147095.\n",
      "epoch: 20, batch: 43, loss: 0.7863322496414185.\n",
      "epoch: 20, batch: 44, loss: 0.6971858739852905.\n",
      "epoch: 20, batch: 45, loss: 0.7803723812103271.\n",
      "epoch: 20, batch: 46, loss: 0.5624145269393921.\n",
      "epoch: 20, batch: 47, loss: 0.5636477470397949.\n",
      "epoch: 20, batch: 48, loss: 0.6450824737548828.\n",
      "epoch: 20, batch: 49, loss: 0.5625391006469727.\n",
      "epoch: 20, batch: 50, loss: 0.683695912361145.\n",
      "epoch: 20, batch: 51, loss: 0.8932299613952637.\n",
      "epoch: 20, batch: 52, loss: 0.6757164001464844.\n",
      "epoch: 21, batch: 0, loss: 0.9197235703468323.\n",
      "epoch: 21, batch: 1, loss: 0.7273837327957153.\n",
      "epoch: 21, batch: 2, loss: 0.5973551273345947.\n",
      "epoch: 21, batch: 3, loss: 0.56423020362854.\n",
      "epoch: 21, batch: 4, loss: 0.6454586982727051.\n",
      "epoch: 21, batch: 5, loss: 0.8335057497024536.\n",
      "epoch: 21, batch: 6, loss: 0.6118183135986328.\n",
      "epoch: 21, batch: 7, loss: 0.6243399381637573.\n",
      "epoch: 21, batch: 8, loss: 0.5588051676750183.\n",
      "epoch: 21, batch: 9, loss: 0.5802786350250244.\n",
      "epoch: 21, batch: 10, loss: 0.5954481959342957.\n",
      "epoch: 21, batch: 11, loss: 0.6059340238571167.\n",
      "epoch: 21, batch: 12, loss: 0.5889869928359985.\n",
      "epoch: 21, batch: 13, loss: 0.7224447727203369.\n",
      "epoch: 21, batch: 14, loss: 0.8423473834991455.\n",
      "epoch: 21, batch: 15, loss: 0.6169012188911438.\n",
      "epoch: 21, batch: 16, loss: 0.6008382439613342.\n",
      "epoch: 21, batch: 17, loss: 0.5740768909454346.\n",
      "epoch: 21, batch: 18, loss: 0.8401433229446411.\n",
      "epoch: 21, batch: 19, loss: 0.6242753267288208.\n",
      "epoch: 21, batch: 20, loss: 0.561495840549469.\n",
      "epoch: 21, batch: 21, loss: 0.6018503308296204.\n",
      "epoch: 21, batch: 22, loss: 0.6455398797988892.\n",
      "epoch: 21, batch: 23, loss: 0.6958378553390503.\n",
      "epoch: 21, batch: 24, loss: 0.6038317680358887.\n",
      "epoch: 21, batch: 25, loss: 0.5892510414123535.\n",
      "epoch: 21, batch: 26, loss: 0.5835814476013184.\n",
      "epoch: 21, batch: 27, loss: 0.5930358171463013.\n",
      "epoch: 21, batch: 28, loss: 0.6303917169570923.\n",
      "epoch: 21, batch: 29, loss: 0.6706260442733765.\n",
      "epoch: 21, batch: 30, loss: 0.7514930963516235.\n",
      "epoch: 21, batch: 31, loss: 0.653093695640564.\n",
      "epoch: 21, batch: 32, loss: 0.6442350149154663.\n",
      "epoch: 21, batch: 33, loss: 0.5869983434677124.\n",
      "epoch: 21, batch: 34, loss: 0.7813841700553894.\n",
      "epoch: 21, batch: 35, loss: 0.6159459352493286.\n",
      "epoch: 21, batch: 36, loss: 0.6259866952896118.\n",
      "epoch: 21, batch: 37, loss: 0.6279569864273071.\n",
      "epoch: 21, batch: 38, loss: 0.5978238582611084.\n",
      "epoch: 21, batch: 39, loss: 0.6569835543632507.\n",
      "epoch: 21, batch: 40, loss: 0.8902900815010071.\n",
      "epoch: 21, batch: 41, loss: 0.651031494140625.\n",
      "epoch: 21, batch: 42, loss: 0.5921506285667419.\n",
      "epoch: 21, batch: 43, loss: 0.5864874720573425.\n",
      "epoch: 21, batch: 44, loss: 0.6572351455688477.\n",
      "epoch: 21, batch: 45, loss: 0.6056729555130005.\n",
      "epoch: 21, batch: 46, loss: 0.5764033794403076.\n",
      "epoch: 21, batch: 47, loss: 0.7045273780822754.\n",
      "epoch: 21, batch: 48, loss: 0.571779191493988.\n",
      "epoch: 21, batch: 49, loss: 0.6885806322097778.\n",
      "epoch: 21, batch: 50, loss: 0.5802893042564392.\n",
      "epoch: 21, batch: 51, loss: 0.5709452033042908.\n",
      "epoch: 21, batch: 52, loss: 0.5757133960723877.\n",
      "epoch: 22, batch: 0, loss: 0.6913881301879883.\n",
      "epoch: 22, batch: 1, loss: 0.5892760753631592.\n",
      "epoch: 22, batch: 2, loss: 0.6561427116394043.\n",
      "epoch: 22, batch: 3, loss: 0.6076866388320923.\n",
      "epoch: 22, batch: 4, loss: 0.6434547305107117.\n",
      "epoch: 22, batch: 5, loss: 0.5629928112030029.\n",
      "epoch: 22, batch: 6, loss: 0.6053240299224854.\n",
      "epoch: 22, batch: 7, loss: 0.5720911622047424.\n",
      "epoch: 22, batch: 8, loss: 0.594883143901825.\n",
      "epoch: 22, batch: 9, loss: 0.7561771869659424.\n",
      "epoch: 22, batch: 10, loss: 0.6015855073928833.\n",
      "epoch: 22, batch: 11, loss: 0.6320905685424805.\n",
      "epoch: 22, batch: 12, loss: 0.5794843435287476.\n",
      "epoch: 22, batch: 13, loss: 0.6056218147277832.\n",
      "epoch: 22, batch: 14, loss: 0.6526840925216675.\n",
      "epoch: 22, batch: 15, loss: 0.7486507892608643.\n",
      "epoch: 22, batch: 16, loss: 0.6722652912139893.\n",
      "epoch: 22, batch: 17, loss: 0.6144387722015381.\n",
      "epoch: 22, batch: 18, loss: 0.5754997730255127.\n",
      "epoch: 22, batch: 19, loss: 0.6780052781105042.\n",
      "epoch: 22, batch: 20, loss: 0.6115158796310425.\n",
      "epoch: 22, batch: 21, loss: 0.6348251104354858.\n",
      "epoch: 22, batch: 22, loss: 0.6595766544342041.\n",
      "epoch: 22, batch: 23, loss: 0.6818563938140869.\n",
      "epoch: 22, batch: 24, loss: 0.5871390104293823.\n",
      "epoch: 22, batch: 25, loss: 0.6984454393386841.\n",
      "epoch: 22, batch: 26, loss: 0.6406470537185669.\n",
      "epoch: 22, batch: 27, loss: 0.6147316694259644.\n",
      "epoch: 22, batch: 28, loss: 0.6340253949165344.\n",
      "epoch: 22, batch: 29, loss: 0.6022062301635742.\n",
      "epoch: 22, batch: 30, loss: 0.5662172436714172.\n",
      "epoch: 22, batch: 31, loss: 0.7996096014976501.\n",
      "epoch: 22, batch: 32, loss: 0.6462039947509766.\n",
      "epoch: 22, batch: 33, loss: 0.8218048810958862.\n",
      "epoch: 22, batch: 34, loss: 0.6076450943946838.\n",
      "epoch: 22, batch: 35, loss: 0.5830247402191162.\n",
      "epoch: 22, batch: 36, loss: 0.5589032173156738.\n",
      "epoch: 22, batch: 37, loss: 0.8237804174423218.\n",
      "epoch: 22, batch: 38, loss: 0.7208794355392456.\n",
      "epoch: 22, batch: 39, loss: 0.5903127193450928.\n",
      "epoch: 22, batch: 40, loss: 0.6489437818527222.\n",
      "epoch: 22, batch: 41, loss: 0.5741430521011353.\n",
      "epoch: 22, batch: 42, loss: 0.633216381072998.\n",
      "epoch: 22, batch: 43, loss: 0.561798095703125.\n",
      "epoch: 22, batch: 44, loss: 0.688788890838623.\n",
      "epoch: 22, batch: 45, loss: 0.7283633351325989.\n",
      "epoch: 22, batch: 46, loss: 0.6191784143447876.\n",
      "epoch: 22, batch: 47, loss: 0.5891599655151367.\n",
      "epoch: 22, batch: 48, loss: 0.8075824975967407.\n",
      "epoch: 22, batch: 49, loss: 0.694091796875.\n",
      "epoch: 22, batch: 50, loss: 0.5571173429489136.\n",
      "epoch: 22, batch: 51, loss: 0.6351396441459656.\n",
      "epoch: 22, batch: 52, loss: 0.5555624961853027.\n",
      "epoch: 23, batch: 0, loss: 0.6619503498077393.\n",
      "epoch: 23, batch: 1, loss: 0.6377098560333252.\n",
      "epoch: 23, batch: 2, loss: 0.6296596527099609.\n",
      "epoch: 23, batch: 3, loss: 0.600421130657196.\n",
      "epoch: 23, batch: 4, loss: 0.6308739185333252.\n",
      "epoch: 23, batch: 5, loss: 0.6009801030158997.\n",
      "epoch: 23, batch: 6, loss: 0.630327582359314.\n",
      "epoch: 23, batch: 7, loss: 0.585398256778717.\n",
      "epoch: 23, batch: 8, loss: 0.5712357759475708.\n",
      "epoch: 23, batch: 9, loss: 0.7268418073654175.\n",
      "epoch: 23, batch: 10, loss: 0.7081838846206665.\n",
      "epoch: 23, batch: 11, loss: 0.5882195234298706.\n",
      "epoch: 23, batch: 12, loss: 0.578835129737854.\n",
      "epoch: 23, batch: 13, loss: 0.5585231184959412.\n",
      "epoch: 23, batch: 14, loss: 0.7948532700538635.\n",
      "epoch: 23, batch: 15, loss: 0.604785144329071.\n",
      "epoch: 23, batch: 16, loss: 0.8325886726379395.\n",
      "epoch: 23, batch: 17, loss: 0.8845033645629883.\n",
      "epoch: 23, batch: 18, loss: 0.6949300765991211.\n",
      "epoch: 23, batch: 19, loss: 0.5801740884780884.\n",
      "epoch: 23, batch: 20, loss: 0.6274622082710266.\n",
      "epoch: 23, batch: 21, loss: 0.647832989692688.\n",
      "epoch: 23, batch: 22, loss: 0.6877940893173218.\n",
      "epoch: 23, batch: 23, loss: 0.8059030175209045.\n",
      "epoch: 23, batch: 24, loss: 0.5716613531112671.\n",
      "epoch: 23, batch: 25, loss: 0.5710620284080505.\n",
      "epoch: 23, batch: 26, loss: 0.6136256456375122.\n",
      "epoch: 23, batch: 27, loss: 0.6462603807449341.\n",
      "epoch: 23, batch: 28, loss: 0.6128944158554077.\n",
      "epoch: 23, batch: 29, loss: 0.7163933515548706.\n",
      "epoch: 23, batch: 30, loss: 0.5572293400764465.\n",
      "epoch: 23, batch: 31, loss: 0.6519675254821777.\n",
      "epoch: 23, batch: 32, loss: 0.600507915019989.\n",
      "epoch: 23, batch: 33, loss: 0.5589383840560913.\n",
      "epoch: 23, batch: 34, loss: 0.596923828125.\n",
      "epoch: 23, batch: 35, loss: 0.5860552787780762.\n",
      "epoch: 23, batch: 36, loss: 0.5566914081573486.\n",
      "epoch: 23, batch: 37, loss: 0.7104974985122681.\n",
      "epoch: 23, batch: 38, loss: 0.607214093208313.\n",
      "epoch: 23, batch: 39, loss: 0.5849184989929199.\n",
      "epoch: 23, batch: 40, loss: 0.6375155448913574.\n",
      "epoch: 23, batch: 41, loss: 0.6141394376754761.\n",
      "epoch: 23, batch: 42, loss: 0.5943607091903687.\n",
      "epoch: 23, batch: 43, loss: 0.9362209439277649.\n",
      "epoch: 23, batch: 44, loss: 0.5833128690719604.\n",
      "epoch: 23, batch: 45, loss: 0.6542177200317383.\n",
      "epoch: 23, batch: 46, loss: 0.6604728698730469.\n",
      "epoch: 23, batch: 47, loss: 0.6150435209274292.\n",
      "epoch: 23, batch: 48, loss: 0.6061050891876221.\n",
      "epoch: 23, batch: 49, loss: 0.7594636678695679.\n",
      "epoch: 23, batch: 50, loss: 0.5680197477340698.\n",
      "epoch: 23, batch: 51, loss: 0.5761812925338745.\n",
      "epoch: 23, batch: 52, loss: 0.5881108641624451.\n",
      "epoch: 24, batch: 0, loss: 0.8648892641067505.\n",
      "epoch: 24, batch: 1, loss: 0.5853000283241272.\n",
      "epoch: 24, batch: 2, loss: 0.5936964750289917.\n",
      "epoch: 24, batch: 3, loss: 0.773566722869873.\n",
      "epoch: 24, batch: 4, loss: 0.5558634996414185.\n",
      "epoch: 24, batch: 5, loss: 0.6074413061141968.\n",
      "epoch: 24, batch: 6, loss: 0.5723695158958435.\n",
      "epoch: 24, batch: 7, loss: 0.8030444979667664.\n",
      "epoch: 24, batch: 8, loss: 0.7790385484695435.\n",
      "epoch: 24, batch: 9, loss: 0.5874890089035034.\n",
      "epoch: 24, batch: 10, loss: 0.6477261781692505.\n",
      "epoch: 24, batch: 11, loss: 0.5802939534187317.\n",
      "epoch: 24, batch: 12, loss: 0.7037588357925415.\n",
      "epoch: 24, batch: 13, loss: 0.666164755821228.\n",
      "epoch: 24, batch: 14, loss: 0.5631504058837891.\n",
      "epoch: 24, batch: 15, loss: 0.7300698161125183.\n",
      "epoch: 24, batch: 16, loss: 0.5676450729370117.\n",
      "epoch: 24, batch: 17, loss: 0.7318372130393982.\n",
      "epoch: 24, batch: 18, loss: 0.5762943625450134.\n",
      "epoch: 24, batch: 19, loss: 0.6251221895217896.\n",
      "epoch: 24, batch: 20, loss: 0.555867612361908.\n",
      "epoch: 24, batch: 21, loss: 0.5736938714981079.\n",
      "epoch: 24, batch: 22, loss: 0.7225180864334106.\n",
      "epoch: 24, batch: 23, loss: 0.5870251655578613.\n",
      "epoch: 24, batch: 24, loss: 0.5889729261398315.\n",
      "epoch: 24, batch: 25, loss: 0.6104899644851685.\n",
      "epoch: 24, batch: 26, loss: 0.572242259979248.\n",
      "epoch: 24, batch: 27, loss: 0.5705205202102661.\n",
      "epoch: 24, batch: 28, loss: 0.5600906014442444.\n",
      "epoch: 24, batch: 29, loss: 0.6473056077957153.\n",
      "epoch: 24, batch: 30, loss: 0.5607539415359497.\n",
      "epoch: 24, batch: 31, loss: 0.5926055908203125.\n",
      "epoch: 24, batch: 32, loss: 0.6663394570350647.\n",
      "epoch: 24, batch: 33, loss: 0.5568311214447021.\n",
      "epoch: 24, batch: 34, loss: 0.6083173155784607.\n",
      "epoch: 24, batch: 35, loss: 0.5707612037658691.\n",
      "epoch: 24, batch: 36, loss: 0.6261827945709229.\n",
      "epoch: 24, batch: 37, loss: 0.6588245034217834.\n",
      "epoch: 24, batch: 38, loss: 0.5782498121261597.\n",
      "epoch: 24, batch: 39, loss: 0.5964186787605286.\n",
      "epoch: 24, batch: 40, loss: 0.5632355809211731.\n",
      "epoch: 24, batch: 41, loss: 0.6168990135192871.\n",
      "epoch: 24, batch: 42, loss: 0.7441271543502808.\n",
      "epoch: 24, batch: 43, loss: 0.7310033440589905.\n",
      "epoch: 24, batch: 44, loss: 0.6136230230331421.\n",
      "epoch: 24, batch: 45, loss: 0.8340917825698853.\n",
      "epoch: 24, batch: 46, loss: 0.5739808082580566.\n",
      "epoch: 24, batch: 47, loss: 0.5997401475906372.\n",
      "epoch: 24, batch: 48, loss: 0.7874807119369507.\n",
      "epoch: 24, batch: 49, loss: 0.630600094795227.\n",
      "epoch: 24, batch: 50, loss: 0.5828665494918823.\n",
      "epoch: 24, batch: 51, loss: 0.5914006233215332.\n",
      "epoch: 24, batch: 52, loss: 0.698222815990448.\n",
      "epoch: 25, batch: 0, loss: 0.626038670539856.\n",
      "epoch: 25, batch: 1, loss: 0.6093667149543762.\n",
      "epoch: 25, batch: 2, loss: 0.667090892791748.\n",
      "epoch: 25, batch: 3, loss: 0.7883104085922241.\n",
      "epoch: 25, batch: 4, loss: 0.5785967111587524.\n",
      "epoch: 25, batch: 5, loss: 0.6237188577651978.\n",
      "epoch: 25, batch: 6, loss: 0.5726367235183716.\n",
      "epoch: 25, batch: 7, loss: 0.5940730571746826.\n",
      "epoch: 25, batch: 8, loss: 0.728182315826416.\n",
      "epoch: 25, batch: 9, loss: 0.626074492931366.\n",
      "epoch: 25, batch: 10, loss: 0.6261801719665527.\n",
      "epoch: 25, batch: 11, loss: 0.5587866306304932.\n",
      "epoch: 25, batch: 12, loss: 0.7649617195129395.\n",
      "epoch: 25, batch: 13, loss: 0.5843175649642944.\n",
      "epoch: 25, batch: 14, loss: 0.5929875373840332.\n",
      "epoch: 25, batch: 15, loss: 0.5734549164772034.\n",
      "epoch: 25, batch: 16, loss: 0.6981889009475708.\n",
      "epoch: 25, batch: 17, loss: 0.5652320384979248.\n",
      "epoch: 25, batch: 18, loss: 0.6798734068870544.\n",
      "epoch: 25, batch: 19, loss: 0.5777277946472168.\n",
      "epoch: 25, batch: 20, loss: 0.6959096193313599.\n",
      "epoch: 25, batch: 21, loss: 0.6401902437210083.\n",
      "epoch: 25, batch: 22, loss: 0.5683590173721313.\n",
      "epoch: 25, batch: 23, loss: 0.8497816920280457.\n",
      "epoch: 25, batch: 24, loss: 0.6354151964187622.\n",
      "epoch: 25, batch: 25, loss: 0.583943247795105.\n",
      "epoch: 25, batch: 26, loss: 0.5657695531845093.\n",
      "epoch: 25, batch: 27, loss: 0.6283239126205444.\n",
      "epoch: 25, batch: 28, loss: 0.572392463684082.\n",
      "epoch: 25, batch: 29, loss: 0.5611535310745239.\n",
      "epoch: 25, batch: 30, loss: 0.5638132095336914.\n",
      "epoch: 25, batch: 31, loss: 0.5988591909408569.\n",
      "epoch: 25, batch: 32, loss: 0.6241557002067566.\n",
      "epoch: 25, batch: 33, loss: 0.6221087574958801.\n",
      "epoch: 25, batch: 34, loss: 0.615703284740448.\n",
      "epoch: 25, batch: 35, loss: 0.5681211948394775.\n",
      "epoch: 25, batch: 36, loss: 0.7092190980911255.\n",
      "epoch: 25, batch: 37, loss: 0.6315820217132568.\n",
      "epoch: 25, batch: 38, loss: 0.595099687576294.\n",
      "epoch: 25, batch: 39, loss: 0.8229546546936035.\n",
      "epoch: 25, batch: 40, loss: 0.5893859267234802.\n",
      "epoch: 25, batch: 41, loss: 0.7284125089645386.\n",
      "epoch: 25, batch: 42, loss: 0.5745849609375.\n",
      "epoch: 25, batch: 43, loss: 0.7708103656768799.\n",
      "epoch: 25, batch: 44, loss: 0.6089979410171509.\n",
      "epoch: 25, batch: 45, loss: 0.589474081993103.\n",
      "epoch: 25, batch: 46, loss: 0.5613446235656738.\n",
      "epoch: 25, batch: 47, loss: 0.5697956085205078.\n",
      "epoch: 25, batch: 48, loss: 0.5917211771011353.\n",
      "epoch: 25, batch: 49, loss: 0.5920066833496094.\n",
      "epoch: 25, batch: 50, loss: 0.556686282157898.\n",
      "epoch: 25, batch: 51, loss: 0.7002044916152954.\n",
      "epoch: 25, batch: 52, loss: 0.5886139869689941.\n",
      "epoch: 26, batch: 0, loss: 0.5702095031738281.\n",
      "epoch: 26, batch: 1, loss: 0.6958720684051514.\n",
      "epoch: 26, batch: 2, loss: 0.5799543857574463.\n",
      "epoch: 26, batch: 3, loss: 0.7254943251609802.\n",
      "epoch: 26, batch: 4, loss: 0.6897884011268616.\n",
      "epoch: 26, batch: 5, loss: 0.6720016002655029.\n",
      "epoch: 26, batch: 6, loss: 0.5977940559387207.\n",
      "epoch: 26, batch: 7, loss: 0.5603716373443604.\n",
      "epoch: 26, batch: 8, loss: 0.5877343416213989.\n",
      "epoch: 26, batch: 9, loss: 0.5683871507644653.\n",
      "epoch: 26, batch: 10, loss: 0.6772990226745605.\n",
      "epoch: 26, batch: 11, loss: 0.760522186756134.\n",
      "epoch: 26, batch: 12, loss: 0.6324905157089233.\n",
      "epoch: 26, batch: 13, loss: 0.5812990665435791.\n",
      "epoch: 26, batch: 14, loss: 0.5676921010017395.\n",
      "epoch: 26, batch: 15, loss: 0.557594358921051.\n",
      "epoch: 26, batch: 16, loss: 0.5759573578834534.\n",
      "epoch: 26, batch: 17, loss: 0.5642935037612915.\n",
      "epoch: 26, batch: 18, loss: 0.574487566947937.\n",
      "epoch: 26, batch: 19, loss: 0.6004742383956909.\n",
      "epoch: 26, batch: 20, loss: 0.8684066534042358.\n",
      "epoch: 26, batch: 21, loss: 0.5974030494689941.\n",
      "epoch: 26, batch: 22, loss: 0.6565828323364258.\n",
      "epoch: 26, batch: 23, loss: 0.5830956697463989.\n",
      "epoch: 26, batch: 24, loss: 0.7575122714042664.\n",
      "epoch: 26, batch: 25, loss: 0.5914425849914551.\n",
      "epoch: 26, batch: 26, loss: 0.569016695022583.\n",
      "epoch: 26, batch: 27, loss: 0.7462014555931091.\n",
      "epoch: 26, batch: 28, loss: 0.5667537450790405.\n",
      "epoch: 26, batch: 29, loss: 0.5860180854797363.\n",
      "epoch: 26, batch: 30, loss: 0.5747833251953125.\n",
      "epoch: 26, batch: 31, loss: 0.7096678018569946.\n",
      "epoch: 26, batch: 32, loss: 0.6105010509490967.\n",
      "epoch: 26, batch: 33, loss: 0.597188413143158.\n",
      "epoch: 26, batch: 34, loss: 0.6004757881164551.\n",
      "epoch: 26, batch: 35, loss: 0.6098566651344299.\n",
      "epoch: 26, batch: 36, loss: 0.5873667001724243.\n",
      "epoch: 26, batch: 37, loss: 0.6123284101486206.\n",
      "epoch: 26, batch: 38, loss: 0.7362073659896851.\n",
      "epoch: 26, batch: 39, loss: 0.5692062973976135.\n",
      "epoch: 26, batch: 40, loss: 0.5813889503479004.\n",
      "epoch: 26, batch: 41, loss: 0.5548245310783386.\n",
      "epoch: 26, batch: 42, loss: 0.6037731170654297.\n",
      "epoch: 26, batch: 43, loss: 0.5652322769165039.\n",
      "epoch: 26, batch: 44, loss: 0.5879308581352234.\n",
      "epoch: 26, batch: 45, loss: 0.6712765693664551.\n",
      "epoch: 26, batch: 46, loss: 0.5799429416656494.\n",
      "epoch: 26, batch: 47, loss: 0.7310937643051147.\n",
      "epoch: 26, batch: 48, loss: 0.5569588541984558.\n",
      "epoch: 26, batch: 49, loss: 0.677483081817627.\n",
      "epoch: 26, batch: 50, loss: 0.7437773942947388.\n",
      "epoch: 26, batch: 51, loss: 0.6702138185501099.\n",
      "epoch: 26, batch: 52, loss: 0.7785762548446655.\n",
      "epoch: 27, batch: 0, loss: 0.6187645196914673.\n",
      "epoch: 27, batch: 1, loss: 0.5747793912887573.\n",
      "epoch: 27, batch: 2, loss: 0.5988242626190186.\n",
      "epoch: 27, batch: 3, loss: 0.611686110496521.\n",
      "epoch: 27, batch: 4, loss: 0.723934531211853.\n",
      "epoch: 27, batch: 5, loss: 0.5846343040466309.\n",
      "epoch: 27, batch: 6, loss: 0.6234416961669922.\n",
      "epoch: 27, batch: 7, loss: 0.5871297121047974.\n",
      "epoch: 27, batch: 8, loss: 0.6682341694831848.\n",
      "epoch: 27, batch: 9, loss: 0.6928703188896179.\n",
      "epoch: 27, batch: 10, loss: 0.6933912038803101.\n",
      "epoch: 27, batch: 11, loss: 0.58077472448349.\n",
      "epoch: 27, batch: 12, loss: 0.5706043839454651.\n",
      "epoch: 27, batch: 13, loss: 0.5614092350006104.\n",
      "epoch: 27, batch: 14, loss: 0.5552899241447449.\n",
      "epoch: 27, batch: 15, loss: 0.5753750205039978.\n",
      "epoch: 27, batch: 16, loss: 0.5905942320823669.\n",
      "epoch: 27, batch: 17, loss: 0.7818788290023804.\n",
      "epoch: 27, batch: 18, loss: 0.5775855779647827.\n",
      "epoch: 27, batch: 19, loss: 0.8475199341773987.\n",
      "epoch: 27, batch: 20, loss: 0.5878309011459351.\n",
      "epoch: 27, batch: 21, loss: 0.7300601601600647.\n",
      "epoch: 27, batch: 22, loss: 0.6976104378700256.\n",
      "epoch: 27, batch: 23, loss: 0.6111464500427246.\n",
      "epoch: 27, batch: 24, loss: 0.5775607824325562.\n",
      "epoch: 27, batch: 25, loss: 0.576877236366272.\n",
      "epoch: 27, batch: 26, loss: 0.5735539793968201.\n",
      "epoch: 27, batch: 27, loss: 0.5571380853652954.\n",
      "epoch: 27, batch: 28, loss: 0.5777842998504639.\n",
      "epoch: 27, batch: 29, loss: 0.5682448148727417.\n",
      "epoch: 27, batch: 30, loss: 0.6070488691329956.\n",
      "epoch: 27, batch: 31, loss: 0.5670815110206604.\n",
      "epoch: 27, batch: 32, loss: 0.8517004251480103.\n",
      "epoch: 27, batch: 33, loss: 0.5747877359390259.\n",
      "epoch: 27, batch: 34, loss: 0.589819073677063.\n",
      "epoch: 27, batch: 35, loss: 0.5631635785102844.\n",
      "epoch: 27, batch: 36, loss: 0.6718094348907471.\n",
      "epoch: 27, batch: 37, loss: 0.5725570917129517.\n",
      "epoch: 27, batch: 38, loss: 0.6877618432044983.\n",
      "epoch: 27, batch: 39, loss: 0.5979468822479248.\n",
      "epoch: 27, batch: 40, loss: 0.573224663734436.\n",
      "epoch: 27, batch: 41, loss: 0.5788604021072388.\n",
      "epoch: 27, batch: 42, loss: 0.6541855335235596.\n",
      "epoch: 27, batch: 43, loss: 0.8627781271934509.\n",
      "epoch: 27, batch: 44, loss: 0.5633059740066528.\n",
      "epoch: 27, batch: 45, loss: 0.5681310892105103.\n",
      "epoch: 27, batch: 46, loss: 0.5556180477142334.\n",
      "epoch: 27, batch: 47, loss: 0.5615867376327515.\n",
      "epoch: 27, batch: 48, loss: 0.6485632658004761.\n",
      "epoch: 27, batch: 49, loss: 0.5836129784584045.\n",
      "epoch: 27, batch: 50, loss: 0.5908544063568115.\n",
      "epoch: 27, batch: 51, loss: 0.6412696838378906.\n",
      "epoch: 27, batch: 52, loss: 0.5777991414070129.\n",
      "epoch: 28, batch: 0, loss: 0.6214674711227417.\n",
      "epoch: 28, batch: 1, loss: 0.5554056763648987.\n",
      "epoch: 28, batch: 2, loss: 0.5684177875518799.\n",
      "epoch: 28, batch: 3, loss: 0.5632030963897705.\n",
      "epoch: 28, batch: 4, loss: 0.6346065402030945.\n",
      "epoch: 28, batch: 5, loss: 0.5993391871452332.\n",
      "epoch: 28, batch: 6, loss: 0.6023167371749878.\n",
      "epoch: 28, batch: 7, loss: 0.5963228940963745.\n",
      "epoch: 28, batch: 8, loss: 0.6583988666534424.\n",
      "epoch: 28, batch: 9, loss: 0.7219727039337158.\n",
      "epoch: 28, batch: 10, loss: 0.563632071018219.\n",
      "epoch: 28, batch: 11, loss: 0.6224852800369263.\n",
      "epoch: 28, batch: 12, loss: 0.580309271812439.\n",
      "epoch: 28, batch: 13, loss: 0.5736472010612488.\n",
      "epoch: 28, batch: 14, loss: 0.7047963738441467.\n",
      "epoch: 28, batch: 15, loss: 0.6736003160476685.\n",
      "epoch: 28, batch: 16, loss: 0.6112551093101501.\n",
      "epoch: 28, batch: 17, loss: 0.6250360012054443.\n",
      "epoch: 28, batch: 18, loss: 0.7014782428741455.\n",
      "epoch: 28, batch: 19, loss: 0.5923579931259155.\n",
      "epoch: 28, batch: 20, loss: 0.7763528823852539.\n",
      "epoch: 28, batch: 21, loss: 0.6509981751441956.\n",
      "epoch: 28, batch: 22, loss: 0.5643608570098877.\n",
      "epoch: 28, batch: 23, loss: 0.5663327574729919.\n",
      "epoch: 28, batch: 24, loss: 0.6189684867858887.\n",
      "epoch: 28, batch: 25, loss: 0.5580897331237793.\n",
      "epoch: 28, batch: 26, loss: 0.5797780752182007.\n",
      "epoch: 28, batch: 27, loss: 0.5988919734954834.\n",
      "epoch: 28, batch: 28, loss: 0.67545485496521.\n",
      "epoch: 28, batch: 29, loss: 0.580432653427124.\n",
      "epoch: 28, batch: 30, loss: 0.5676131248474121.\n",
      "epoch: 28, batch: 31, loss: 0.6357085108757019.\n",
      "epoch: 28, batch: 32, loss: 0.5666013956069946.\n",
      "epoch: 28, batch: 33, loss: 0.6962487101554871.\n",
      "epoch: 28, batch: 34, loss: 0.7706388235092163.\n",
      "epoch: 28, batch: 35, loss: 0.5988531112670898.\n",
      "epoch: 28, batch: 36, loss: 0.5576011538505554.\n",
      "epoch: 28, batch: 37, loss: 0.5601158142089844.\n",
      "epoch: 28, batch: 38, loss: 0.6017981171607971.\n",
      "epoch: 28, batch: 39, loss: 0.6013974547386169.\n",
      "epoch: 28, batch: 40, loss: 0.5708200931549072.\n",
      "epoch: 28, batch: 41, loss: 0.8008083701133728.\n",
      "epoch: 28, batch: 42, loss: 0.607642412185669.\n",
      "epoch: 28, batch: 43, loss: 0.6120204329490662.\n",
      "epoch: 28, batch: 44, loss: 0.5946917533874512.\n",
      "epoch: 28, batch: 45, loss: 0.5616453886032104.\n",
      "epoch: 28, batch: 46, loss: 1.0960639715194702.\n",
      "epoch: 28, batch: 47, loss: 0.5773210525512695.\n",
      "epoch: 28, batch: 48, loss: 0.6095672845840454.\n",
      "epoch: 28, batch: 49, loss: 0.5606924295425415.\n",
      "epoch: 28, batch: 50, loss: 0.5555353164672852.\n",
      "epoch: 28, batch: 51, loss: 0.5755821466445923.\n",
      "epoch: 28, batch: 52, loss: 0.5557332634925842.\n",
      "epoch: 29, batch: 0, loss: 0.6507109999656677.\n",
      "epoch: 29, batch: 1, loss: 0.6631671190261841.\n",
      "epoch: 29, batch: 2, loss: 0.5599936246871948.\n",
      "epoch: 29, batch: 3, loss: 0.6926655173301697.\n",
      "epoch: 29, batch: 4, loss: 0.7266983985900879.\n",
      "epoch: 29, batch: 5, loss: 0.5579546093940735.\n",
      "epoch: 29, batch: 6, loss: 0.5681139826774597.\n",
      "epoch: 29, batch: 7, loss: 0.5664044618606567.\n",
      "epoch: 29, batch: 8, loss: 0.5551108121871948.\n",
      "epoch: 29, batch: 9, loss: 0.6626025438308716.\n",
      "epoch: 29, batch: 10, loss: 0.5590380430221558.\n",
      "epoch: 29, batch: 11, loss: 0.567440390586853.\n",
      "epoch: 29, batch: 12, loss: 0.7193793654441833.\n",
      "epoch: 29, batch: 13, loss: 0.5619510412216187.\n",
      "epoch: 29, batch: 14, loss: 0.5716962218284607.\n",
      "epoch: 29, batch: 15, loss: 0.5689756870269775.\n",
      "epoch: 29, batch: 16, loss: 0.7387916445732117.\n",
      "epoch: 29, batch: 17, loss: 0.5920900702476501.\n",
      "epoch: 29, batch: 18, loss: 0.5741541385650635.\n",
      "epoch: 29, batch: 19, loss: 0.5593919157981873.\n",
      "epoch: 29, batch: 20, loss: 0.5864561796188354.\n",
      "epoch: 29, batch: 21, loss: 0.6536120772361755.\n",
      "epoch: 29, batch: 22, loss: 0.5567049384117126.\n",
      "epoch: 29, batch: 23, loss: 0.5632933378219604.\n",
      "epoch: 29, batch: 24, loss: 0.5655460357666016.\n",
      "epoch: 29, batch: 25, loss: 0.7716277837753296.\n",
      "epoch: 29, batch: 26, loss: 0.6049914956092834.\n",
      "epoch: 29, batch: 27, loss: 0.6691064238548279.\n",
      "epoch: 29, batch: 28, loss: 0.5919897556304932.\n",
      "epoch: 29, batch: 29, loss: 0.5727804899215698.\n",
      "epoch: 29, batch: 30, loss: 0.7639372944831848.\n",
      "epoch: 29, batch: 31, loss: 0.7827680110931396.\n",
      "epoch: 29, batch: 32, loss: 0.5781049728393555.\n",
      "epoch: 29, batch: 33, loss: 0.5981842279434204.\n",
      "epoch: 29, batch: 34, loss: 0.5666371583938599.\n",
      "epoch: 29, batch: 35, loss: 0.8642616868019104.\n",
      "epoch: 29, batch: 36, loss: 0.6071410179138184.\n",
      "epoch: 29, batch: 37, loss: 0.5932241678237915.\n",
      "epoch: 29, batch: 38, loss: 0.568513810634613.\n",
      "epoch: 29, batch: 39, loss: 0.5799329280853271.\n",
      "epoch: 29, batch: 40, loss: 0.6371833682060242.\n",
      "epoch: 29, batch: 41, loss: 0.5713204145431519.\n",
      "epoch: 29, batch: 42, loss: 0.5785397291183472.\n",
      "epoch: 29, batch: 43, loss: 0.649131178855896.\n",
      "epoch: 29, batch: 44, loss: 0.8841122388839722.\n",
      "epoch: 29, batch: 45, loss: 0.6150224208831787.\n",
      "epoch: 29, batch: 46, loss: 0.575006365776062.\n",
      "epoch: 29, batch: 47, loss: 0.5848647356033325.\n",
      "epoch: 29, batch: 48, loss: 0.5741382837295532.\n",
      "epoch: 29, batch: 49, loss: 0.5582118034362793.\n",
      "epoch: 29, batch: 50, loss: 0.5867511034011841.\n",
      "epoch: 29, batch: 51, loss: 0.56731116771698.\n",
      "epoch: 29, batch: 52, loss: 0.6297924518585205.\n",
      "epoch: 30, batch: 0, loss: 0.5688117146492004.\n",
      "epoch: 30, batch: 1, loss: 0.5957645177841187.\n",
      "epoch: 30, batch: 2, loss: 0.5626958012580872.\n",
      "epoch: 30, batch: 3, loss: 0.5688204765319824.\n",
      "epoch: 30, batch: 4, loss: 0.6374185681343079.\n",
      "epoch: 30, batch: 5, loss: 0.5726876258850098.\n",
      "epoch: 30, batch: 6, loss: 0.5785713791847229.\n",
      "epoch: 30, batch: 7, loss: 0.5597348213195801.\n",
      "epoch: 30, batch: 8, loss: 0.5733990669250488.\n",
      "epoch: 30, batch: 9, loss: 0.5627357959747314.\n",
      "epoch: 30, batch: 10, loss: 0.565387487411499.\n",
      "epoch: 30, batch: 11, loss: 0.5584848523139954.\n",
      "epoch: 30, batch: 12, loss: 0.5568419694900513.\n",
      "epoch: 30, batch: 13, loss: 0.5689305067062378.\n",
      "epoch: 30, batch: 14, loss: 0.5715488195419312.\n",
      "epoch: 30, batch: 15, loss: 0.6370987296104431.\n",
      "epoch: 30, batch: 16, loss: 0.5751200914382935.\n",
      "epoch: 30, batch: 17, loss: 0.6038608551025391.\n",
      "epoch: 30, batch: 18, loss: 0.7059869766235352.\n",
      "epoch: 30, batch: 19, loss: 0.6417117118835449.\n",
      "epoch: 30, batch: 20, loss: 0.6990994215011597.\n",
      "epoch: 30, batch: 21, loss: 0.8517439365386963.\n",
      "epoch: 30, batch: 22, loss: 0.5679855346679688.\n",
      "epoch: 30, batch: 23, loss: 0.5679013133049011.\n",
      "epoch: 30, batch: 24, loss: 0.6170746684074402.\n",
      "epoch: 30, batch: 25, loss: 0.5947967767715454.\n",
      "epoch: 30, batch: 26, loss: 0.7427457571029663.\n",
      "epoch: 30, batch: 27, loss: 0.5841772556304932.\n",
      "epoch: 30, batch: 28, loss: 0.6010916233062744.\n",
      "epoch: 30, batch: 29, loss: 0.5856651663780212.\n",
      "epoch: 30, batch: 30, loss: 0.5775272846221924.\n",
      "epoch: 30, batch: 31, loss: 0.5712407827377319.\n",
      "epoch: 30, batch: 32, loss: 0.6056617498397827.\n",
      "epoch: 30, batch: 33, loss: 0.7017004489898682.\n",
      "epoch: 30, batch: 34, loss: 0.5551947951316833.\n",
      "epoch: 30, batch: 35, loss: 0.5761594772338867.\n",
      "epoch: 30, batch: 36, loss: 0.7793364524841309.\n",
      "epoch: 30, batch: 37, loss: 0.5754153728485107.\n",
      "epoch: 30, batch: 38, loss: 0.5655465722084045.\n",
      "epoch: 30, batch: 39, loss: 0.6229590177536011.\n",
      "epoch: 30, batch: 40, loss: 0.5642310380935669.\n",
      "epoch: 30, batch: 41, loss: 0.8064440488815308.\n",
      "epoch: 30, batch: 42, loss: 0.6479208469390869.\n",
      "epoch: 30, batch: 43, loss: 0.5549278855323792.\n",
      "epoch: 30, batch: 44, loss: 0.5717347264289856.\n",
      "epoch: 30, batch: 45, loss: 0.8689607977867126.\n",
      "epoch: 30, batch: 46, loss: 0.62822425365448.\n",
      "epoch: 30, batch: 47, loss: 0.5778690576553345.\n",
      "epoch: 30, batch: 48, loss: 0.7122621536254883.\n",
      "epoch: 30, batch: 49, loss: 0.5726875066757202.\n",
      "epoch: 30, batch: 50, loss: 0.5553593635559082.\n",
      "epoch: 30, batch: 51, loss: 0.5807141065597534.\n",
      "epoch: 30, batch: 52, loss: 0.6311845779418945.\n",
      "epoch: 31, batch: 0, loss: 0.556892991065979.\n",
      "epoch: 31, batch: 1, loss: 0.5580634474754333.\n",
      "epoch: 31, batch: 2, loss: 0.6034554243087769.\n",
      "epoch: 31, batch: 3, loss: 0.5870991945266724.\n",
      "epoch: 31, batch: 4, loss: 0.57768714427948.\n",
      "epoch: 31, batch: 5, loss: 0.5760437250137329.\n",
      "epoch: 31, batch: 6, loss: 0.5630311965942383.\n",
      "epoch: 31, batch: 7, loss: 0.7771227955818176.\n",
      "epoch: 31, batch: 8, loss: 0.6893348693847656.\n",
      "epoch: 31, batch: 9, loss: 0.5789094567298889.\n",
      "epoch: 31, batch: 10, loss: 0.5624643564224243.\n",
      "epoch: 31, batch: 11, loss: 0.5728331804275513.\n",
      "epoch: 31, batch: 12, loss: 0.5760427713394165.\n",
      "epoch: 31, batch: 13, loss: 0.6346575617790222.\n",
      "epoch: 31, batch: 14, loss: 0.7016262412071228.\n",
      "epoch: 31, batch: 15, loss: 0.6159266233444214.\n",
      "epoch: 31, batch: 16, loss: 0.7241777181625366.\n",
      "epoch: 31, batch: 17, loss: 0.644577145576477.\n",
      "epoch: 31, batch: 18, loss: 0.5601948499679565.\n",
      "epoch: 31, batch: 19, loss: 0.6201472282409668.\n",
      "epoch: 31, batch: 20, loss: 0.5962687134742737.\n",
      "epoch: 31, batch: 21, loss: 0.6422412991523743.\n",
      "epoch: 31, batch: 22, loss: 0.7220964431762695.\n",
      "epoch: 31, batch: 23, loss: 0.5958613753318787.\n",
      "epoch: 31, batch: 24, loss: 0.5601578950881958.\n",
      "epoch: 31, batch: 25, loss: 0.5995197296142578.\n",
      "epoch: 31, batch: 26, loss: 0.5566538572311401.\n",
      "epoch: 31, batch: 27, loss: 0.5620205998420715.\n",
      "epoch: 31, batch: 28, loss: 0.5581357479095459.\n",
      "epoch: 31, batch: 29, loss: 0.5793994069099426.\n",
      "epoch: 31, batch: 30, loss: 0.5608217120170593.\n",
      "epoch: 31, batch: 31, loss: 0.576077938079834.\n",
      "epoch: 31, batch: 32, loss: 0.6440204381942749.\n",
      "epoch: 31, batch: 33, loss: 0.5634872317314148.\n",
      "epoch: 31, batch: 34, loss: 0.6796384453773499.\n",
      "epoch: 31, batch: 35, loss: 0.5644993782043457.\n",
      "epoch: 31, batch: 36, loss: 0.8335491418838501.\n",
      "epoch: 31, batch: 37, loss: 0.5716456770896912.\n",
      "epoch: 31, batch: 38, loss: 0.562650203704834.\n",
      "epoch: 31, batch: 39, loss: 0.691063642501831.\n",
      "epoch: 31, batch: 40, loss: 0.6541227102279663.\n",
      "epoch: 31, batch: 41, loss: 0.5700694918632507.\n",
      "epoch: 31, batch: 42, loss: 0.6721843481063843.\n",
      "epoch: 31, batch: 43, loss: 0.5684782266616821.\n",
      "epoch: 31, batch: 44, loss: 0.6221769452095032.\n",
      "epoch: 31, batch: 45, loss: 0.5566946268081665.\n",
      "epoch: 31, batch: 46, loss: 0.5527474880218506.\n",
      "epoch: 31, batch: 47, loss: 0.6514899730682373.\n",
      "epoch: 31, batch: 48, loss: 0.6950624585151672.\n",
      "epoch: 31, batch: 49, loss: 0.569778561592102.\n",
      "epoch: 31, batch: 50, loss: 0.6596279144287109.\n",
      "epoch: 31, batch: 51, loss: 0.5642070770263672.\n",
      "epoch: 31, batch: 52, loss: 0.5995153784751892.\n",
      "epoch: 32, batch: 0, loss: 0.5674059391021729.\n",
      "epoch: 32, batch: 1, loss: 0.5767306089401245.\n",
      "epoch: 32, batch: 2, loss: 0.6017076969146729.\n",
      "epoch: 32, batch: 3, loss: 0.5753852725028992.\n",
      "epoch: 32, batch: 4, loss: 0.5611296892166138.\n",
      "epoch: 32, batch: 5, loss: 0.5649330615997314.\n",
      "epoch: 32, batch: 6, loss: 0.5621058940887451.\n",
      "epoch: 32, batch: 7, loss: 0.6481454372406006.\n",
      "epoch: 32, batch: 8, loss: 0.568687915802002.\n",
      "epoch: 32, batch: 9, loss: 0.7906556129455566.\n",
      "epoch: 32, batch: 10, loss: 0.5540131330490112.\n",
      "epoch: 32, batch: 11, loss: 0.6148191690444946.\n",
      "epoch: 32, batch: 12, loss: 0.7627520561218262.\n",
      "epoch: 32, batch: 13, loss: 0.562366247177124.\n",
      "epoch: 32, batch: 14, loss: 0.5654403567314148.\n",
      "epoch: 32, batch: 15, loss: 0.5937729477882385.\n",
      "epoch: 32, batch: 16, loss: 0.725427508354187.\n",
      "epoch: 32, batch: 17, loss: 0.5540335774421692.\n",
      "epoch: 32, batch: 18, loss: 0.5607593059539795.\n",
      "epoch: 32, batch: 19, loss: 0.8312286138534546.\n",
      "epoch: 32, batch: 20, loss: 0.6517646312713623.\n",
      "epoch: 32, batch: 21, loss: 0.5651780366897583.\n",
      "epoch: 32, batch: 22, loss: 0.5655595064163208.\n",
      "epoch: 32, batch: 23, loss: 0.5708553791046143.\n",
      "epoch: 32, batch: 24, loss: 0.733054518699646.\n",
      "epoch: 32, batch: 25, loss: 0.5610866546630859.\n",
      "epoch: 32, batch: 26, loss: 0.5600664615631104.\n",
      "epoch: 32, batch: 27, loss: 0.5880786180496216.\n",
      "epoch: 32, batch: 28, loss: 0.7349023818969727.\n",
      "epoch: 32, batch: 29, loss: 0.5915624499320984.\n",
      "epoch: 32, batch: 30, loss: 0.5528583526611328.\n",
      "epoch: 32, batch: 31, loss: 0.5790311098098755.\n",
      "epoch: 32, batch: 32, loss: 0.562010645866394.\n",
      "epoch: 32, batch: 33, loss: 0.879207193851471.\n",
      "epoch: 32, batch: 34, loss: 0.5535606145858765.\n",
      "epoch: 32, batch: 35, loss: 0.6310622096061707.\n",
      "epoch: 32, batch: 36, loss: 0.5945051312446594.\n",
      "epoch: 32, batch: 37, loss: 0.5599508285522461.\n",
      "epoch: 32, batch: 38, loss: 0.5590050220489502.\n",
      "epoch: 32, batch: 39, loss: 0.7645597457885742.\n",
      "epoch: 32, batch: 40, loss: 0.5894343256950378.\n",
      "epoch: 32, batch: 41, loss: 0.5674924850463867.\n",
      "epoch: 32, batch: 42, loss: 0.6058801412582397.\n",
      "epoch: 32, batch: 43, loss: 0.564216673374176.\n",
      "epoch: 32, batch: 44, loss: 0.559914231300354.\n",
      "epoch: 32, batch: 45, loss: 0.5586410164833069.\n",
      "epoch: 32, batch: 46, loss: 0.6526907682418823.\n",
      "epoch: 32, batch: 47, loss: 0.6281962394714355.\n",
      "epoch: 32, batch: 48, loss: 0.5773240327835083.\n",
      "epoch: 32, batch: 49, loss: 0.7219772934913635.\n",
      "epoch: 32, batch: 50, loss: 0.6178210973739624.\n",
      "epoch: 32, batch: 51, loss: 0.5884121656417847.\n",
      "epoch: 32, batch: 52, loss: 0.5749828815460205.\n",
      "epoch: 33, batch: 0, loss: 0.5572967529296875.\n",
      "epoch: 33, batch: 1, loss: 0.5997477769851685.\n",
      "epoch: 33, batch: 2, loss: 0.6076869964599609.\n",
      "epoch: 33, batch: 3, loss: 0.5622552633285522.\n",
      "epoch: 33, batch: 4, loss: 0.6748685836791992.\n",
      "epoch: 33, batch: 5, loss: 0.6631791591644287.\n",
      "epoch: 33, batch: 6, loss: 0.5589772462844849.\n",
      "epoch: 33, batch: 7, loss: 0.8466460704803467.\n",
      "epoch: 33, batch: 8, loss: 0.5613391399383545.\n",
      "epoch: 33, batch: 9, loss: 0.5572504997253418.\n",
      "epoch: 33, batch: 10, loss: 0.5688548684120178.\n",
      "epoch: 33, batch: 11, loss: 0.8340240716934204.\n",
      "epoch: 33, batch: 12, loss: 0.566206693649292.\n",
      "epoch: 33, batch: 13, loss: 0.640923023223877.\n",
      "epoch: 33, batch: 14, loss: 0.6190313100814819.\n",
      "epoch: 33, batch: 15, loss: 0.57187819480896.\n",
      "epoch: 33, batch: 16, loss: 0.5705573558807373.\n",
      "epoch: 33, batch: 17, loss: 0.5714665651321411.\n",
      "epoch: 33, batch: 18, loss: 0.7617477178573608.\n",
      "epoch: 33, batch: 19, loss: 0.6173442006111145.\n",
      "epoch: 33, batch: 20, loss: 0.570995032787323.\n",
      "epoch: 33, batch: 21, loss: 0.57499760389328.\n",
      "epoch: 33, batch: 22, loss: 0.6846951246261597.\n",
      "epoch: 33, batch: 23, loss: 0.5629085302352905.\n",
      "epoch: 33, batch: 24, loss: 0.6525605916976929.\n",
      "epoch: 33, batch: 25, loss: 0.5749483108520508.\n",
      "epoch: 33, batch: 26, loss: 0.5736686587333679.\n",
      "epoch: 33, batch: 27, loss: 0.5732458829879761.\n",
      "epoch: 33, batch: 28, loss: 0.6427711248397827.\n",
      "epoch: 33, batch: 29, loss: 0.5605787038803101.\n",
      "epoch: 33, batch: 30, loss: 0.5605318546295166.\n",
      "epoch: 33, batch: 31, loss: 0.6211810111999512.\n",
      "epoch: 33, batch: 32, loss: 0.6783080697059631.\n",
      "epoch: 33, batch: 33, loss: 0.7315334677696228.\n",
      "epoch: 33, batch: 34, loss: 0.5723109245300293.\n",
      "epoch: 33, batch: 35, loss: 0.5626730918884277.\n",
      "epoch: 33, batch: 36, loss: 0.5657004117965698.\n",
      "epoch: 33, batch: 37, loss: 0.5692370533943176.\n",
      "epoch: 33, batch: 38, loss: 0.5986026525497437.\n",
      "epoch: 33, batch: 39, loss: 0.6472222805023193.\n",
      "epoch: 33, batch: 40, loss: 0.55998295545578.\n",
      "epoch: 33, batch: 41, loss: 0.5696181058883667.\n",
      "epoch: 33, batch: 42, loss: 0.554053008556366.\n",
      "epoch: 33, batch: 43, loss: 0.7184388041496277.\n",
      "epoch: 33, batch: 44, loss: 0.5960191488265991.\n",
      "epoch: 33, batch: 45, loss: 0.5574982762336731.\n",
      "epoch: 33, batch: 46, loss: 0.5599777698516846.\n",
      "epoch: 33, batch: 47, loss: 0.5678629279136658.\n",
      "epoch: 33, batch: 48, loss: 0.6451525092124939.\n",
      "epoch: 33, batch: 49, loss: 0.5645864605903625.\n",
      "epoch: 33, batch: 50, loss: 0.5725425481796265.\n",
      "epoch: 33, batch: 51, loss: 0.5790016651153564.\n",
      "epoch: 33, batch: 52, loss: 0.5541250109672546.\n",
      "epoch: 34, batch: 0, loss: 0.5593030452728271.\n",
      "epoch: 34, batch: 1, loss: 0.7307828664779663.\n",
      "epoch: 34, batch: 2, loss: 0.5733891725540161.\n",
      "epoch: 34, batch: 3, loss: 0.7078645825386047.\n",
      "epoch: 34, batch: 4, loss: 0.5556976795196533.\n",
      "epoch: 34, batch: 5, loss: 0.6383673548698425.\n",
      "epoch: 34, batch: 6, loss: 0.560314953327179.\n",
      "epoch: 34, batch: 7, loss: 0.6119301319122314.\n",
      "epoch: 34, batch: 8, loss: 0.723952054977417.\n",
      "epoch: 34, batch: 9, loss: 0.557884156703949.\n",
      "epoch: 34, batch: 10, loss: 0.6765346527099609.\n",
      "epoch: 34, batch: 11, loss: 0.6062425374984741.\n",
      "epoch: 34, batch: 12, loss: 0.6143348217010498.\n",
      "epoch: 34, batch: 13, loss: 0.5937072038650513.\n",
      "epoch: 34, batch: 14, loss: 0.5963308811187744.\n",
      "epoch: 34, batch: 15, loss: 0.5743540525436401.\n",
      "epoch: 34, batch: 16, loss: 0.6027429103851318.\n",
      "epoch: 34, batch: 17, loss: 0.598902702331543.\n",
      "epoch: 34, batch: 18, loss: 0.5591556429862976.\n",
      "epoch: 34, batch: 19, loss: 0.6359843015670776.\n",
      "epoch: 34, batch: 20, loss: 0.5593127012252808.\n",
      "epoch: 34, batch: 21, loss: 0.5715185403823853.\n",
      "epoch: 34, batch: 22, loss: 0.736579418182373.\n",
      "epoch: 34, batch: 23, loss: 0.5553915500640869.\n",
      "epoch: 34, batch: 24, loss: 1.0320603847503662.\n",
      "epoch: 34, batch: 25, loss: 0.5859590768814087.\n",
      "epoch: 34, batch: 26, loss: 0.581432044506073.\n",
      "epoch: 34, batch: 27, loss: 0.5633864998817444.\n",
      "epoch: 34, batch: 28, loss: 0.5619503259658813.\n",
      "epoch: 34, batch: 29, loss: 0.6265665888786316.\n",
      "epoch: 34, batch: 30, loss: 0.5621107816696167.\n",
      "epoch: 34, batch: 31, loss: 0.7522497773170471.\n",
      "epoch: 34, batch: 32, loss: 0.5616286993026733.\n",
      "epoch: 34, batch: 33, loss: 0.560123860836029.\n",
      "epoch: 34, batch: 34, loss: 0.5936937928199768.\n",
      "epoch: 34, batch: 35, loss: 0.5684353113174438.\n",
      "epoch: 34, batch: 36, loss: 0.562842845916748.\n",
      "epoch: 34, batch: 37, loss: 0.5694810152053833.\n",
      "epoch: 34, batch: 38, loss: 0.5569378733634949.\n",
      "epoch: 34, batch: 39, loss: 0.5958016514778137.\n",
      "epoch: 34, batch: 40, loss: 0.5562341213226318.\n",
      "epoch: 34, batch: 41, loss: 0.5617817640304565.\n",
      "epoch: 34, batch: 42, loss: 0.5541388988494873.\n",
      "epoch: 34, batch: 43, loss: 0.6212284564971924.\n",
      "epoch: 34, batch: 44, loss: 0.6289232969284058.\n",
      "epoch: 34, batch: 45, loss: 0.5846484899520874.\n",
      "epoch: 34, batch: 46, loss: 0.5846080780029297.\n",
      "epoch: 34, batch: 47, loss: 0.5681922435760498.\n",
      "epoch: 34, batch: 48, loss: 0.5569150447845459.\n",
      "epoch: 34, batch: 49, loss: 0.6387472748756409.\n",
      "epoch: 34, batch: 50, loss: 0.5660040378570557.\n",
      "epoch: 34, batch: 51, loss: 0.5545519590377808.\n",
      "epoch: 34, batch: 52, loss: 0.563457727432251.\n",
      "epoch: 35, batch: 0, loss: 0.5685655474662781.\n",
      "epoch: 35, batch: 1, loss: 0.5567907094955444.\n",
      "epoch: 35, batch: 2, loss: 0.5644655227661133.\n",
      "epoch: 35, batch: 3, loss: 0.7111496925354004.\n",
      "epoch: 35, batch: 4, loss: 0.5624834299087524.\n",
      "epoch: 35, batch: 5, loss: 0.5612852573394775.\n",
      "epoch: 35, batch: 6, loss: 0.5548418760299683.\n",
      "epoch: 35, batch: 7, loss: 0.5582001209259033.\n",
      "epoch: 35, batch: 8, loss: 0.5682259798049927.\n",
      "epoch: 35, batch: 9, loss: 0.6786875128746033.\n",
      "epoch: 35, batch: 10, loss: 0.57440185546875.\n",
      "epoch: 35, batch: 11, loss: 0.596137285232544.\n",
      "epoch: 35, batch: 12, loss: 0.5824650526046753.\n",
      "epoch: 35, batch: 13, loss: 0.7217833399772644.\n",
      "epoch: 35, batch: 14, loss: 0.6277796030044556.\n",
      "epoch: 35, batch: 15, loss: 0.5585148334503174.\n",
      "epoch: 35, batch: 16, loss: 0.5552670359611511.\n",
      "epoch: 35, batch: 17, loss: 0.6348947286605835.\n",
      "epoch: 35, batch: 18, loss: 0.5783122777938843.\n",
      "epoch: 35, batch: 19, loss: 0.6098743677139282.\n",
      "epoch: 35, batch: 20, loss: 0.5755168199539185.\n",
      "epoch: 35, batch: 21, loss: 0.5540846586227417.\n",
      "epoch: 35, batch: 22, loss: 0.6048503518104553.\n",
      "epoch: 35, batch: 23, loss: 0.5946605205535889.\n",
      "epoch: 35, batch: 24, loss: 0.9923361539840698.\n",
      "epoch: 35, batch: 25, loss: 0.6001416444778442.\n",
      "epoch: 35, batch: 26, loss: 0.578869104385376.\n",
      "epoch: 35, batch: 27, loss: 0.5563225150108337.\n",
      "epoch: 35, batch: 28, loss: 0.5971589088439941.\n",
      "epoch: 35, batch: 29, loss: 0.8598566055297852.\n",
      "epoch: 35, batch: 30, loss: 0.560126781463623.\n",
      "epoch: 35, batch: 31, loss: 0.6521131992340088.\n",
      "epoch: 35, batch: 32, loss: 0.6441230177879333.\n",
      "epoch: 35, batch: 33, loss: 0.612767219543457.\n",
      "epoch: 35, batch: 34, loss: 0.5545763969421387.\n",
      "epoch: 35, batch: 35, loss: 0.7832188010215759.\n",
      "epoch: 35, batch: 36, loss: 0.5552242994308472.\n",
      "epoch: 35, batch: 37, loss: 0.5613434314727783.\n",
      "epoch: 35, batch: 38, loss: 0.594802737236023.\n",
      "epoch: 35, batch: 39, loss: 0.5655779838562012.\n",
      "epoch: 35, batch: 40, loss: 0.5568282008171082.\n",
      "epoch: 35, batch: 41, loss: 0.5619012117385864.\n",
      "epoch: 35, batch: 42, loss: 0.6509358882904053.\n",
      "epoch: 35, batch: 43, loss: 0.564353883266449.\n",
      "epoch: 35, batch: 44, loss: 0.5573197603225708.\n",
      "epoch: 35, batch: 45, loss: 0.5567961931228638.\n",
      "epoch: 35, batch: 46, loss: 0.5534535646438599.\n",
      "epoch: 35, batch: 47, loss: 0.5704537034034729.\n",
      "epoch: 35, batch: 48, loss: 0.5572615265846252.\n",
      "epoch: 35, batch: 49, loss: 0.6616946458816528.\n",
      "epoch: 35, batch: 50, loss: 0.6307855844497681.\n",
      "epoch: 35, batch: 51, loss: 0.6318753957748413.\n",
      "epoch: 35, batch: 52, loss: 0.5573474168777466.\n",
      "epoch: 36, batch: 0, loss: 0.560527503490448.\n",
      "epoch: 36, batch: 1, loss: 0.5938791036605835.\n",
      "epoch: 36, batch: 2, loss: 0.6035605669021606.\n",
      "epoch: 36, batch: 3, loss: 0.5675742030143738.\n",
      "epoch: 36, batch: 4, loss: 0.6030939817428589.\n",
      "epoch: 36, batch: 5, loss: 0.7706358432769775.\n",
      "epoch: 36, batch: 6, loss: 0.9083110094070435.\n",
      "epoch: 36, batch: 7, loss: 0.5751363039016724.\n",
      "epoch: 36, batch: 8, loss: 0.7293039560317993.\n",
      "epoch: 36, batch: 9, loss: 0.6991754174232483.\n",
      "epoch: 36, batch: 10, loss: 0.5774377584457397.\n",
      "epoch: 36, batch: 11, loss: 0.7764742374420166.\n",
      "epoch: 36, batch: 12, loss: 0.565377414226532.\n",
      "epoch: 36, batch: 13, loss: 0.6214019060134888.\n",
      "epoch: 36, batch: 14, loss: 0.5791423320770264.\n",
      "epoch: 36, batch: 15, loss: 0.5893545150756836.\n",
      "epoch: 36, batch: 16, loss: 0.5742050409317017.\n",
      "epoch: 36, batch: 17, loss: 0.6104748845100403.\n",
      "epoch: 36, batch: 18, loss: 0.6022551655769348.\n",
      "epoch: 36, batch: 19, loss: 0.5555449724197388.\n",
      "epoch: 36, batch: 20, loss: 0.5626936554908752.\n",
      "epoch: 36, batch: 21, loss: 0.5635688304901123.\n",
      "epoch: 36, batch: 22, loss: 0.9080705642700195.\n",
      "epoch: 36, batch: 23, loss: 0.6460312604904175.\n",
      "epoch: 36, batch: 24, loss: 0.5911487340927124.\n",
      "epoch: 36, batch: 25, loss: 0.5597032308578491.\n",
      "epoch: 36, batch: 26, loss: 0.610205888748169.\n",
      "epoch: 36, batch: 27, loss: 0.5670109987258911.\n",
      "epoch: 36, batch: 28, loss: 0.5614189505577087.\n",
      "epoch: 36, batch: 29, loss: 0.5588353872299194.\n",
      "epoch: 36, batch: 30, loss: 0.6458662152290344.\n",
      "epoch: 36, batch: 31, loss: 0.5548241138458252.\n",
      "epoch: 36, batch: 32, loss: 0.7965419888496399.\n",
      "epoch: 36, batch: 33, loss: 0.6069676876068115.\n",
      "epoch: 36, batch: 34, loss: 0.5901855230331421.\n",
      "epoch: 36, batch: 35, loss: 0.5607454776763916.\n",
      "epoch: 36, batch: 36, loss: 0.5685285925865173.\n",
      "epoch: 36, batch: 37, loss: 0.598578691482544.\n",
      "epoch: 36, batch: 38, loss: 0.5583423376083374.\n",
      "epoch: 36, batch: 39, loss: 0.5570054054260254.\n",
      "epoch: 36, batch: 40, loss: 0.557281494140625.\n",
      "epoch: 36, batch: 41, loss: 0.5565323829650879.\n",
      "epoch: 36, batch: 42, loss: 0.562118411064148.\n",
      "epoch: 36, batch: 43, loss: 0.8579219579696655.\n",
      "epoch: 36, batch: 44, loss: 0.606960117816925.\n",
      "epoch: 36, batch: 45, loss: 0.564986526966095.\n",
      "epoch: 36, batch: 46, loss: 0.5562984943389893.\n",
      "epoch: 36, batch: 47, loss: 0.5728433132171631.\n",
      "epoch: 36, batch: 48, loss: 0.5552353858947754.\n",
      "epoch: 36, batch: 49, loss: 0.557258129119873.\n",
      "epoch: 36, batch: 50, loss: 0.5586420893669128.\n",
      "epoch: 36, batch: 51, loss: 0.568941593170166.\n",
      "epoch: 36, batch: 52, loss: 0.5537398457527161.\n",
      "epoch: 37, batch: 0, loss: 0.5586882829666138.\n",
      "epoch: 37, batch: 1, loss: 0.5558581352233887.\n",
      "epoch: 37, batch: 2, loss: 0.5623917579650879.\n",
      "epoch: 37, batch: 3, loss: 0.554567813873291.\n",
      "epoch: 37, batch: 4, loss: 0.6640398502349854.\n",
      "epoch: 37, batch: 5, loss: 0.579079270362854.\n",
      "epoch: 37, batch: 6, loss: 0.5721592903137207.\n",
      "epoch: 37, batch: 7, loss: 0.5602481365203857.\n",
      "epoch: 37, batch: 8, loss: 0.5879752039909363.\n",
      "epoch: 37, batch: 9, loss: 0.5731278657913208.\n",
      "epoch: 37, batch: 10, loss: 0.5675340890884399.\n",
      "epoch: 37, batch: 11, loss: 0.5531629920005798.\n",
      "epoch: 37, batch: 12, loss: 0.6958472728729248.\n",
      "epoch: 37, batch: 13, loss: 0.7604001760482788.\n",
      "epoch: 37, batch: 14, loss: 0.6222759485244751.\n",
      "epoch: 37, batch: 15, loss: 0.768802285194397.\n",
      "epoch: 37, batch: 16, loss: 0.6598836183547974.\n",
      "epoch: 37, batch: 17, loss: 0.5645841956138611.\n",
      "epoch: 37, batch: 18, loss: 0.5845365524291992.\n",
      "epoch: 37, batch: 19, loss: 0.707568347454071.\n",
      "epoch: 37, batch: 20, loss: 0.5538671612739563.\n",
      "epoch: 37, batch: 21, loss: 0.7281482219696045.\n",
      "epoch: 37, batch: 22, loss: 0.608020544052124.\n",
      "epoch: 37, batch: 23, loss: 0.5547560453414917.\n",
      "epoch: 37, batch: 24, loss: 0.5838404893875122.\n",
      "epoch: 37, batch: 25, loss: 0.588495135307312.\n",
      "epoch: 37, batch: 26, loss: 0.573168158531189.\n",
      "epoch: 37, batch: 27, loss: 0.5724390745162964.\n",
      "epoch: 37, batch: 28, loss: 0.570621132850647.\n",
      "epoch: 37, batch: 29, loss: 0.6349385976791382.\n",
      "epoch: 37, batch: 30, loss: 0.5644549131393433.\n",
      "epoch: 37, batch: 31, loss: 0.6881482601165771.\n",
      "epoch: 37, batch: 32, loss: 0.5551375150680542.\n",
      "epoch: 37, batch: 33, loss: 0.5717830061912537.\n",
      "epoch: 37, batch: 34, loss: 0.5678823590278625.\n",
      "epoch: 37, batch: 35, loss: 0.5704715251922607.\n",
      "epoch: 37, batch: 36, loss: 0.5579445362091064.\n",
      "epoch: 37, batch: 37, loss: 0.562180757522583.\n",
      "epoch: 37, batch: 38, loss: 0.562591552734375.\n",
      "epoch: 37, batch: 39, loss: 0.6635909080505371.\n",
      "epoch: 37, batch: 40, loss: 0.5581580996513367.\n",
      "epoch: 37, batch: 41, loss: 0.580062985420227.\n",
      "epoch: 37, batch: 42, loss: 0.725009560585022.\n",
      "epoch: 37, batch: 43, loss: 0.5957716703414917.\n",
      "epoch: 37, batch: 44, loss: 0.570519745349884.\n",
      "epoch: 37, batch: 45, loss: 0.5623050928115845.\n",
      "epoch: 37, batch: 46, loss: 0.7906408309936523.\n",
      "epoch: 37, batch: 47, loss: 0.6173900365829468.\n",
      "epoch: 37, batch: 48, loss: 0.5600243806838989.\n",
      "epoch: 37, batch: 49, loss: 0.5746914744377136.\n",
      "epoch: 37, batch: 50, loss: 0.563704788684845.\n",
      "epoch: 37, batch: 51, loss: 0.5626161098480225.\n",
      "epoch: 37, batch: 52, loss: 0.6042889952659607.\n",
      "epoch: 38, batch: 0, loss: 0.6047751903533936.\n",
      "epoch: 38, batch: 1, loss: 0.5659080147743225.\n",
      "epoch: 38, batch: 2, loss: 0.6842166185379028.\n",
      "epoch: 38, batch: 3, loss: 0.5550669431686401.\n",
      "epoch: 38, batch: 4, loss: 0.6032038927078247.\n",
      "epoch: 38, batch: 5, loss: 0.6093971133232117.\n",
      "epoch: 38, batch: 6, loss: 0.5619263648986816.\n",
      "epoch: 38, batch: 7, loss: 0.8250546455383301.\n",
      "epoch: 38, batch: 8, loss: 0.5549085736274719.\n",
      "epoch: 38, batch: 9, loss: 0.6298967003822327.\n",
      "epoch: 38, batch: 10, loss: 0.5686008930206299.\n",
      "epoch: 38, batch: 11, loss: 0.5642330646514893.\n",
      "epoch: 38, batch: 12, loss: 0.5922958850860596.\n",
      "epoch: 38, batch: 13, loss: 0.6865429282188416.\n",
      "epoch: 38, batch: 14, loss: 0.5933071374893188.\n",
      "epoch: 38, batch: 15, loss: 0.5680238008499146.\n",
      "epoch: 38, batch: 16, loss: 0.5633581280708313.\n",
      "epoch: 38, batch: 17, loss: 0.5523451566696167.\n",
      "epoch: 38, batch: 18, loss: 0.6513874530792236.\n",
      "epoch: 38, batch: 19, loss: 0.5680144429206848.\n",
      "epoch: 38, batch: 20, loss: 0.5974911451339722.\n",
      "epoch: 38, batch: 21, loss: 0.7873769998550415.\n",
      "epoch: 38, batch: 22, loss: 0.5525408983230591.\n",
      "epoch: 38, batch: 23, loss: 0.6000590324401855.\n",
      "epoch: 38, batch: 24, loss: 0.79620361328125.\n",
      "epoch: 38, batch: 25, loss: 0.6924296021461487.\n",
      "epoch: 38, batch: 26, loss: 0.5696176290512085.\n",
      "epoch: 38, batch: 27, loss: 0.5651577711105347.\n",
      "epoch: 38, batch: 28, loss: 0.5539296269416809.\n",
      "epoch: 38, batch: 29, loss: 0.5533467531204224.\n",
      "epoch: 38, batch: 30, loss: 0.6008679866790771.\n",
      "epoch: 38, batch: 31, loss: 0.5573773384094238.\n",
      "epoch: 38, batch: 32, loss: 0.55613112449646.\n",
      "epoch: 38, batch: 33, loss: 0.5745810866355896.\n",
      "epoch: 38, batch: 34, loss: 0.7166005373001099.\n",
      "epoch: 38, batch: 35, loss: 0.5600982904434204.\n",
      "epoch: 38, batch: 36, loss: 0.5622943639755249.\n",
      "epoch: 38, batch: 37, loss: 0.5555534362792969.\n",
      "epoch: 38, batch: 38, loss: 0.778069257736206.\n",
      "epoch: 38, batch: 39, loss: 0.5636303424835205.\n",
      "epoch: 38, batch: 40, loss: 0.5548219680786133.\n",
      "epoch: 38, batch: 41, loss: 0.6004780530929565.\n",
      "epoch: 38, batch: 42, loss: 0.596957802772522.\n",
      "epoch: 38, batch: 43, loss: 0.5558698177337646.\n",
      "epoch: 38, batch: 44, loss: 0.6515249013900757.\n",
      "epoch: 38, batch: 45, loss: 0.5622721910476685.\n",
      "epoch: 38, batch: 46, loss: 0.5580246448516846.\n",
      "epoch: 38, batch: 47, loss: 0.5647521018981934.\n",
      "epoch: 38, batch: 48, loss: 0.5667874813079834.\n",
      "epoch: 38, batch: 49, loss: 0.5640120506286621.\n",
      "epoch: 38, batch: 50, loss: 0.5550289154052734.\n",
      "epoch: 38, batch: 51, loss: 0.5863608121871948.\n",
      "epoch: 38, batch: 52, loss: 0.7284950017929077.\n",
      "epoch: 39, batch: 0, loss: 0.6629513502120972.\n",
      "epoch: 39, batch: 1, loss: 0.6939345598220825.\n",
      "epoch: 39, batch: 2, loss: 0.5671579837799072.\n",
      "epoch: 39, batch: 3, loss: 0.5585203766822815.\n",
      "epoch: 39, batch: 4, loss: 0.5552245378494263.\n",
      "epoch: 39, batch: 5, loss: 0.5756711363792419.\n",
      "epoch: 39, batch: 6, loss: 0.5535572171211243.\n",
      "epoch: 39, batch: 7, loss: 0.5839305520057678.\n",
      "epoch: 39, batch: 8, loss: 0.5609427094459534.\n",
      "epoch: 39, batch: 9, loss: 0.5641272664070129.\n",
      "epoch: 39, batch: 10, loss: 0.574692964553833.\n",
      "epoch: 39, batch: 11, loss: 0.560142993927002.\n",
      "epoch: 39, batch: 12, loss: 0.5749652981758118.\n",
      "epoch: 39, batch: 13, loss: 0.562850832939148.\n",
      "epoch: 39, batch: 14, loss: 0.6128160953521729.\n",
      "epoch: 39, batch: 15, loss: 0.7182427048683167.\n",
      "epoch: 39, batch: 16, loss: 0.5675406455993652.\n",
      "epoch: 39, batch: 17, loss: 0.5535154342651367.\n",
      "epoch: 39, batch: 18, loss: 0.557877242565155.\n",
      "epoch: 39, batch: 19, loss: 0.6184735894203186.\n",
      "epoch: 39, batch: 20, loss: 0.555881142616272.\n",
      "epoch: 39, batch: 21, loss: 0.5574555993080139.\n",
      "epoch: 39, batch: 22, loss: 0.5655235052108765.\n",
      "epoch: 39, batch: 23, loss: 0.5809640288352966.\n",
      "epoch: 39, batch: 24, loss: 0.775080680847168.\n",
      "epoch: 39, batch: 25, loss: 0.5555852651596069.\n",
      "epoch: 39, batch: 26, loss: 0.625777006149292.\n",
      "epoch: 39, batch: 27, loss: 0.5949965715408325.\n",
      "epoch: 39, batch: 28, loss: 0.6432946920394897.\n",
      "epoch: 39, batch: 29, loss: 0.5559879541397095.\n",
      "epoch: 39, batch: 30, loss: 0.5750594139099121.\n",
      "epoch: 39, batch: 31, loss: 0.8825380206108093.\n",
      "epoch: 39, batch: 32, loss: 0.6233686208724976.\n",
      "epoch: 39, batch: 33, loss: 0.5605007410049438.\n",
      "epoch: 39, batch: 34, loss: 0.5555011034011841.\n",
      "epoch: 39, batch: 35, loss: 0.6538617610931396.\n",
      "epoch: 39, batch: 36, loss: 0.949323296546936.\n",
      "epoch: 39, batch: 37, loss: 0.5569374561309814.\n",
      "epoch: 39, batch: 38, loss: 0.5751413106918335.\n",
      "epoch: 39, batch: 39, loss: 0.5604629516601562.\n",
      "epoch: 39, batch: 40, loss: 0.5570580363273621.\n",
      "epoch: 39, batch: 41, loss: 0.5786430835723877.\n",
      "epoch: 39, batch: 42, loss: 0.5521450042724609.\n",
      "epoch: 39, batch: 43, loss: 0.5931624174118042.\n",
      "epoch: 39, batch: 44, loss: 0.5560789108276367.\n",
      "epoch: 39, batch: 45, loss: 0.5651267766952515.\n",
      "epoch: 39, batch: 46, loss: 0.5583953857421875.\n",
      "epoch: 39, batch: 47, loss: 0.5774435997009277.\n",
      "epoch: 39, batch: 48, loss: 0.5671408772468567.\n",
      "epoch: 39, batch: 49, loss: 0.6211374998092651.\n",
      "epoch: 39, batch: 50, loss: 0.6594741344451904.\n",
      "epoch: 39, batch: 51, loss: 0.595454216003418.\n",
      "epoch: 39, batch: 52, loss: 0.5579296350479126.\n",
      "epoch: 40, batch: 0, loss: 0.5609135627746582.\n",
      "epoch: 40, batch: 1, loss: 0.7251899242401123.\n",
      "epoch: 40, batch: 2, loss: 0.5568038821220398.\n",
      "epoch: 40, batch: 3, loss: 0.6354876160621643.\n",
      "epoch: 40, batch: 4, loss: 0.557424783706665.\n",
      "epoch: 40, batch: 5, loss: 0.5592809915542603.\n",
      "epoch: 40, batch: 6, loss: 0.6449732780456543.\n",
      "epoch: 40, batch: 7, loss: 0.5711169838905334.\n",
      "epoch: 40, batch: 8, loss: 0.5685868263244629.\n",
      "epoch: 40, batch: 9, loss: 0.5725577473640442.\n",
      "epoch: 40, batch: 10, loss: 0.5568171739578247.\n",
      "epoch: 40, batch: 11, loss: 0.5574842691421509.\n",
      "epoch: 40, batch: 12, loss: 0.5575461387634277.\n",
      "epoch: 40, batch: 13, loss: 0.6200857162475586.\n",
      "epoch: 40, batch: 14, loss: 0.8889096975326538.\n",
      "epoch: 40, batch: 15, loss: 0.5722436904907227.\n",
      "epoch: 40, batch: 16, loss: 0.7722381353378296.\n",
      "epoch: 40, batch: 17, loss: 0.5984982848167419.\n",
      "epoch: 40, batch: 18, loss: 0.5561785697937012.\n",
      "epoch: 40, batch: 19, loss: 0.5766308307647705.\n",
      "epoch: 40, batch: 20, loss: 0.5811715126037598.\n",
      "epoch: 40, batch: 21, loss: 0.7487845420837402.\n",
      "epoch: 40, batch: 22, loss: 0.5830219984054565.\n",
      "epoch: 40, batch: 23, loss: 0.5615864992141724.\n",
      "epoch: 40, batch: 24, loss: 0.576847493648529.\n",
      "epoch: 40, batch: 25, loss: 0.55825275182724.\n",
      "epoch: 40, batch: 26, loss: 0.573312520980835.\n",
      "epoch: 40, batch: 27, loss: 0.5811606645584106.\n",
      "epoch: 40, batch: 28, loss: 0.5557531714439392.\n",
      "epoch: 40, batch: 29, loss: 0.5536439418792725.\n",
      "epoch: 40, batch: 30, loss: 0.5631506443023682.\n",
      "epoch: 40, batch: 31, loss: 0.6384852528572083.\n",
      "epoch: 40, batch: 32, loss: 0.5942245125770569.\n",
      "epoch: 40, batch: 33, loss: 0.5885749459266663.\n",
      "epoch: 40, batch: 34, loss: 0.5553127527236938.\n",
      "epoch: 40, batch: 35, loss: 0.7746028304100037.\n",
      "epoch: 40, batch: 36, loss: 0.5630329847335815.\n",
      "epoch: 40, batch: 37, loss: 0.6683050394058228.\n",
      "epoch: 40, batch: 38, loss: 0.564624011516571.\n",
      "epoch: 40, batch: 39, loss: 0.5962176322937012.\n",
      "epoch: 40, batch: 40, loss: 0.564451277256012.\n",
      "epoch: 40, batch: 41, loss: 0.5548163652420044.\n",
      "epoch: 40, batch: 42, loss: 0.5557786822319031.\n",
      "epoch: 40, batch: 43, loss: 0.5562002062797546.\n",
      "epoch: 40, batch: 44, loss: 0.558407723903656.\n",
      "epoch: 40, batch: 45, loss: 0.5548647046089172.\n",
      "epoch: 40, batch: 46, loss: 0.5634452104568481.\n",
      "epoch: 40, batch: 47, loss: 0.636005163192749.\n",
      "epoch: 40, batch: 48, loss: 0.5636634826660156.\n",
      "epoch: 40, batch: 49, loss: 0.5964154005050659.\n",
      "epoch: 40, batch: 50, loss: 0.6301823854446411.\n",
      "epoch: 40, batch: 51, loss: 0.7486461997032166.\n",
      "epoch: 40, batch: 52, loss: 0.55305415391922.\n",
      "epoch: 41, batch: 0, loss: 0.5592803955078125.\n",
      "epoch: 41, batch: 1, loss: 0.5557390451431274.\n",
      "epoch: 41, batch: 2, loss: 0.5522993803024292.\n",
      "epoch: 41, batch: 3, loss: 0.8026015758514404.\n",
      "epoch: 41, batch: 4, loss: 0.5543258786201477.\n",
      "epoch: 41, batch: 5, loss: 0.7981539964675903.\n",
      "epoch: 41, batch: 6, loss: 0.5809321999549866.\n",
      "epoch: 41, batch: 7, loss: 0.565570592880249.\n",
      "epoch: 41, batch: 8, loss: 0.5572067499160767.\n",
      "epoch: 41, batch: 9, loss: 0.6914054155349731.\n",
      "epoch: 41, batch: 10, loss: 0.5579690337181091.\n",
      "epoch: 41, batch: 11, loss: 0.557022213935852.\n",
      "epoch: 41, batch: 12, loss: 0.5565201640129089.\n",
      "epoch: 41, batch: 13, loss: 0.555902361869812.\n",
      "epoch: 41, batch: 14, loss: 0.5994254350662231.\n",
      "epoch: 41, batch: 15, loss: 0.5570171475410461.\n",
      "epoch: 41, batch: 16, loss: 0.5550903081893921.\n",
      "epoch: 41, batch: 17, loss: 0.5793362855911255.\n",
      "epoch: 41, batch: 18, loss: 0.8725433349609375.\n",
      "epoch: 41, batch: 19, loss: 0.6001630425453186.\n",
      "epoch: 41, batch: 20, loss: 0.6328368186950684.\n",
      "epoch: 41, batch: 21, loss: 0.555713415145874.\n",
      "epoch: 41, batch: 22, loss: 0.554408848285675.\n",
      "epoch: 41, batch: 23, loss: 0.6909428238868713.\n",
      "epoch: 41, batch: 24, loss: 0.558373212814331.\n",
      "epoch: 41, batch: 25, loss: 0.6171976923942566.\n",
      "epoch: 41, batch: 26, loss: 0.6620426177978516.\n",
      "epoch: 41, batch: 27, loss: 0.5649586319923401.\n",
      "epoch: 41, batch: 28, loss: 0.5531395673751831.\n",
      "epoch: 41, batch: 29, loss: 0.6341152787208557.\n",
      "epoch: 41, batch: 30, loss: 0.5614451766014099.\n",
      "epoch: 41, batch: 31, loss: 0.570797324180603.\n",
      "epoch: 41, batch: 32, loss: 0.6246272325515747.\n",
      "epoch: 41, batch: 33, loss: 0.6042770147323608.\n",
      "epoch: 41, batch: 34, loss: 0.5627733469009399.\n",
      "epoch: 41, batch: 35, loss: 0.554348349571228.\n",
      "epoch: 41, batch: 36, loss: 0.583815336227417.\n",
      "epoch: 41, batch: 37, loss: 0.6251040697097778.\n",
      "epoch: 41, batch: 38, loss: 0.5619202852249146.\n",
      "epoch: 41, batch: 39, loss: 0.5556102991104126.\n",
      "epoch: 41, batch: 40, loss: 0.6166622638702393.\n",
      "epoch: 41, batch: 41, loss: 0.5621964335441589.\n",
      "epoch: 41, batch: 42, loss: 0.5857536792755127.\n",
      "epoch: 41, batch: 43, loss: 0.5657916069030762.\n",
      "epoch: 41, batch: 44, loss: 0.5614510774612427.\n",
      "epoch: 41, batch: 45, loss: 0.5631179809570312.\n",
      "epoch: 41, batch: 46, loss: 0.5547159910202026.\n",
      "epoch: 41, batch: 47, loss: 0.5736855268478394.\n",
      "epoch: 41, batch: 48, loss: 0.7265989184379578.\n",
      "epoch: 41, batch: 49, loss: 0.5532214641571045.\n",
      "epoch: 41, batch: 50, loss: 0.5764479637145996.\n",
      "epoch: 41, batch: 51, loss: 0.5726336240768433.\n",
      "epoch: 41, batch: 52, loss: 1.0185754299163818.\n",
      "epoch: 42, batch: 0, loss: 0.5669330358505249.\n",
      "epoch: 42, batch: 1, loss: 0.5583829879760742.\n",
      "epoch: 42, batch: 2, loss: 0.6046121120452881.\n",
      "epoch: 42, batch: 3, loss: 0.6617631316184998.\n",
      "epoch: 42, batch: 4, loss: 0.6168752908706665.\n",
      "epoch: 42, batch: 5, loss: 0.5575907230377197.\n",
      "epoch: 42, batch: 6, loss: 0.5540417432785034.\n",
      "epoch: 42, batch: 7, loss: 0.5930216312408447.\n",
      "epoch: 42, batch: 8, loss: 0.5557818412780762.\n",
      "epoch: 42, batch: 9, loss: 0.5612891912460327.\n",
      "epoch: 42, batch: 10, loss: 0.7885688543319702.\n",
      "epoch: 42, batch: 11, loss: 0.554648220539093.\n",
      "epoch: 42, batch: 12, loss: 0.5690284967422485.\n",
      "epoch: 42, batch: 13, loss: 0.5538326501846313.\n",
      "epoch: 42, batch: 14, loss: 0.5558446645736694.\n",
      "epoch: 42, batch: 15, loss: 0.5528954267501831.\n",
      "epoch: 42, batch: 16, loss: 0.6867577433586121.\n",
      "epoch: 42, batch: 17, loss: 0.7173223495483398.\n",
      "epoch: 42, batch: 18, loss: 0.7867262363433838.\n",
      "epoch: 42, batch: 19, loss: 0.6678767800331116.\n",
      "epoch: 42, batch: 20, loss: 0.6192631721496582.\n",
      "epoch: 42, batch: 21, loss: 0.556786298751831.\n",
      "epoch: 42, batch: 22, loss: 0.5873348712921143.\n",
      "epoch: 42, batch: 23, loss: 0.5614076852798462.\n",
      "epoch: 42, batch: 24, loss: 0.5743936896324158.\n",
      "epoch: 42, batch: 25, loss: 0.5563857555389404.\n",
      "epoch: 42, batch: 26, loss: 0.5575929880142212.\n",
      "epoch: 42, batch: 27, loss: 0.570832371711731.\n",
      "epoch: 42, batch: 28, loss: 0.557693600654602.\n",
      "epoch: 42, batch: 29, loss: 0.5676581859588623.\n",
      "epoch: 42, batch: 30, loss: 0.5572841167449951.\n",
      "epoch: 42, batch: 31, loss: 0.5537840723991394.\n",
      "epoch: 42, batch: 32, loss: 0.6778846383094788.\n",
      "epoch: 42, batch: 33, loss: 0.5873246788978577.\n",
      "epoch: 42, batch: 34, loss: 0.5606632828712463.\n",
      "epoch: 42, batch: 35, loss: 0.5884494781494141.\n",
      "epoch: 42, batch: 36, loss: 0.5561925172805786.\n",
      "epoch: 42, batch: 37, loss: 0.5550364851951599.\n",
      "epoch: 42, batch: 38, loss: 0.9565503597259521.\n",
      "epoch: 42, batch: 39, loss: 0.5760447978973389.\n",
      "epoch: 42, batch: 40, loss: 0.5547792315483093.\n",
      "epoch: 42, batch: 41, loss: 0.554623007774353.\n",
      "epoch: 42, batch: 42, loss: 0.7442676424980164.\n",
      "epoch: 42, batch: 43, loss: 0.5889342427253723.\n",
      "epoch: 42, batch: 44, loss: 0.5530011057853699.\n",
      "epoch: 42, batch: 45, loss: 0.5759451389312744.\n",
      "epoch: 42, batch: 46, loss: 0.5565996170043945.\n",
      "epoch: 42, batch: 47, loss: 0.5853687524795532.\n",
      "epoch: 42, batch: 48, loss: 0.5558367967605591.\n",
      "epoch: 42, batch: 49, loss: 0.5643534064292908.\n",
      "epoch: 42, batch: 50, loss: 0.7930987477302551.\n",
      "epoch: 42, batch: 51, loss: 0.593086302280426.\n",
      "epoch: 42, batch: 52, loss: 0.5543433427810669.\n",
      "epoch: 43, batch: 0, loss: 0.7804011702537537.\n",
      "epoch: 43, batch: 1, loss: 0.572384774684906.\n",
      "epoch: 43, batch: 2, loss: 0.5580297112464905.\n",
      "epoch: 43, batch: 3, loss: 0.6151491403579712.\n",
      "epoch: 43, batch: 4, loss: 0.5912444591522217.\n",
      "epoch: 43, batch: 5, loss: 0.68917316198349.\n",
      "epoch: 43, batch: 6, loss: 0.5635024309158325.\n",
      "epoch: 43, batch: 7, loss: 0.7697691321372986.\n",
      "epoch: 43, batch: 8, loss: 0.5546877384185791.\n",
      "epoch: 43, batch: 9, loss: 0.5557965636253357.\n",
      "epoch: 43, batch: 10, loss: 0.655468225479126.\n",
      "epoch: 43, batch: 11, loss: 0.5558751821517944.\n",
      "epoch: 43, batch: 12, loss: 0.631163477897644.\n",
      "epoch: 43, batch: 13, loss: 0.5560410022735596.\n",
      "epoch: 43, batch: 14, loss: 0.5601866841316223.\n",
      "epoch: 43, batch: 15, loss: 0.5543272495269775.\n",
      "epoch: 43, batch: 16, loss: 0.5599216222763062.\n",
      "epoch: 43, batch: 17, loss: 0.6089621782302856.\n",
      "epoch: 43, batch: 18, loss: 0.5581744909286499.\n",
      "epoch: 43, batch: 19, loss: 0.565521240234375.\n",
      "epoch: 43, batch: 20, loss: 0.5544798374176025.\n",
      "epoch: 43, batch: 21, loss: 0.552629828453064.\n",
      "epoch: 43, batch: 22, loss: 0.7923394441604614.\n",
      "epoch: 43, batch: 23, loss: 0.5596587657928467.\n",
      "epoch: 43, batch: 24, loss: 0.5588475465774536.\n",
      "epoch: 43, batch: 25, loss: 0.5532107353210449.\n",
      "epoch: 43, batch: 26, loss: 0.710424542427063.\n",
      "epoch: 43, batch: 27, loss: 0.5569608807563782.\n",
      "epoch: 43, batch: 28, loss: 0.5717013478279114.\n",
      "epoch: 43, batch: 29, loss: 0.5584808588027954.\n",
      "epoch: 43, batch: 30, loss: 0.5537883043289185.\n",
      "epoch: 43, batch: 31, loss: 0.5719476938247681.\n",
      "epoch: 43, batch: 32, loss: 0.578342080116272.\n",
      "epoch: 43, batch: 33, loss: 0.5553861856460571.\n",
      "epoch: 43, batch: 34, loss: 0.5562441349029541.\n",
      "epoch: 43, batch: 35, loss: 0.5937308073043823.\n",
      "epoch: 43, batch: 36, loss: 0.5633187294006348.\n",
      "epoch: 43, batch: 37, loss: 0.6158765554428101.\n",
      "epoch: 43, batch: 38, loss: 0.5554205179214478.\n",
      "epoch: 43, batch: 39, loss: 0.5809158086776733.\n",
      "epoch: 43, batch: 40, loss: 0.5860332250595093.\n",
      "epoch: 43, batch: 41, loss: 0.553076982498169.\n",
      "epoch: 43, batch: 42, loss: 0.5595880746841431.\n",
      "epoch: 43, batch: 43, loss: 0.5656834244728088.\n",
      "epoch: 43, batch: 44, loss: 0.7959991097450256.\n",
      "epoch: 43, batch: 45, loss: 0.6257373094558716.\n",
      "epoch: 43, batch: 46, loss: 0.6051367521286011.\n",
      "epoch: 43, batch: 47, loss: 0.6491516828536987.\n",
      "epoch: 43, batch: 48, loss: 0.6691907048225403.\n",
      "epoch: 43, batch: 49, loss: 0.5534898638725281.\n",
      "epoch: 43, batch: 50, loss: 0.5546644926071167.\n",
      "epoch: 43, batch: 51, loss: 0.5681183338165283.\n",
      "epoch: 43, batch: 52, loss: 0.561647891998291.\n",
      "epoch: 44, batch: 0, loss: 0.5712414979934692.\n",
      "epoch: 44, batch: 1, loss: 0.6152634620666504.\n",
      "epoch: 44, batch: 2, loss: 0.5534229278564453.\n",
      "epoch: 44, batch: 3, loss: 0.5618071556091309.\n",
      "epoch: 44, batch: 4, loss: 0.5625001192092896.\n",
      "epoch: 44, batch: 5, loss: 0.5812693238258362.\n",
      "epoch: 44, batch: 6, loss: 0.6594144105911255.\n",
      "epoch: 44, batch: 7, loss: 0.7386206388473511.\n",
      "epoch: 44, batch: 8, loss: 0.5681619644165039.\n",
      "epoch: 44, batch: 9, loss: 0.5660035014152527.\n",
      "epoch: 44, batch: 10, loss: 0.5552151203155518.\n",
      "epoch: 44, batch: 11, loss: 0.5527251958847046.\n",
      "epoch: 44, batch: 12, loss: 0.8729674816131592.\n",
      "epoch: 44, batch: 13, loss: 0.5536500215530396.\n",
      "epoch: 44, batch: 14, loss: 0.5676324367523193.\n",
      "epoch: 44, batch: 15, loss: 0.5757248401641846.\n",
      "epoch: 44, batch: 16, loss: 0.5590446591377258.\n",
      "epoch: 44, batch: 17, loss: 0.5543184876441956.\n",
      "epoch: 44, batch: 18, loss: 0.5636551380157471.\n",
      "epoch: 44, batch: 19, loss: 0.5663560628890991.\n",
      "epoch: 44, batch: 20, loss: 0.5549213886260986.\n",
      "epoch: 44, batch: 21, loss: 0.6965296268463135.\n",
      "epoch: 44, batch: 22, loss: 0.5618149042129517.\n",
      "epoch: 44, batch: 23, loss: 0.5994879007339478.\n",
      "epoch: 44, batch: 24, loss: 0.5576575994491577.\n",
      "epoch: 44, batch: 25, loss: 0.6575800180435181.\n",
      "epoch: 44, batch: 26, loss: 0.5573387145996094.\n",
      "epoch: 44, batch: 27, loss: 0.5602234601974487.\n",
      "epoch: 44, batch: 28, loss: 0.5548626780509949.\n",
      "epoch: 44, batch: 29, loss: 0.8213043808937073.\n",
      "epoch: 44, batch: 30, loss: 0.66219162940979.\n",
      "epoch: 44, batch: 31, loss: 0.5588911771774292.\n",
      "epoch: 44, batch: 32, loss: 0.5553979277610779.\n",
      "epoch: 44, batch: 33, loss: 0.7799171209335327.\n",
      "epoch: 44, batch: 34, loss: 0.558433473110199.\n",
      "epoch: 44, batch: 35, loss: 0.5827944278717041.\n",
      "epoch: 44, batch: 36, loss: 0.5787847638130188.\n",
      "epoch: 44, batch: 37, loss: 0.5548315048217773.\n",
      "epoch: 44, batch: 38, loss: 0.5541888475418091.\n",
      "epoch: 44, batch: 39, loss: 0.5646518468856812.\n",
      "epoch: 44, batch: 40, loss: 0.6198782920837402.\n",
      "epoch: 44, batch: 41, loss: 0.573218822479248.\n",
      "epoch: 44, batch: 42, loss: 0.6660783886909485.\n",
      "epoch: 44, batch: 43, loss: 0.577992856502533.\n",
      "epoch: 44, batch: 44, loss: 0.5534095764160156.\n",
      "epoch: 44, batch: 45, loss: 0.5553075075149536.\n",
      "epoch: 44, batch: 46, loss: 0.5554678440093994.\n",
      "epoch: 44, batch: 47, loss: 0.6480193734169006.\n",
      "epoch: 44, batch: 48, loss: 0.5630656480789185.\n",
      "epoch: 44, batch: 49, loss: 0.5634127855300903.\n",
      "epoch: 44, batch: 50, loss: 0.5533924102783203.\n",
      "epoch: 44, batch: 51, loss: 0.7148932814598083.\n",
      "epoch: 44, batch: 52, loss: 0.5554332137107849.\n",
      "epoch: 45, batch: 0, loss: 0.5535855293273926.\n",
      "epoch: 45, batch: 1, loss: 0.5557617545127869.\n",
      "epoch: 45, batch: 2, loss: 0.6371864080429077.\n",
      "epoch: 45, batch: 3, loss: 0.5663526058197021.\n",
      "epoch: 45, batch: 4, loss: 0.6197331547737122.\n",
      "epoch: 45, batch: 5, loss: 0.5660308599472046.\n",
      "epoch: 45, batch: 6, loss: 0.5645829439163208.\n",
      "epoch: 45, batch: 7, loss: 0.5715007781982422.\n",
      "epoch: 45, batch: 8, loss: 0.5550528168678284.\n",
      "epoch: 45, batch: 9, loss: 0.554241418838501.\n",
      "epoch: 45, batch: 10, loss: 0.5637627840042114.\n",
      "epoch: 45, batch: 11, loss: 0.5549269914627075.\n",
      "epoch: 45, batch: 12, loss: 0.5683009624481201.\n",
      "epoch: 45, batch: 13, loss: 0.5772567987442017.\n",
      "epoch: 45, batch: 14, loss: 0.6448750495910645.\n",
      "epoch: 45, batch: 15, loss: 0.5907208919525146.\n",
      "epoch: 45, batch: 16, loss: 0.6017969846725464.\n",
      "epoch: 45, batch: 17, loss: 0.5542066097259521.\n",
      "epoch: 45, batch: 18, loss: 0.7791174054145813.\n",
      "epoch: 45, batch: 19, loss: 0.554416835308075.\n",
      "epoch: 45, batch: 20, loss: 0.5633461475372314.\n",
      "epoch: 45, batch: 21, loss: 0.5569465160369873.\n",
      "epoch: 45, batch: 22, loss: 0.5558120012283325.\n",
      "epoch: 45, batch: 23, loss: 0.5712548494338989.\n",
      "epoch: 45, batch: 24, loss: 0.5782774686813354.\n",
      "epoch: 45, batch: 25, loss: 0.5573207139968872.\n",
      "epoch: 45, batch: 26, loss: 0.5844827890396118.\n",
      "epoch: 45, batch: 27, loss: 0.5781296491622925.\n",
      "epoch: 45, batch: 28, loss: 0.5657556653022766.\n",
      "epoch: 45, batch: 29, loss: 0.5970240235328674.\n",
      "epoch: 45, batch: 30, loss: 0.5550801157951355.\n",
      "epoch: 45, batch: 31, loss: 0.5539209246635437.\n",
      "epoch: 45, batch: 32, loss: 0.5577510595321655.\n",
      "epoch: 45, batch: 33, loss: 0.5541274547576904.\n",
      "epoch: 45, batch: 34, loss: 0.5867327451705933.\n",
      "epoch: 45, batch: 35, loss: 0.5566136837005615.\n",
      "epoch: 45, batch: 36, loss: 0.5548242330551147.\n",
      "epoch: 45, batch: 37, loss: 0.5597729086875916.\n",
      "epoch: 45, batch: 38, loss: 0.765135645866394.\n",
      "epoch: 45, batch: 39, loss: 0.5545821785926819.\n",
      "epoch: 45, batch: 40, loss: 0.5685730576515198.\n",
      "epoch: 45, batch: 41, loss: 0.6377438306808472.\n",
      "epoch: 45, batch: 42, loss: 0.6526751518249512.\n",
      "epoch: 45, batch: 43, loss: 0.5531378984451294.\n",
      "epoch: 45, batch: 44, loss: 0.5552615523338318.\n",
      "epoch: 45, batch: 45, loss: 0.6006690263748169.\n",
      "epoch: 45, batch: 46, loss: 0.7426217198371887.\n",
      "epoch: 45, batch: 47, loss: 0.7302789092063904.\n",
      "epoch: 45, batch: 48, loss: 0.5686478614807129.\n",
      "epoch: 45, batch: 49, loss: 0.5975213050842285.\n",
      "epoch: 45, batch: 50, loss: 1.0133025646209717.\n",
      "epoch: 45, batch: 51, loss: 0.5535308122634888.\n",
      "epoch: 45, batch: 52, loss: 0.5520570874214172.\n",
      "epoch: 46, batch: 0, loss: 0.5603072047233582.\n",
      "epoch: 46, batch: 1, loss: 0.5812674760818481.\n",
      "epoch: 46, batch: 2, loss: 0.6968599557876587.\n",
      "epoch: 46, batch: 3, loss: 0.5620620250701904.\n",
      "epoch: 46, batch: 4, loss: 0.5548748970031738.\n",
      "epoch: 46, batch: 5, loss: 0.5555897951126099.\n",
      "epoch: 46, batch: 6, loss: 0.5541640520095825.\n",
      "epoch: 46, batch: 7, loss: 0.5549460053443909.\n",
      "epoch: 46, batch: 8, loss: 0.5825619101524353.\n",
      "epoch: 46, batch: 9, loss: 0.8082489371299744.\n",
      "epoch: 46, batch: 10, loss: 0.5544294118881226.\n",
      "epoch: 46, batch: 11, loss: 0.5813109874725342.\n",
      "epoch: 46, batch: 12, loss: 0.5546543598175049.\n",
      "epoch: 46, batch: 13, loss: 0.5646771788597107.\n",
      "epoch: 46, batch: 14, loss: 0.570348858833313.\n",
      "epoch: 46, batch: 15, loss: 0.5577699542045593.\n",
      "epoch: 46, batch: 16, loss: 0.5700160264968872.\n",
      "epoch: 46, batch: 17, loss: 0.5945556163787842.\n",
      "epoch: 46, batch: 18, loss: 0.5666468143463135.\n",
      "epoch: 46, batch: 19, loss: 0.782744288444519.\n",
      "epoch: 46, batch: 20, loss: 0.5560356974601746.\n",
      "epoch: 46, batch: 21, loss: 0.5727214813232422.\n",
      "epoch: 46, batch: 22, loss: 0.6315301060676575.\n",
      "epoch: 46, batch: 23, loss: 0.7947424650192261.\n",
      "epoch: 46, batch: 24, loss: 0.553736686706543.\n",
      "epoch: 46, batch: 25, loss: 0.5546611547470093.\n",
      "epoch: 46, batch: 26, loss: 0.5587775707244873.\n",
      "epoch: 46, batch: 27, loss: 0.5938395261764526.\n",
      "epoch: 46, batch: 28, loss: 0.660403847694397.\n",
      "epoch: 46, batch: 29, loss: 0.5836403369903564.\n",
      "epoch: 46, batch: 30, loss: 0.5603642463684082.\n",
      "epoch: 46, batch: 31, loss: 0.6249958276748657.\n",
      "epoch: 46, batch: 32, loss: 0.5779308080673218.\n",
      "epoch: 46, batch: 33, loss: 0.624416708946228.\n",
      "epoch: 46, batch: 34, loss: 0.5571320652961731.\n",
      "epoch: 46, batch: 35, loss: 0.656744658946991.\n",
      "epoch: 46, batch: 36, loss: 0.7555322051048279.\n",
      "epoch: 46, batch: 37, loss: 0.5577050447463989.\n",
      "epoch: 46, batch: 38, loss: 0.5560678839683533.\n",
      "epoch: 46, batch: 39, loss: 0.5892611145973206.\n",
      "epoch: 46, batch: 40, loss: 0.6392098069190979.\n",
      "epoch: 46, batch: 41, loss: 0.5563005805015564.\n",
      "epoch: 46, batch: 42, loss: 0.6345748901367188.\n",
      "epoch: 46, batch: 43, loss: 0.5539534687995911.\n",
      "epoch: 46, batch: 44, loss: 0.5532479286193848.\n",
      "epoch: 46, batch: 45, loss: 0.5553640723228455.\n",
      "epoch: 46, batch: 46, loss: 0.5598366260528564.\n",
      "epoch: 46, batch: 47, loss: 0.5718188285827637.\n",
      "epoch: 46, batch: 48, loss: 0.5544624924659729.\n",
      "epoch: 46, batch: 49, loss: 0.5537691712379456.\n",
      "epoch: 46, batch: 50, loss: 0.5643330216407776.\n",
      "epoch: 46, batch: 51, loss: 0.5566878914833069.\n",
      "epoch: 46, batch: 52, loss: 0.5526621341705322.\n",
      "epoch: 47, batch: 0, loss: 0.6963050365447998.\n",
      "epoch: 47, batch: 1, loss: 0.5567901134490967.\n",
      "epoch: 47, batch: 2, loss: 0.7461350560188293.\n",
      "epoch: 47, batch: 3, loss: 0.556128740310669.\n",
      "epoch: 47, batch: 4, loss: 0.7585564851760864.\n",
      "epoch: 47, batch: 5, loss: 0.5760327577590942.\n",
      "epoch: 47, batch: 6, loss: 0.5551449060440063.\n",
      "epoch: 47, batch: 7, loss: 0.5717741250991821.\n",
      "epoch: 47, batch: 8, loss: 0.5630619525909424.\n",
      "epoch: 47, batch: 9, loss: 0.6312670707702637.\n",
      "epoch: 47, batch: 10, loss: 0.5617247223854065.\n",
      "epoch: 47, batch: 11, loss: 0.5551762580871582.\n",
      "epoch: 47, batch: 12, loss: 0.5699162483215332.\n",
      "epoch: 47, batch: 13, loss: 0.5717981457710266.\n",
      "epoch: 47, batch: 14, loss: 0.5642703175544739.\n",
      "epoch: 47, batch: 15, loss: 0.555091142654419.\n",
      "epoch: 47, batch: 16, loss: 0.5582103133201599.\n",
      "epoch: 47, batch: 17, loss: 0.5604661703109741.\n",
      "epoch: 47, batch: 18, loss: 0.5526774525642395.\n",
      "epoch: 47, batch: 19, loss: 0.5552687644958496.\n",
      "epoch: 47, batch: 20, loss: 0.5537440180778503.\n",
      "epoch: 47, batch: 21, loss: 0.5557782649993896.\n",
      "epoch: 47, batch: 22, loss: 0.5589529275894165.\n",
      "epoch: 47, batch: 23, loss: 0.5527850389480591.\n",
      "epoch: 47, batch: 24, loss: 0.6680595874786377.\n",
      "epoch: 47, batch: 25, loss: 0.5657877922058105.\n",
      "epoch: 47, batch: 26, loss: 0.5585461854934692.\n",
      "epoch: 47, batch: 27, loss: 0.6198651790618896.\n",
      "epoch: 47, batch: 28, loss: 0.6246709227561951.\n",
      "epoch: 47, batch: 29, loss: 0.5701903700828552.\n",
      "epoch: 47, batch: 30, loss: 0.553676962852478.\n",
      "epoch: 47, batch: 31, loss: 0.5694617629051208.\n",
      "epoch: 47, batch: 32, loss: 0.5712897777557373.\n",
      "epoch: 47, batch: 33, loss: 0.5718073844909668.\n",
      "epoch: 47, batch: 34, loss: 0.6160354614257812.\n",
      "epoch: 47, batch: 35, loss: 0.6457051038742065.\n",
      "epoch: 47, batch: 36, loss: 0.5570868253707886.\n",
      "epoch: 47, batch: 37, loss: 0.5824607610702515.\n",
      "epoch: 47, batch: 38, loss: 0.6536190509796143.\n",
      "epoch: 47, batch: 39, loss: 0.5568561553955078.\n",
      "epoch: 47, batch: 40, loss: 0.5555887222290039.\n",
      "epoch: 47, batch: 41, loss: 0.5869301557540894.\n",
      "epoch: 47, batch: 42, loss: 0.554323136806488.\n",
      "epoch: 47, batch: 43, loss: 0.8362075090408325.\n",
      "epoch: 47, batch: 44, loss: 0.5648756623268127.\n",
      "epoch: 47, batch: 45, loss: 0.5543642044067383.\n",
      "epoch: 47, batch: 46, loss: 0.5546033382415771.\n",
      "epoch: 47, batch: 47, loss: 0.5547327399253845.\n",
      "epoch: 47, batch: 48, loss: 0.6024436950683594.\n",
      "epoch: 47, batch: 49, loss: 0.8318387866020203.\n",
      "epoch: 47, batch: 50, loss: 0.5542936325073242.\n",
      "epoch: 47, batch: 51, loss: 0.6034359931945801.\n",
      "epoch: 47, batch: 52, loss: 0.5518938302993774.\n",
      "epoch: 48, batch: 0, loss: 0.5599777698516846.\n",
      "epoch: 48, batch: 1, loss: 0.562833309173584.\n",
      "epoch: 48, batch: 2, loss: 0.5532751083374023.\n",
      "epoch: 48, batch: 3, loss: 0.6808065176010132.\n",
      "epoch: 48, batch: 4, loss: 0.5784906148910522.\n",
      "epoch: 48, batch: 5, loss: 0.5592490434646606.\n",
      "epoch: 48, batch: 6, loss: 0.5640982389450073.\n",
      "epoch: 48, batch: 7, loss: 0.5839533805847168.\n",
      "epoch: 48, batch: 8, loss: 0.5606793165206909.\n",
      "epoch: 48, batch: 9, loss: 0.5527386665344238.\n",
      "epoch: 48, batch: 10, loss: 0.5625163316726685.\n",
      "epoch: 48, batch: 11, loss: 0.574178159236908.\n",
      "epoch: 48, batch: 12, loss: 0.5886971950531006.\n",
      "epoch: 48, batch: 13, loss: 0.5844385623931885.\n",
      "epoch: 48, batch: 14, loss: 0.554198682308197.\n",
      "epoch: 48, batch: 15, loss: 0.571298360824585.\n",
      "epoch: 48, batch: 16, loss: 0.7729721069335938.\n",
      "epoch: 48, batch: 17, loss: 0.5552671551704407.\n",
      "epoch: 48, batch: 18, loss: 0.5573714375495911.\n",
      "epoch: 48, batch: 19, loss: 0.6192114949226379.\n",
      "epoch: 48, batch: 20, loss: 0.5550217628479004.\n",
      "epoch: 48, batch: 21, loss: 0.5566699504852295.\n",
      "epoch: 48, batch: 22, loss: 0.5600869655609131.\n",
      "epoch: 48, batch: 23, loss: 0.5593745708465576.\n",
      "epoch: 48, batch: 24, loss: 0.5560182332992554.\n",
      "epoch: 48, batch: 25, loss: 0.7358320951461792.\n",
      "epoch: 48, batch: 26, loss: 0.6174496412277222.\n",
      "epoch: 48, batch: 27, loss: 0.5534877777099609.\n",
      "epoch: 48, batch: 28, loss: 0.57222580909729.\n",
      "epoch: 48, batch: 29, loss: 0.5579210519790649.\n",
      "epoch: 48, batch: 30, loss: 0.5636520385742188.\n",
      "epoch: 48, batch: 31, loss: 0.5531774759292603.\n",
      "epoch: 48, batch: 32, loss: 0.5554759502410889.\n",
      "epoch: 48, batch: 33, loss: 0.8990278244018555.\n",
      "epoch: 48, batch: 34, loss: 0.5881038308143616.\n",
      "epoch: 48, batch: 35, loss: 0.5887402296066284.\n",
      "epoch: 48, batch: 36, loss: 0.9526685476303101.\n",
      "epoch: 48, batch: 37, loss: 0.5749053955078125.\n",
      "epoch: 48, batch: 38, loss: 0.5602083206176758.\n",
      "epoch: 48, batch: 39, loss: 0.5577589273452759.\n",
      "epoch: 48, batch: 40, loss: 0.5654135942459106.\n",
      "epoch: 48, batch: 41, loss: 0.6588120460510254.\n",
      "epoch: 48, batch: 42, loss: 0.556941032409668.\n",
      "epoch: 48, batch: 43, loss: 0.5568068027496338.\n",
      "epoch: 48, batch: 44, loss: 0.5535844564437866.\n",
      "epoch: 48, batch: 45, loss: 0.5524921417236328.\n",
      "epoch: 48, batch: 46, loss: 0.5532841682434082.\n",
      "epoch: 48, batch: 47, loss: 0.6174371242523193.\n",
      "epoch: 48, batch: 48, loss: 0.552787184715271.\n",
      "epoch: 48, batch: 49, loss: 0.7630656957626343.\n",
      "epoch: 48, batch: 50, loss: 0.5526182055473328.\n",
      "epoch: 48, batch: 51, loss: 0.564039409160614.\n",
      "epoch: 48, batch: 52, loss: 0.5527195930480957.\n",
      "epoch: 49, batch: 0, loss: 0.5566333532333374.\n",
      "epoch: 49, batch: 1, loss: 0.5619953274726868.\n",
      "epoch: 49, batch: 2, loss: 0.5538365840911865.\n",
      "epoch: 49, batch: 3, loss: 0.5553033947944641.\n",
      "epoch: 49, batch: 4, loss: 0.552984893321991.\n",
      "epoch: 49, batch: 5, loss: 0.5663660764694214.\n",
      "epoch: 49, batch: 6, loss: 0.7352348566055298.\n",
      "epoch: 49, batch: 7, loss: 0.5540435910224915.\n",
      "epoch: 49, batch: 8, loss: 0.5530884861946106.\n",
      "epoch: 49, batch: 9, loss: 0.708856463432312.\n",
      "epoch: 49, batch: 10, loss: 0.560350775718689.\n",
      "epoch: 49, batch: 11, loss: 0.5603272914886475.\n",
      "epoch: 49, batch: 12, loss: 0.5559220314025879.\n",
      "epoch: 49, batch: 13, loss: 0.5533466339111328.\n",
      "epoch: 49, batch: 14, loss: 0.5729790329933167.\n",
      "epoch: 49, batch: 15, loss: 0.562526524066925.\n",
      "epoch: 49, batch: 16, loss: 0.5534012317657471.\n",
      "epoch: 49, batch: 17, loss: 0.7370785474777222.\n",
      "epoch: 49, batch: 18, loss: 0.6630851626396179.\n",
      "epoch: 49, batch: 19, loss: 0.5645921230316162.\n",
      "epoch: 49, batch: 20, loss: 0.565847635269165.\n",
      "epoch: 49, batch: 21, loss: 0.7260977625846863.\n",
      "epoch: 49, batch: 22, loss: 0.59610915184021.\n",
      "epoch: 49, batch: 23, loss: 0.5708365440368652.\n",
      "epoch: 49, batch: 24, loss: 0.5707660913467407.\n",
      "epoch: 49, batch: 25, loss: 0.7083067297935486.\n",
      "epoch: 49, batch: 26, loss: 0.7995780110359192.\n",
      "epoch: 49, batch: 27, loss: 0.5603334903717041.\n",
      "epoch: 49, batch: 28, loss: 0.5658853054046631.\n",
      "epoch: 49, batch: 29, loss: 0.5562436580657959.\n",
      "epoch: 49, batch: 30, loss: 0.5532947182655334.\n",
      "epoch: 49, batch: 31, loss: 0.5589380264282227.\n",
      "epoch: 49, batch: 32, loss: 0.599379301071167.\n",
      "epoch: 49, batch: 33, loss: 0.5540533661842346.\n",
      "epoch: 49, batch: 34, loss: 0.5523257851600647.\n",
      "epoch: 49, batch: 35, loss: 0.5572395324707031.\n",
      "epoch: 49, batch: 36, loss: 0.7195553183555603.\n",
      "epoch: 49, batch: 37, loss: 0.6079469919204712.\n",
      "epoch: 49, batch: 38, loss: 0.5902707576751709.\n",
      "epoch: 49, batch: 39, loss: 0.5563710927963257.\n",
      "epoch: 49, batch: 40, loss: 0.5536737442016602.\n",
      "epoch: 49, batch: 41, loss: 0.8184822797775269.\n",
      "epoch: 49, batch: 42, loss: 0.5995774269104004.\n",
      "epoch: 49, batch: 43, loss: 0.5561124086380005.\n",
      "epoch: 49, batch: 44, loss: 0.5751345753669739.\n",
      "epoch: 49, batch: 45, loss: 0.5656252503395081.\n",
      "epoch: 49, batch: 46, loss: 0.6219860315322876.\n",
      "epoch: 49, batch: 47, loss: 0.5838514566421509.\n",
      "epoch: 49, batch: 48, loss: 0.586457371711731.\n",
      "epoch: 49, batch: 49, loss: 0.5607448220252991.\n",
      "epoch: 49, batch: 50, loss: 0.5528520345687866.\n",
      "epoch: 49, batch: 51, loss: 0.5719054341316223.\n",
      "epoch: 49, batch: 52, loss: 0.5556178092956543.\n",
      "epoch: 50, batch: 0, loss: 0.5525338649749756.\n",
      "epoch: 50, batch: 1, loss: 0.5611577033996582.\n",
      "epoch: 50, batch: 2, loss: 0.5525908470153809.\n",
      "epoch: 50, batch: 3, loss: 0.5761789083480835.\n",
      "epoch: 50, batch: 4, loss: 0.5575768947601318.\n",
      "epoch: 50, batch: 5, loss: 0.5623028874397278.\n",
      "epoch: 50, batch: 6, loss: 0.5524930357933044.\n",
      "epoch: 50, batch: 7, loss: 0.5890144109725952.\n",
      "epoch: 50, batch: 8, loss: 0.5554450750350952.\n",
      "epoch: 50, batch: 9, loss: 0.5624828338623047.\n",
      "epoch: 50, batch: 10, loss: 0.5602242946624756.\n",
      "epoch: 50, batch: 11, loss: 0.5849728584289551.\n",
      "epoch: 50, batch: 12, loss: 0.5536605715751648.\n",
      "epoch: 50, batch: 13, loss: 0.6104974746704102.\n",
      "epoch: 50, batch: 14, loss: 0.5552298426628113.\n",
      "epoch: 50, batch: 15, loss: 0.5560152530670166.\n",
      "epoch: 50, batch: 16, loss: 0.5670880079269409.\n",
      "epoch: 50, batch: 17, loss: 0.5548028349876404.\n",
      "epoch: 50, batch: 18, loss: 0.5825471878051758.\n",
      "epoch: 50, batch: 19, loss: 0.58406662940979.\n",
      "epoch: 50, batch: 20, loss: 0.655923068523407.\n",
      "epoch: 50, batch: 21, loss: 0.5539159178733826.\n",
      "epoch: 50, batch: 22, loss: 0.5721299648284912.\n",
      "epoch: 50, batch: 23, loss: 0.5602061748504639.\n",
      "epoch: 50, batch: 24, loss: 0.5699348449707031.\n",
      "epoch: 50, batch: 25, loss: 0.6550276279449463.\n",
      "epoch: 50, batch: 26, loss: 0.7453005313873291.\n",
      "epoch: 50, batch: 27, loss: 0.5552164316177368.\n",
      "epoch: 50, batch: 28, loss: 0.5880914926528931.\n",
      "epoch: 50, batch: 29, loss: 0.5771992206573486.\n",
      "epoch: 50, batch: 30, loss: 0.5544439554214478.\n",
      "epoch: 50, batch: 31, loss: 0.7134962677955627.\n",
      "epoch: 50, batch: 32, loss: 0.5701289176940918.\n",
      "epoch: 50, batch: 33, loss: 0.5527779459953308.\n",
      "epoch: 50, batch: 34, loss: 0.6663331985473633.\n",
      "epoch: 50, batch: 35, loss: 0.552558183670044.\n",
      "epoch: 50, batch: 36, loss: 0.553887128829956.\n",
      "epoch: 50, batch: 37, loss: 0.5553643703460693.\n",
      "epoch: 50, batch: 38, loss: 0.7758499383926392.\n",
      "epoch: 50, batch: 39, loss: 0.5558699369430542.\n",
      "epoch: 50, batch: 40, loss: 0.7562816143035889.\n",
      "epoch: 50, batch: 41, loss: 0.6221708059310913.\n",
      "epoch: 50, batch: 42, loss: 0.5689441561698914.\n",
      "epoch: 50, batch: 43, loss: 0.8802415132522583.\n",
      "epoch: 50, batch: 44, loss: 0.6886862516403198.\n",
      "epoch: 50, batch: 45, loss: 0.6026419401168823.\n",
      "epoch: 50, batch: 46, loss: 0.5631734132766724.\n",
      "epoch: 50, batch: 47, loss: 0.5538383722305298.\n",
      "epoch: 50, batch: 48, loss: 0.5644969344139099.\n",
      "epoch: 50, batch: 49, loss: 0.5524414777755737.\n",
      "epoch: 50, batch: 50, loss: 0.5555500984191895.\n",
      "epoch: 50, batch: 51, loss: 0.5553561449050903.\n",
      "epoch: 50, batch: 52, loss: 0.6280032396316528.\n",
      "epoch: 51, batch: 0, loss: 0.5760369300842285.\n",
      "epoch: 51, batch: 1, loss: 0.5950849056243896.\n",
      "epoch: 51, batch: 2, loss: 0.863764762878418.\n",
      "epoch: 51, batch: 3, loss: 0.5570023655891418.\n",
      "epoch: 51, batch: 4, loss: 0.5575826168060303.\n",
      "epoch: 51, batch: 5, loss: 0.564414381980896.\n",
      "epoch: 51, batch: 6, loss: 0.5571203827857971.\n",
      "epoch: 51, batch: 7, loss: 0.5659267902374268.\n",
      "epoch: 51, batch: 8, loss: 0.5558543801307678.\n",
      "epoch: 51, batch: 9, loss: 0.5547954440116882.\n",
      "epoch: 51, batch: 10, loss: 0.7468587160110474.\n",
      "epoch: 51, batch: 11, loss: 0.577338457107544.\n",
      "epoch: 51, batch: 12, loss: 0.5526353120803833.\n",
      "epoch: 51, batch: 13, loss: 0.5561676025390625.\n",
      "epoch: 51, batch: 14, loss: 0.5909892320632935.\n",
      "epoch: 51, batch: 15, loss: 0.5526604056358337.\n",
      "epoch: 51, batch: 16, loss: 0.5711357593536377.\n",
      "epoch: 51, batch: 17, loss: 0.7754350900650024.\n",
      "epoch: 51, batch: 18, loss: 0.5573790073394775.\n",
      "epoch: 51, batch: 19, loss: 0.5537317991256714.\n",
      "epoch: 51, batch: 20, loss: 0.6074137687683105.\n",
      "epoch: 51, batch: 21, loss: 0.5534411668777466.\n",
      "epoch: 51, batch: 22, loss: 0.553301215171814.\n",
      "epoch: 51, batch: 23, loss: 0.5552076101303101.\n",
      "epoch: 51, batch: 24, loss: 0.5601580142974854.\n",
      "epoch: 51, batch: 25, loss: 0.7825878858566284.\n",
      "epoch: 51, batch: 26, loss: 0.6556106209754944.\n",
      "epoch: 51, batch: 27, loss: 0.5819202661514282.\n",
      "epoch: 51, batch: 28, loss: 0.8189172148704529.\n",
      "epoch: 51, batch: 29, loss: 0.5566955804824829.\n",
      "epoch: 51, batch: 30, loss: 0.5538569688796997.\n",
      "epoch: 51, batch: 31, loss: 0.6703961491584778.\n",
      "epoch: 51, batch: 32, loss: 0.5754337310791016.\n",
      "epoch: 51, batch: 33, loss: 0.5610859394073486.\n",
      "epoch: 51, batch: 34, loss: 0.5575149059295654.\n",
      "epoch: 51, batch: 35, loss: 0.5542382001876831.\n",
      "epoch: 51, batch: 36, loss: 0.5572867393493652.\n",
      "epoch: 51, batch: 37, loss: 0.5625342130661011.\n",
      "epoch: 51, batch: 38, loss: 0.5724372863769531.\n",
      "epoch: 51, batch: 39, loss: 0.5560240149497986.\n",
      "epoch: 51, batch: 40, loss: 0.6139355897903442.\n",
      "epoch: 51, batch: 41, loss: 0.5550252199172974.\n",
      "epoch: 51, batch: 42, loss: 0.5524295568466187.\n",
      "epoch: 51, batch: 43, loss: 0.6750744581222534.\n",
      "epoch: 51, batch: 44, loss: 0.5543456077575684.\n",
      "epoch: 51, batch: 45, loss: 0.5587244033813477.\n",
      "epoch: 51, batch: 46, loss: 0.5540353655815125.\n",
      "epoch: 51, batch: 47, loss: 0.6514107584953308.\n",
      "epoch: 51, batch: 48, loss: 0.554885745048523.\n",
      "epoch: 51, batch: 49, loss: 0.5562491416931152.\n",
      "epoch: 51, batch: 50, loss: 0.6048588752746582.\n",
      "epoch: 51, batch: 51, loss: 0.6303189992904663.\n",
      "epoch: 51, batch: 52, loss: 0.6382060647010803.\n",
      "epoch: 52, batch: 0, loss: 0.588464081287384.\n",
      "epoch: 52, batch: 1, loss: 0.5556588768959045.\n",
      "epoch: 52, batch: 2, loss: 0.5576694011688232.\n",
      "epoch: 52, batch: 3, loss: 0.6836687326431274.\n",
      "epoch: 52, batch: 4, loss: 0.5675896406173706.\n",
      "epoch: 52, batch: 5, loss: 0.554817259311676.\n",
      "epoch: 52, batch: 6, loss: 0.5653232932090759.\n",
      "epoch: 52, batch: 7, loss: 0.5542936325073242.\n",
      "epoch: 52, batch: 8, loss: 0.6806124448776245.\n",
      "epoch: 52, batch: 9, loss: 0.5607969164848328.\n",
      "epoch: 52, batch: 10, loss: 0.5529192090034485.\n",
      "epoch: 52, batch: 11, loss: 0.5538854598999023.\n",
      "epoch: 52, batch: 12, loss: 0.5570478439331055.\n",
      "epoch: 52, batch: 13, loss: 0.572738766670227.\n",
      "epoch: 52, batch: 14, loss: 0.5774362087249756.\n",
      "epoch: 52, batch: 15, loss: 0.5583169460296631.\n",
      "epoch: 52, batch: 16, loss: 0.5826271176338196.\n",
      "epoch: 52, batch: 17, loss: 0.5776625871658325.\n",
      "epoch: 52, batch: 18, loss: 0.555142879486084.\n",
      "epoch: 52, batch: 19, loss: 0.5521667003631592.\n",
      "epoch: 52, batch: 20, loss: 0.6095693111419678.\n",
      "epoch: 52, batch: 21, loss: 0.5734966397285461.\n",
      "epoch: 52, batch: 22, loss: 0.5671740770339966.\n",
      "epoch: 52, batch: 23, loss: 0.5600870847702026.\n",
      "epoch: 52, batch: 24, loss: 0.5728198289871216.\n",
      "epoch: 52, batch: 25, loss: 0.6732136011123657.\n",
      "epoch: 52, batch: 26, loss: 0.5521645545959473.\n",
      "epoch: 52, batch: 27, loss: 0.8220950365066528.\n",
      "epoch: 52, batch: 28, loss: 0.5578789710998535.\n",
      "epoch: 52, batch: 29, loss: 0.5793610215187073.\n",
      "epoch: 52, batch: 30, loss: 0.5533998012542725.\n",
      "epoch: 52, batch: 31, loss: 0.5595587491989136.\n",
      "epoch: 52, batch: 32, loss: 0.7213776111602783.\n",
      "epoch: 52, batch: 33, loss: 0.5603963136672974.\n",
      "epoch: 52, batch: 34, loss: 0.5538133978843689.\n",
      "epoch: 52, batch: 35, loss: 0.5590099096298218.\n",
      "epoch: 52, batch: 36, loss: 0.55780029296875.\n",
      "epoch: 52, batch: 37, loss: 0.6561295390129089.\n",
      "epoch: 52, batch: 38, loss: 0.5594873428344727.\n",
      "epoch: 52, batch: 39, loss: 0.5591546297073364.\n",
      "epoch: 52, batch: 40, loss: 0.5594414472579956.\n",
      "epoch: 52, batch: 41, loss: 0.5524892807006836.\n",
      "epoch: 52, batch: 42, loss: 0.6238163709640503.\n",
      "epoch: 52, batch: 43, loss: 0.553815484046936.\n",
      "epoch: 52, batch: 44, loss: 0.5592297315597534.\n",
      "epoch: 52, batch: 45, loss: 0.7796335220336914.\n",
      "epoch: 52, batch: 46, loss: 0.5557783842086792.\n",
      "epoch: 52, batch: 47, loss: 0.6214150190353394.\n",
      "epoch: 52, batch: 48, loss: 0.5526374578475952.\n",
      "epoch: 52, batch: 49, loss: 0.5721641778945923.\n",
      "epoch: 52, batch: 50, loss: 0.8460929989814758.\n",
      "epoch: 52, batch: 51, loss: 0.5575433969497681.\n",
      "epoch: 52, batch: 52, loss: 0.5874861478805542.\n",
      "epoch: 53, batch: 0, loss: 0.5968573093414307.\n",
      "epoch: 53, batch: 1, loss: 0.5546333193778992.\n",
      "epoch: 53, batch: 2, loss: 0.5689864158630371.\n",
      "epoch: 53, batch: 3, loss: 0.5960257053375244.\n",
      "epoch: 53, batch: 4, loss: 0.6284431219100952.\n",
      "epoch: 53, batch: 5, loss: 0.603408932685852.\n",
      "epoch: 53, batch: 6, loss: 0.5533824563026428.\n",
      "epoch: 53, batch: 7, loss: 0.5522165894508362.\n",
      "epoch: 53, batch: 8, loss: 0.558751106262207.\n",
      "epoch: 53, batch: 9, loss: 0.6691274642944336.\n",
      "epoch: 53, batch: 10, loss: 0.5810269713401794.\n",
      "epoch: 53, batch: 11, loss: 0.5597554445266724.\n",
      "epoch: 53, batch: 12, loss: 0.5534158945083618.\n",
      "epoch: 53, batch: 13, loss: 0.5976296663284302.\n",
      "epoch: 53, batch: 14, loss: 0.8781003952026367.\n",
      "epoch: 53, batch: 15, loss: 0.5981439352035522.\n",
      "epoch: 53, batch: 16, loss: 0.5579689741134644.\n",
      "epoch: 53, batch: 17, loss: 0.5558016300201416.\n",
      "epoch: 53, batch: 18, loss: 0.7343852519989014.\n",
      "epoch: 53, batch: 19, loss: 0.5526522397994995.\n",
      "epoch: 53, batch: 20, loss: 0.5531879663467407.\n",
      "epoch: 53, batch: 21, loss: 0.5544992685317993.\n",
      "epoch: 53, batch: 22, loss: 0.5686864852905273.\n",
      "epoch: 53, batch: 23, loss: 0.5606702566146851.\n",
      "epoch: 53, batch: 24, loss: 0.5616716146469116.\n",
      "epoch: 53, batch: 25, loss: 0.5719072222709656.\n",
      "epoch: 53, batch: 26, loss: 0.8207643032073975.\n",
      "epoch: 53, batch: 27, loss: 0.5638629794120789.\n",
      "epoch: 53, batch: 28, loss: 0.5595878958702087.\n",
      "epoch: 53, batch: 29, loss: 0.5530499815940857.\n",
      "epoch: 53, batch: 30, loss: 0.5532572865486145.\n",
      "epoch: 53, batch: 31, loss: 0.5541194677352905.\n",
      "epoch: 53, batch: 32, loss: 0.5579369068145752.\n",
      "epoch: 53, batch: 33, loss: 0.554252028465271.\n",
      "epoch: 53, batch: 34, loss: 0.5539835691452026.\n",
      "epoch: 53, batch: 35, loss: 0.5560851097106934.\n",
      "epoch: 53, batch: 36, loss: 0.554841935634613.\n",
      "epoch: 53, batch: 37, loss: 0.5743260383605957.\n",
      "epoch: 53, batch: 38, loss: 0.5555261373519897.\n",
      "epoch: 53, batch: 39, loss: 0.5773907899856567.\n",
      "epoch: 53, batch: 40, loss: 0.5535381436347961.\n",
      "epoch: 53, batch: 41, loss: 0.5606405735015869.\n",
      "epoch: 53, batch: 42, loss: 0.5884160995483398.\n",
      "epoch: 53, batch: 43, loss: 0.5762894749641418.\n",
      "epoch: 53, batch: 44, loss: 0.5548391342163086.\n",
      "epoch: 53, batch: 45, loss: 0.556167483329773.\n",
      "epoch: 53, batch: 46, loss: 0.881497323513031.\n",
      "epoch: 53, batch: 47, loss: 0.5521122217178345.\n",
      "epoch: 53, batch: 48, loss: 0.5890620946884155.\n",
      "epoch: 53, batch: 49, loss: 0.603774905204773.\n",
      "epoch: 53, batch: 50, loss: 0.6847466230392456.\n",
      "epoch: 53, batch: 51, loss: 0.5534412860870361.\n",
      "epoch: 53, batch: 52, loss: 0.5707353353500366.\n",
      "epoch: 54, batch: 0, loss: 0.5662851333618164.\n",
      "epoch: 54, batch: 1, loss: 0.5532430410385132.\n",
      "epoch: 54, batch: 2, loss: 0.5875221490859985.\n",
      "epoch: 54, batch: 3, loss: 0.5537549257278442.\n",
      "epoch: 54, batch: 4, loss: 0.5600825548171997.\n",
      "epoch: 54, batch: 5, loss: 0.5892300605773926.\n",
      "epoch: 54, batch: 6, loss: 0.5523453950881958.\n",
      "epoch: 54, batch: 7, loss: 0.7575459480285645.\n",
      "epoch: 54, batch: 8, loss: 0.5662869215011597.\n",
      "epoch: 54, batch: 9, loss: 0.5579240918159485.\n",
      "epoch: 54, batch: 10, loss: 0.5526837110519409.\n",
      "epoch: 54, batch: 11, loss: 0.6434147357940674.\n",
      "epoch: 54, batch: 12, loss: 0.553117036819458.\n",
      "epoch: 54, batch: 13, loss: 0.8373020887374878.\n",
      "epoch: 54, batch: 14, loss: 0.5540906190872192.\n",
      "epoch: 54, batch: 15, loss: 0.8549084663391113.\n",
      "epoch: 54, batch: 16, loss: 0.5559774041175842.\n",
      "epoch: 54, batch: 17, loss: 0.7172940373420715.\n",
      "epoch: 54, batch: 18, loss: 0.5573733448982239.\n",
      "epoch: 54, batch: 19, loss: 0.5762418508529663.\n",
      "epoch: 54, batch: 20, loss: 0.5533504486083984.\n",
      "epoch: 54, batch: 21, loss: 0.5682214498519897.\n",
      "epoch: 54, batch: 22, loss: 0.6101466417312622.\n",
      "epoch: 54, batch: 23, loss: 0.5593734979629517.\n",
      "epoch: 54, batch: 24, loss: 0.5562094449996948.\n",
      "epoch: 54, batch: 25, loss: 0.5541355609893799.\n",
      "epoch: 54, batch: 26, loss: 0.5578621625900269.\n",
      "epoch: 54, batch: 27, loss: 0.5559809803962708.\n",
      "epoch: 54, batch: 28, loss: 0.6509042382240295.\n",
      "epoch: 54, batch: 29, loss: 0.5546352863311768.\n",
      "epoch: 54, batch: 30, loss: 0.6524167060852051.\n",
      "epoch: 54, batch: 31, loss: 0.560356616973877.\n",
      "epoch: 54, batch: 32, loss: 0.5560550689697266.\n",
      "epoch: 54, batch: 33, loss: 0.6019497513771057.\n",
      "epoch: 54, batch: 34, loss: 0.6808960437774658.\n",
      "epoch: 54, batch: 35, loss: 0.562908411026001.\n",
      "epoch: 54, batch: 36, loss: 0.5541930198669434.\n",
      "epoch: 54, batch: 37, loss: 0.5743377208709717.\n",
      "epoch: 54, batch: 38, loss: 0.556065559387207.\n",
      "epoch: 54, batch: 39, loss: 0.5567224621772766.\n",
      "epoch: 54, batch: 40, loss: 0.6206127405166626.\n",
      "epoch: 54, batch: 41, loss: 0.5572841167449951.\n",
      "epoch: 54, batch: 42, loss: 0.5522431135177612.\n",
      "epoch: 54, batch: 43, loss: 0.5538390874862671.\n",
      "epoch: 54, batch: 44, loss: 0.5520946383476257.\n",
      "epoch: 54, batch: 45, loss: 0.5645936131477356.\n",
      "epoch: 54, batch: 46, loss: 0.5805073976516724.\n",
      "epoch: 54, batch: 47, loss: 0.5536230802536011.\n",
      "epoch: 54, batch: 48, loss: 0.5877313613891602.\n",
      "epoch: 54, batch: 49, loss: 0.5655192136764526.\n",
      "epoch: 54, batch: 50, loss: 0.5725690126419067.\n",
      "epoch: 54, batch: 51, loss: 0.5742653608322144.\n",
      "epoch: 54, batch: 52, loss: 0.5524629354476929.\n",
      "epoch: 55, batch: 0, loss: 0.5587722659111023.\n",
      "epoch: 55, batch: 1, loss: 0.6209008693695068.\n",
      "epoch: 55, batch: 2, loss: 0.5886173248291016.\n",
      "epoch: 55, batch: 3, loss: 0.7311714887619019.\n",
      "epoch: 55, batch: 4, loss: 0.5596643686294556.\n",
      "epoch: 55, batch: 5, loss: 0.5521783828735352.\n",
      "epoch: 55, batch: 6, loss: 0.5540497303009033.\n",
      "epoch: 55, batch: 7, loss: 0.5661360025405884.\n",
      "epoch: 55, batch: 8, loss: 0.6477351188659668.\n",
      "epoch: 55, batch: 9, loss: 0.574339747428894.\n",
      "epoch: 55, batch: 10, loss: 0.6556980013847351.\n",
      "epoch: 55, batch: 11, loss: 0.5546084642410278.\n",
      "epoch: 55, batch: 12, loss: 0.552614152431488.\n",
      "epoch: 55, batch: 13, loss: 0.5522704124450684.\n",
      "epoch: 55, batch: 14, loss: 0.5522698163986206.\n",
      "epoch: 55, batch: 15, loss: 0.625389814376831.\n",
      "epoch: 55, batch: 16, loss: 0.5544885396957397.\n",
      "epoch: 55, batch: 17, loss: 0.8259839415550232.\n",
      "epoch: 55, batch: 18, loss: 0.8544490337371826.\n",
      "epoch: 55, batch: 19, loss: 0.5545755624771118.\n",
      "epoch: 55, batch: 20, loss: 0.5558899641036987.\n",
      "epoch: 55, batch: 21, loss: 0.555292546749115.\n",
      "epoch: 55, batch: 22, loss: 0.555362343788147.\n",
      "epoch: 55, batch: 23, loss: 0.5546020269393921.\n",
      "epoch: 55, batch: 24, loss: 0.5572904944419861.\n",
      "epoch: 55, batch: 25, loss: 0.6212096214294434.\n",
      "epoch: 55, batch: 26, loss: 0.5862245559692383.\n",
      "epoch: 55, batch: 27, loss: 0.580514669418335.\n",
      "epoch: 55, batch: 28, loss: 0.5526444911956787.\n",
      "epoch: 55, batch: 29, loss: 0.5532075762748718.\n",
      "epoch: 55, batch: 30, loss: 0.6541750431060791.\n",
      "epoch: 55, batch: 31, loss: 0.5640419125556946.\n",
      "epoch: 55, batch: 32, loss: 0.5899368524551392.\n",
      "epoch: 55, batch: 33, loss: 0.553780198097229.\n",
      "epoch: 55, batch: 34, loss: 0.5571411848068237.\n",
      "epoch: 55, batch: 35, loss: 0.5744997262954712.\n",
      "epoch: 55, batch: 36, loss: 0.7324777841567993.\n",
      "epoch: 55, batch: 37, loss: 0.5546144247055054.\n",
      "epoch: 55, batch: 38, loss: 0.5569856762886047.\n",
      "epoch: 55, batch: 39, loss: 0.5558266639709473.\n",
      "epoch: 55, batch: 40, loss: 0.5717876553535461.\n",
      "epoch: 55, batch: 41, loss: 0.5605671405792236.\n",
      "epoch: 55, batch: 42, loss: 0.5591537952423096.\n",
      "epoch: 55, batch: 43, loss: 0.5544445514678955.\n",
      "epoch: 55, batch: 44, loss: 0.5533782839775085.\n",
      "epoch: 55, batch: 45, loss: 0.5596075057983398.\n",
      "epoch: 55, batch: 46, loss: 0.5531632900238037.\n",
      "epoch: 55, batch: 47, loss: 0.5590320825576782.\n",
      "epoch: 55, batch: 48, loss: 0.5536532998085022.\n",
      "epoch: 55, batch: 49, loss: 0.5530238151550293.\n",
      "epoch: 55, batch: 50, loss: 0.690304696559906.\n",
      "epoch: 55, batch: 51, loss: 0.6471903920173645.\n",
      "epoch: 55, batch: 52, loss: 0.5538583397865295.\n",
      "epoch: 56, batch: 0, loss: 0.5533812642097473.\n",
      "epoch: 56, batch: 1, loss: 0.5524670481681824.\n",
      "epoch: 56, batch: 2, loss: 0.7959017753601074.\n",
      "epoch: 56, batch: 3, loss: 0.5568105578422546.\n",
      "epoch: 56, batch: 4, loss: 0.565922737121582.\n",
      "epoch: 56, batch: 5, loss: 0.5683207511901855.\n",
      "epoch: 56, batch: 6, loss: 0.5735155344009399.\n",
      "epoch: 56, batch: 7, loss: 0.5668622255325317.\n",
      "epoch: 56, batch: 8, loss: 0.5525588393211365.\n",
      "epoch: 56, batch: 9, loss: 0.5596794486045837.\n",
      "epoch: 56, batch: 10, loss: 0.5553776025772095.\n",
      "epoch: 56, batch: 11, loss: 0.583954930305481.\n",
      "epoch: 56, batch: 12, loss: 0.6022651791572571.\n",
      "epoch: 56, batch: 13, loss: 0.5554490089416504.\n",
      "epoch: 56, batch: 14, loss: 0.5909818410873413.\n",
      "epoch: 56, batch: 15, loss: 0.5538250207901001.\n",
      "epoch: 56, batch: 16, loss: 0.6869799494743347.\n",
      "epoch: 56, batch: 17, loss: 0.5613542795181274.\n",
      "epoch: 56, batch: 18, loss: 0.559267520904541.\n",
      "epoch: 56, batch: 19, loss: 0.6467975974082947.\n",
      "epoch: 56, batch: 20, loss: 0.5545554161071777.\n",
      "epoch: 56, batch: 21, loss: 0.567111611366272.\n",
      "epoch: 56, batch: 22, loss: 0.5544345378875732.\n",
      "epoch: 56, batch: 23, loss: 0.5524693727493286.\n",
      "epoch: 56, batch: 24, loss: 0.5621353387832642.\n",
      "epoch: 56, batch: 25, loss: 0.7333132028579712.\n",
      "epoch: 56, batch: 26, loss: 0.9908447265625.\n",
      "epoch: 56, batch: 27, loss: 0.5540102124214172.\n",
      "epoch: 56, batch: 28, loss: 0.5540008544921875.\n",
      "epoch: 56, batch: 29, loss: 0.5540812015533447.\n",
      "epoch: 56, batch: 30, loss: 0.5527888536453247.\n",
      "epoch: 56, batch: 31, loss: 0.5588424205780029.\n",
      "epoch: 56, batch: 32, loss: 0.5545421838760376.\n",
      "epoch: 56, batch: 33, loss: 0.5555356740951538.\n",
      "epoch: 56, batch: 34, loss: 0.5529381036758423.\n",
      "epoch: 56, batch: 35, loss: 0.5528889894485474.\n",
      "epoch: 56, batch: 36, loss: 0.552895188331604.\n",
      "epoch: 56, batch: 37, loss: 0.5634682178497314.\n",
      "epoch: 56, batch: 38, loss: 0.6596024036407471.\n",
      "epoch: 56, batch: 39, loss: 0.5732936859130859.\n",
      "epoch: 56, batch: 40, loss: 0.6046029925346375.\n",
      "epoch: 56, batch: 41, loss: 0.5522992014884949.\n",
      "epoch: 56, batch: 42, loss: 0.5653331279754639.\n",
      "epoch: 56, batch: 43, loss: 0.5547657012939453.\n",
      "epoch: 56, batch: 44, loss: 0.5878154039382935.\n",
      "epoch: 56, batch: 45, loss: 0.5610994100570679.\n",
      "epoch: 56, batch: 46, loss: 0.5759774446487427.\n",
      "epoch: 56, batch: 47, loss: 0.7492883801460266.\n",
      "epoch: 56, batch: 48, loss: 0.5522270202636719.\n",
      "epoch: 56, batch: 49, loss: 0.5523463487625122.\n",
      "epoch: 56, batch: 50, loss: 0.6543521881103516.\n",
      "epoch: 56, batch: 51, loss: 0.5536128282546997.\n",
      "epoch: 56, batch: 52, loss: 0.5557460784912109.\n",
      "epoch: 57, batch: 0, loss: 0.5541253089904785.\n",
      "epoch: 57, batch: 1, loss: 0.5592020750045776.\n",
      "epoch: 57, batch: 2, loss: 0.5734000205993652.\n",
      "epoch: 57, batch: 3, loss: 0.6754553318023682.\n",
      "epoch: 57, batch: 4, loss: 0.9257165193557739.\n",
      "epoch: 57, batch: 5, loss: 0.5596338510513306.\n",
      "epoch: 57, batch: 6, loss: 0.5550225377082825.\n",
      "epoch: 57, batch: 7, loss: 0.5525977611541748.\n",
      "epoch: 57, batch: 8, loss: 0.552688717842102.\n",
      "epoch: 57, batch: 9, loss: 0.5572446584701538.\n",
      "epoch: 57, batch: 10, loss: 0.6090992093086243.\n",
      "epoch: 57, batch: 11, loss: 0.7300435304641724.\n",
      "epoch: 57, batch: 12, loss: 0.5717387199401855.\n",
      "epoch: 57, batch: 13, loss: 0.5555413961410522.\n",
      "epoch: 57, batch: 14, loss: 0.5547221899032593.\n",
      "epoch: 57, batch: 15, loss: 0.5720070600509644.\n",
      "epoch: 57, batch: 16, loss: 0.5588417649269104.\n",
      "epoch: 57, batch: 17, loss: 0.5696739554405212.\n",
      "epoch: 57, batch: 18, loss: 0.6146492958068848.\n",
      "epoch: 57, batch: 19, loss: 0.5533663630485535.\n",
      "epoch: 57, batch: 20, loss: 0.5625482797622681.\n",
      "epoch: 57, batch: 21, loss: 0.5649665594100952.\n",
      "epoch: 57, batch: 22, loss: 0.5591641664505005.\n",
      "epoch: 57, batch: 23, loss: 0.5535469055175781.\n",
      "epoch: 57, batch: 24, loss: 0.5912297964096069.\n",
      "epoch: 57, batch: 25, loss: 0.5572669506072998.\n",
      "epoch: 57, batch: 26, loss: 0.7379260063171387.\n",
      "epoch: 57, batch: 27, loss: 0.5553436875343323.\n",
      "epoch: 57, batch: 28, loss: 0.5570691823959351.\n",
      "epoch: 57, batch: 29, loss: 0.5534780025482178.\n",
      "epoch: 57, batch: 30, loss: 0.5525439977645874.\n",
      "epoch: 57, batch: 31, loss: 0.5669201612472534.\n",
      "epoch: 57, batch: 32, loss: 0.5858513116836548.\n",
      "epoch: 57, batch: 33, loss: 0.5524502992630005.\n",
      "epoch: 57, batch: 34, loss: 0.5521897673606873.\n",
      "epoch: 57, batch: 35, loss: 0.5535706877708435.\n",
      "epoch: 57, batch: 36, loss: 0.5648950338363647.\n",
      "epoch: 57, batch: 37, loss: 0.5536336898803711.\n",
      "epoch: 57, batch: 38, loss: 0.5990637540817261.\n",
      "epoch: 57, batch: 39, loss: 0.6721744537353516.\n",
      "epoch: 57, batch: 40, loss: 0.5571662187576294.\n",
      "epoch: 57, batch: 41, loss: 0.6945688724517822.\n",
      "epoch: 57, batch: 42, loss: 0.5546860098838806.\n",
      "epoch: 57, batch: 43, loss: 0.5571396350860596.\n",
      "epoch: 57, batch: 44, loss: 0.7385929226875305.\n",
      "epoch: 57, batch: 45, loss: 0.5524784326553345.\n",
      "epoch: 57, batch: 46, loss: 0.5526596307754517.\n",
      "epoch: 57, batch: 47, loss: 0.5534529685974121.\n",
      "epoch: 57, batch: 48, loss: 0.8122339248657227.\n",
      "epoch: 57, batch: 49, loss: 0.5594902634620667.\n",
      "epoch: 57, batch: 50, loss: 0.5553992390632629.\n",
      "epoch: 57, batch: 51, loss: 0.5536303520202637.\n",
      "epoch: 57, batch: 52, loss: 0.5534580945968628.\n",
      "epoch: 58, batch: 0, loss: 0.5643850564956665.\n",
      "epoch: 58, batch: 1, loss: 0.5562483072280884.\n",
      "epoch: 58, batch: 2, loss: 0.5540727376937866.\n",
      "epoch: 58, batch: 3, loss: 0.5531125068664551.\n",
      "epoch: 58, batch: 4, loss: 0.5531322956085205.\n",
      "epoch: 58, batch: 5, loss: 0.5555545091629028.\n",
      "epoch: 58, batch: 6, loss: 0.5559910535812378.\n",
      "epoch: 58, batch: 7, loss: 0.5665074586868286.\n",
      "epoch: 58, batch: 8, loss: 0.5546894073486328.\n",
      "epoch: 58, batch: 9, loss: 0.5644440650939941.\n",
      "epoch: 58, batch: 10, loss: 0.5555474758148193.\n",
      "epoch: 58, batch: 11, loss: 0.5563517808914185.\n",
      "epoch: 58, batch: 12, loss: 0.5782985687255859.\n",
      "epoch: 58, batch: 13, loss: 0.5689133405685425.\n",
      "epoch: 58, batch: 14, loss: 0.5528016090393066.\n",
      "epoch: 58, batch: 15, loss: 0.5684950351715088.\n",
      "epoch: 58, batch: 16, loss: 0.554051399230957.\n",
      "epoch: 58, batch: 17, loss: 0.552859902381897.\n",
      "epoch: 58, batch: 18, loss: 0.5774766206741333.\n",
      "epoch: 58, batch: 19, loss: 0.7620616555213928.\n",
      "epoch: 58, batch: 20, loss: 0.563458263874054.\n",
      "epoch: 58, batch: 21, loss: 0.59540855884552.\n",
      "epoch: 58, batch: 22, loss: 0.5587379932403564.\n",
      "epoch: 58, batch: 23, loss: 0.6567269563674927.\n",
      "epoch: 58, batch: 24, loss: 0.5721541047096252.\n",
      "epoch: 58, batch: 25, loss: 0.5731456279754639.\n",
      "epoch: 58, batch: 26, loss: 0.6167539954185486.\n",
      "epoch: 58, batch: 27, loss: 0.6005057096481323.\n",
      "epoch: 58, batch: 28, loss: 0.5588120818138123.\n",
      "epoch: 58, batch: 29, loss: 0.55925452709198.\n",
      "epoch: 58, batch: 30, loss: 0.5537832975387573.\n",
      "epoch: 58, batch: 31, loss: 0.552553653717041.\n",
      "epoch: 58, batch: 32, loss: 0.5565797090530396.\n",
      "epoch: 58, batch: 33, loss: 0.5549697875976562.\n",
      "epoch: 58, batch: 34, loss: 0.5704512596130371.\n",
      "epoch: 58, batch: 35, loss: 0.5833603739738464.\n",
      "epoch: 58, batch: 36, loss: 0.5522453784942627.\n",
      "epoch: 58, batch: 37, loss: 0.5628279447555542.\n",
      "epoch: 58, batch: 38, loss: 0.5525664687156677.\n",
      "epoch: 58, batch: 39, loss: 0.5554785132408142.\n",
      "epoch: 58, batch: 40, loss: 0.8206202983856201.\n",
      "epoch: 58, batch: 41, loss: 0.65166175365448.\n",
      "epoch: 58, batch: 42, loss: 0.5708277225494385.\n",
      "epoch: 58, batch: 43, loss: 0.5537006855010986.\n",
      "epoch: 58, batch: 44, loss: 0.7697542905807495.\n",
      "epoch: 58, batch: 45, loss: 0.6033459305763245.\n",
      "epoch: 58, batch: 46, loss: 0.5538229942321777.\n",
      "epoch: 58, batch: 47, loss: 0.5536967515945435.\n",
      "epoch: 58, batch: 48, loss: 0.6516338586807251.\n",
      "epoch: 58, batch: 49, loss: 0.5530824661254883.\n",
      "epoch: 58, batch: 50, loss: 0.5591091513633728.\n",
      "epoch: 58, batch: 51, loss: 0.9008680582046509.\n",
      "epoch: 58, batch: 52, loss: 0.551678478717804.\n",
      "epoch: 59, batch: 0, loss: 0.5722945332527161.\n",
      "epoch: 59, batch: 1, loss: 0.55225670337677.\n",
      "epoch: 59, batch: 2, loss: 0.5644814968109131.\n",
      "epoch: 59, batch: 3, loss: 0.55828857421875.\n",
      "epoch: 59, batch: 4, loss: 0.5534229278564453.\n",
      "epoch: 59, batch: 5, loss: 0.6138670444488525.\n",
      "epoch: 59, batch: 6, loss: 0.7120945453643799.\n",
      "epoch: 59, batch: 7, loss: 0.5762673616409302.\n",
      "epoch: 59, batch: 8, loss: 0.5688736438751221.\n",
      "epoch: 59, batch: 9, loss: 0.552892804145813.\n",
      "epoch: 59, batch: 10, loss: 0.599385142326355.\n",
      "epoch: 59, batch: 11, loss: 0.5804921388626099.\n",
      "epoch: 59, batch: 12, loss: 0.5589581727981567.\n",
      "epoch: 59, batch: 13, loss: 0.5522856712341309.\n",
      "epoch: 59, batch: 14, loss: 0.5569126605987549.\n",
      "epoch: 59, batch: 15, loss: 0.8011211156845093.\n",
      "epoch: 59, batch: 16, loss: 0.5572888851165771.\n",
      "epoch: 59, batch: 17, loss: 0.5543978214263916.\n",
      "epoch: 59, batch: 18, loss: 0.5838383436203003.\n",
      "epoch: 59, batch: 19, loss: 0.6574809551239014.\n",
      "epoch: 59, batch: 20, loss: 0.5545269250869751.\n",
      "epoch: 59, batch: 21, loss: 0.6098675727844238.\n",
      "epoch: 59, batch: 22, loss: 0.5540680885314941.\n",
      "epoch: 59, batch: 23, loss: 0.5522420406341553.\n",
      "epoch: 59, batch: 24, loss: 0.5827317833900452.\n",
      "epoch: 59, batch: 25, loss: 0.5629490613937378.\n",
      "epoch: 59, batch: 26, loss: 0.5659765005111694.\n",
      "epoch: 59, batch: 27, loss: 0.6055668592453003.\n",
      "epoch: 59, batch: 28, loss: 0.5518800020217896.\n",
      "epoch: 59, batch: 29, loss: 0.5533252358436584.\n",
      "epoch: 59, batch: 30, loss: 0.554364800453186.\n",
      "epoch: 59, batch: 31, loss: 0.5583418607711792.\n",
      "epoch: 59, batch: 32, loss: 0.5536507368087769.\n",
      "epoch: 59, batch: 33, loss: 0.5589427947998047.\n",
      "epoch: 59, batch: 34, loss: 0.5560305714607239.\n",
      "epoch: 59, batch: 35, loss: 0.552016019821167.\n",
      "epoch: 59, batch: 36, loss: 0.5830881595611572.\n",
      "epoch: 59, batch: 37, loss: 0.5554457902908325.\n",
      "epoch: 59, batch: 38, loss: 0.5550103187561035.\n",
      "epoch: 59, batch: 39, loss: 0.5522460341453552.\n",
      "epoch: 59, batch: 40, loss: 0.9190541505813599.\n",
      "epoch: 59, batch: 41, loss: 0.7079410552978516.\n",
      "epoch: 59, batch: 42, loss: 0.5825990438461304.\n",
      "epoch: 59, batch: 43, loss: 0.5536784529685974.\n",
      "epoch: 59, batch: 44, loss: 0.5577353239059448.\n",
      "epoch: 59, batch: 45, loss: 0.553570032119751.\n",
      "epoch: 59, batch: 46, loss: 0.556368350982666.\n",
      "epoch: 59, batch: 47, loss: 0.5557215213775635.\n",
      "epoch: 59, batch: 48, loss: 0.7206124067306519.\n",
      "epoch: 59, batch: 49, loss: 0.8695856332778931.\n",
      "epoch: 59, batch: 50, loss: 0.5537227988243103.\n",
      "epoch: 59, batch: 51, loss: 0.5539750456809998.\n",
      "epoch: 59, batch: 52, loss: 0.5525930523872375.\n",
      "epoch: 60, batch: 0, loss: 0.552001416683197.\n",
      "epoch: 60, batch: 1, loss: 0.5583481788635254.\n",
      "epoch: 60, batch: 2, loss: 0.5531409382820129.\n",
      "epoch: 60, batch: 3, loss: 0.5526979565620422.\n",
      "epoch: 60, batch: 4, loss: 0.5567225813865662.\n",
      "epoch: 60, batch: 5, loss: 0.5561427474021912.\n",
      "epoch: 60, batch: 6, loss: 0.558040976524353.\n",
      "epoch: 60, batch: 7, loss: 0.621322512626648.\n",
      "epoch: 60, batch: 8, loss: 0.5734795331954956.\n",
      "epoch: 60, batch: 9, loss: 0.5518556833267212.\n",
      "epoch: 60, batch: 10, loss: 0.5681296586990356.\n",
      "epoch: 60, batch: 11, loss: 0.5550811290740967.\n",
      "epoch: 60, batch: 12, loss: 0.5539284944534302.\n",
      "epoch: 60, batch: 13, loss: 0.6635632514953613.\n",
      "epoch: 60, batch: 14, loss: 0.5786887407302856.\n",
      "epoch: 60, batch: 15, loss: 0.5607314705848694.\n",
      "epoch: 60, batch: 16, loss: 0.5673868656158447.\n",
      "epoch: 60, batch: 17, loss: 0.5527965426445007.\n",
      "epoch: 60, batch: 18, loss: 0.5525479912757874.\n",
      "epoch: 60, batch: 19, loss: 0.5527294874191284.\n",
      "epoch: 60, batch: 20, loss: 0.559465765953064.\n",
      "epoch: 60, batch: 21, loss: 0.5540637373924255.\n",
      "epoch: 60, batch: 22, loss: 0.5522127151489258.\n",
      "epoch: 60, batch: 23, loss: 0.5538866519927979.\n",
      "epoch: 60, batch: 24, loss: 0.5840745568275452.\n",
      "epoch: 60, batch: 25, loss: 0.5536069869995117.\n",
      "epoch: 60, batch: 26, loss: 0.6766703128814697.\n",
      "epoch: 60, batch: 27, loss: 0.5567614436149597.\n",
      "epoch: 60, batch: 28, loss: 0.5601000189781189.\n",
      "epoch: 60, batch: 29, loss: 0.5565981864929199.\n",
      "epoch: 60, batch: 30, loss: 0.5599994659423828.\n",
      "epoch: 60, batch: 31, loss: 0.5541613101959229.\n",
      "epoch: 60, batch: 32, loss: 0.5577291250228882.\n",
      "epoch: 60, batch: 33, loss: 0.5594078302383423.\n",
      "epoch: 60, batch: 34, loss: 0.5689598321914673.\n",
      "epoch: 60, batch: 35, loss: 0.5562199354171753.\n",
      "epoch: 60, batch: 36, loss: 0.8928370475769043.\n",
      "epoch: 60, batch: 37, loss: 0.5519121289253235.\n",
      "epoch: 60, batch: 38, loss: 0.5570683479309082.\n",
      "epoch: 60, batch: 39, loss: 0.5540163516998291.\n",
      "epoch: 60, batch: 40, loss: 0.5709919929504395.\n",
      "epoch: 60, batch: 41, loss: 0.5570452213287354.\n",
      "epoch: 60, batch: 42, loss: 0.6081584692001343.\n",
      "epoch: 60, batch: 43, loss: 0.5530729293823242.\n",
      "epoch: 60, batch: 44, loss: 0.6686911582946777.\n",
      "epoch: 60, batch: 45, loss: 0.5553479194641113.\n",
      "epoch: 60, batch: 46, loss: 0.5547283887863159.\n",
      "epoch: 60, batch: 47, loss: 0.5531525611877441.\n",
      "epoch: 60, batch: 48, loss: 0.7990540862083435.\n",
      "epoch: 60, batch: 49, loss: 0.567719578742981.\n",
      "epoch: 60, batch: 50, loss: 0.5672684907913208.\n",
      "epoch: 60, batch: 51, loss: 0.9194636344909668.\n",
      "epoch: 60, batch: 52, loss: 0.7938119769096375.\n",
      "epoch: 61, batch: 0, loss: 0.6197791695594788.\n",
      "epoch: 61, batch: 1, loss: 0.5571202635765076.\n",
      "epoch: 61, batch: 2, loss: 0.557138204574585.\n",
      "epoch: 61, batch: 3, loss: 0.5535344481468201.\n",
      "epoch: 61, batch: 4, loss: 0.5537634491920471.\n",
      "epoch: 61, batch: 5, loss: 0.5531963109970093.\n",
      "epoch: 61, batch: 6, loss: 0.5687177181243896.\n",
      "epoch: 61, batch: 7, loss: 0.5521497130393982.\n",
      "epoch: 61, batch: 8, loss: 0.5536470413208008.\n",
      "epoch: 61, batch: 9, loss: 0.5958698987960815.\n",
      "epoch: 61, batch: 10, loss: 0.5533299446105957.\n",
      "epoch: 61, batch: 11, loss: 0.6882354021072388.\n",
      "epoch: 61, batch: 12, loss: 0.5831985473632812.\n",
      "epoch: 61, batch: 13, loss: 0.5533097982406616.\n",
      "epoch: 61, batch: 14, loss: 0.8457474112510681.\n",
      "epoch: 61, batch: 15, loss: 0.5601356029510498.\n",
      "epoch: 61, batch: 16, loss: 0.7438760995864868.\n",
      "epoch: 61, batch: 17, loss: 0.5900519490242004.\n",
      "epoch: 61, batch: 18, loss: 0.5762417316436768.\n",
      "epoch: 61, batch: 19, loss: 0.554450273513794.\n",
      "epoch: 61, batch: 20, loss: 0.5543705821037292.\n",
      "epoch: 61, batch: 21, loss: 0.5552105903625488.\n",
      "epoch: 61, batch: 22, loss: 0.5541214346885681.\n",
      "epoch: 61, batch: 23, loss: 0.5527818202972412.\n",
      "epoch: 61, batch: 24, loss: 0.5715168714523315.\n",
      "epoch: 61, batch: 25, loss: 0.5535173416137695.\n",
      "epoch: 61, batch: 26, loss: 0.552177369594574.\n",
      "epoch: 61, batch: 27, loss: 0.5533167719841003.\n",
      "epoch: 61, batch: 28, loss: 0.5604010820388794.\n",
      "epoch: 61, batch: 29, loss: 0.6902793645858765.\n",
      "epoch: 61, batch: 30, loss: 0.6286119222640991.\n",
      "epoch: 61, batch: 31, loss: 0.5598607659339905.\n",
      "epoch: 61, batch: 32, loss: 0.5531260967254639.\n",
      "epoch: 61, batch: 33, loss: 0.6376795768737793.\n",
      "epoch: 61, batch: 34, loss: 0.5727838277816772.\n",
      "epoch: 61, batch: 35, loss: 0.6243205666542053.\n",
      "epoch: 61, batch: 36, loss: 0.5522019863128662.\n",
      "epoch: 61, batch: 37, loss: 0.7479794025421143.\n",
      "epoch: 61, batch: 38, loss: 0.5591439008712769.\n",
      "epoch: 61, batch: 39, loss: 0.5559027194976807.\n",
      "epoch: 61, batch: 40, loss: 0.6678370833396912.\n",
      "epoch: 61, batch: 41, loss: 0.5543876886367798.\n",
      "epoch: 61, batch: 42, loss: 0.5524454116821289.\n",
      "epoch: 61, batch: 43, loss: 0.5617403984069824.\n",
      "epoch: 61, batch: 44, loss: 0.5721238851547241.\n",
      "epoch: 61, batch: 45, loss: 0.5554897785186768.\n",
      "epoch: 61, batch: 46, loss: 0.5537087321281433.\n",
      "epoch: 61, batch: 47, loss: 0.5567969679832458.\n",
      "epoch: 61, batch: 48, loss: 0.5852658748626709.\n",
      "epoch: 61, batch: 49, loss: 0.6124194860458374.\n",
      "epoch: 61, batch: 50, loss: 0.5630453824996948.\n",
      "epoch: 61, batch: 51, loss: 0.5564349889755249.\n",
      "epoch: 61, batch: 52, loss: 0.5517441630363464.\n",
      "epoch: 62, batch: 0, loss: 0.8240339159965515.\n",
      "epoch: 62, batch: 1, loss: 0.5676875114440918.\n",
      "epoch: 62, batch: 2, loss: 0.5689191818237305.\n",
      "epoch: 62, batch: 3, loss: 0.5519490838050842.\n",
      "epoch: 62, batch: 4, loss: 0.7635315656661987.\n",
      "epoch: 62, batch: 5, loss: 0.5544390678405762.\n",
      "epoch: 62, batch: 6, loss: 0.5736877918243408.\n",
      "epoch: 62, batch: 7, loss: 0.6669073104858398.\n",
      "epoch: 62, batch: 8, loss: 0.5530533194541931.\n",
      "epoch: 62, batch: 9, loss: 0.557719349861145.\n",
      "epoch: 62, batch: 10, loss: 0.5527600049972534.\n",
      "epoch: 62, batch: 11, loss: 0.5651865005493164.\n",
      "epoch: 62, batch: 12, loss: 0.5533245801925659.\n",
      "epoch: 62, batch: 13, loss: 0.6063143014907837.\n",
      "epoch: 62, batch: 14, loss: 0.5551020503044128.\n",
      "epoch: 62, batch: 15, loss: 0.5614054203033447.\n",
      "epoch: 62, batch: 16, loss: 0.5532801747322083.\n",
      "epoch: 62, batch: 17, loss: 0.5559372901916504.\n",
      "epoch: 62, batch: 18, loss: 0.6017436385154724.\n",
      "epoch: 62, batch: 19, loss: 0.5537800192832947.\n",
      "epoch: 62, batch: 20, loss: 0.5533722043037415.\n",
      "epoch: 62, batch: 21, loss: 0.5708390474319458.\n",
      "epoch: 62, batch: 22, loss: 0.5529006123542786.\n",
      "epoch: 62, batch: 23, loss: 0.6260603070259094.\n",
      "epoch: 62, batch: 24, loss: 0.5538620948791504.\n",
      "epoch: 62, batch: 25, loss: 0.5524988174438477.\n",
      "epoch: 62, batch: 26, loss: 0.9533694982528687.\n",
      "epoch: 62, batch: 27, loss: 0.5530977249145508.\n",
      "epoch: 62, batch: 28, loss: 0.5622451305389404.\n",
      "epoch: 62, batch: 29, loss: 0.5564445853233337.\n",
      "epoch: 62, batch: 30, loss: 0.5580884218215942.\n",
      "epoch: 62, batch: 31, loss: 0.5524132251739502.\n",
      "epoch: 62, batch: 32, loss: 0.5650006532669067.\n",
      "epoch: 62, batch: 33, loss: 0.5595349073410034.\n",
      "epoch: 62, batch: 34, loss: 0.5527517795562744.\n",
      "epoch: 62, batch: 35, loss: 0.5533256530761719.\n",
      "epoch: 62, batch: 36, loss: 0.5682754516601562.\n",
      "epoch: 62, batch: 37, loss: 0.5536686778068542.\n",
      "epoch: 62, batch: 38, loss: 0.5580196380615234.\n",
      "epoch: 62, batch: 39, loss: 0.5533641576766968.\n",
      "epoch: 62, batch: 40, loss: 0.5847991108894348.\n",
      "epoch: 62, batch: 41, loss: 0.5686656832695007.\n",
      "epoch: 62, batch: 42, loss: 0.5867387652397156.\n",
      "epoch: 62, batch: 43, loss: 0.7648522853851318.\n",
      "epoch: 62, batch: 44, loss: 0.6650427579879761.\n",
      "epoch: 62, batch: 45, loss: 0.5983972549438477.\n",
      "epoch: 62, batch: 46, loss: 0.5684349536895752.\n",
      "epoch: 62, batch: 47, loss: 0.5530221462249756.\n",
      "epoch: 62, batch: 48, loss: 0.5899804830551147.\n",
      "epoch: 62, batch: 49, loss: 0.5551753044128418.\n",
      "epoch: 62, batch: 50, loss: 0.5537466406822205.\n",
      "epoch: 62, batch: 51, loss: 0.5960622429847717.\n",
      "epoch: 62, batch: 52, loss: 0.5525895357131958.\n",
      "epoch: 63, batch: 0, loss: 0.5517915487289429.\n",
      "epoch: 63, batch: 1, loss: 0.5990390777587891.\n",
      "epoch: 63, batch: 2, loss: 0.5577696561813354.\n",
      "epoch: 63, batch: 3, loss: 0.5528833866119385.\n",
      "epoch: 63, batch: 4, loss: 0.5895723104476929.\n",
      "epoch: 63, batch: 5, loss: 0.5583593845367432.\n",
      "epoch: 63, batch: 6, loss: 0.594441294670105.\n",
      "epoch: 63, batch: 7, loss: 0.6285127401351929.\n",
      "epoch: 63, batch: 8, loss: 0.9036532640457153.\n",
      "epoch: 63, batch: 9, loss: 0.5682392120361328.\n",
      "epoch: 63, batch: 10, loss: 0.5530910491943359.\n",
      "epoch: 63, batch: 11, loss: 0.5534919500350952.\n",
      "epoch: 63, batch: 12, loss: 0.5569159388542175.\n",
      "epoch: 63, batch: 13, loss: 0.5529192686080933.\n",
      "epoch: 63, batch: 14, loss: 0.6007603406906128.\n",
      "epoch: 63, batch: 15, loss: 0.5779210329055786.\n",
      "epoch: 63, batch: 16, loss: 0.5529858469963074.\n",
      "epoch: 63, batch: 17, loss: 0.5562851428985596.\n",
      "epoch: 63, batch: 18, loss: 0.5522434711456299.\n",
      "epoch: 63, batch: 19, loss: 0.560110330581665.\n",
      "epoch: 63, batch: 20, loss: 0.5538029670715332.\n",
      "epoch: 63, batch: 21, loss: 0.5542573928833008.\n",
      "epoch: 63, batch: 22, loss: 0.5524343848228455.\n",
      "epoch: 63, batch: 23, loss: 0.5580994486808777.\n",
      "epoch: 63, batch: 24, loss: 0.5546583533287048.\n",
      "epoch: 63, batch: 25, loss: 0.7600993514060974.\n",
      "epoch: 63, batch: 26, loss: 0.5943937301635742.\n",
      "epoch: 63, batch: 27, loss: 0.5524487495422363.\n",
      "epoch: 63, batch: 28, loss: 0.5832589864730835.\n",
      "epoch: 63, batch: 29, loss: 0.5539836883544922.\n",
      "epoch: 63, batch: 30, loss: 0.5541747212409973.\n",
      "epoch: 63, batch: 31, loss: 0.5534632802009583.\n",
      "epoch: 63, batch: 32, loss: 0.561948299407959.\n",
      "epoch: 63, batch: 33, loss: 0.5547587871551514.\n",
      "epoch: 63, batch: 34, loss: 0.5527572631835938.\n",
      "epoch: 63, batch: 35, loss: 0.5531522631645203.\n",
      "epoch: 63, batch: 36, loss: 0.8578004240989685.\n",
      "epoch: 63, batch: 37, loss: 0.589088499546051.\n",
      "epoch: 63, batch: 38, loss: 0.5555641651153564.\n",
      "epoch: 63, batch: 39, loss: 0.5555158853530884.\n",
      "epoch: 63, batch: 40, loss: 0.5635586977005005.\n",
      "epoch: 63, batch: 41, loss: 0.560050368309021.\n",
      "epoch: 63, batch: 42, loss: 0.5556226372718811.\n",
      "epoch: 63, batch: 43, loss: 0.5569586753845215.\n",
      "epoch: 63, batch: 44, loss: 0.5995278358459473.\n",
      "epoch: 63, batch: 45, loss: 0.6236540079116821.\n",
      "epoch: 63, batch: 46, loss: 0.5647501945495605.\n",
      "epoch: 63, batch: 47, loss: 0.552117109298706.\n",
      "epoch: 63, batch: 48, loss: 0.5608876347541809.\n",
      "epoch: 63, batch: 49, loss: 0.6097456216812134.\n",
      "epoch: 63, batch: 50, loss: 0.7858504056930542.\n",
      "epoch: 63, batch: 51, loss: 0.5592622756958008.\n",
      "epoch: 63, batch: 52, loss: 0.5525385141372681.\n",
      "epoch: 64, batch: 0, loss: 0.6432901620864868.\n",
      "epoch: 64, batch: 1, loss: 0.5573725700378418.\n",
      "epoch: 64, batch: 2, loss: 0.5571537017822266.\n",
      "epoch: 64, batch: 3, loss: 0.5553066730499268.\n",
      "epoch: 64, batch: 4, loss: 0.7167901396751404.\n",
      "epoch: 64, batch: 5, loss: 0.5528329610824585.\n",
      "epoch: 64, batch: 6, loss: 0.5596046447753906.\n",
      "epoch: 64, batch: 7, loss: 0.5804072618484497.\n",
      "epoch: 64, batch: 8, loss: 0.5570600032806396.\n",
      "epoch: 64, batch: 9, loss: 0.5540791749954224.\n",
      "epoch: 64, batch: 10, loss: 0.5529604554176331.\n",
      "epoch: 64, batch: 11, loss: 0.5527810454368591.\n",
      "epoch: 64, batch: 12, loss: 0.5602990388870239.\n",
      "epoch: 64, batch: 13, loss: 0.5535316467285156.\n",
      "epoch: 64, batch: 14, loss: 0.6462761163711548.\n",
      "epoch: 64, batch: 15, loss: 0.5771244168281555.\n",
      "epoch: 64, batch: 16, loss: 0.5526573657989502.\n",
      "epoch: 64, batch: 17, loss: 0.5530301332473755.\n",
      "epoch: 64, batch: 18, loss: 0.5544866323471069.\n",
      "epoch: 64, batch: 19, loss: 0.5536361932754517.\n",
      "epoch: 64, batch: 20, loss: 0.5847069025039673.\n",
      "epoch: 64, batch: 21, loss: 0.8109403848648071.\n",
      "epoch: 64, batch: 22, loss: 0.6291695237159729.\n",
      "epoch: 64, batch: 23, loss: 0.5522520542144775.\n",
      "epoch: 64, batch: 24, loss: 0.8265202045440674.\n",
      "epoch: 64, batch: 25, loss: 0.5776543617248535.\n",
      "epoch: 64, batch: 26, loss: 0.5535287261009216.\n",
      "epoch: 64, batch: 27, loss: 0.5549918413162231.\n",
      "epoch: 64, batch: 28, loss: 0.8128592371940613.\n",
      "epoch: 64, batch: 29, loss: 0.5530321598052979.\n",
      "epoch: 64, batch: 30, loss: 0.5537640452384949.\n",
      "epoch: 64, batch: 31, loss: 0.5719295740127563.\n",
      "epoch: 64, batch: 32, loss: 0.5708882212638855.\n",
      "epoch: 64, batch: 33, loss: 0.5522481203079224.\n",
      "epoch: 64, batch: 34, loss: 0.5560052394866943.\n",
      "epoch: 64, batch: 35, loss: 0.5573552846908569.\n",
      "epoch: 64, batch: 36, loss: 0.5759564638137817.\n",
      "epoch: 64, batch: 37, loss: 0.5551655888557434.\n",
      "epoch: 64, batch: 38, loss: 0.5686591267585754.\n",
      "epoch: 64, batch: 39, loss: 0.5598664879798889.\n",
      "epoch: 64, batch: 40, loss: 0.5520256757736206.\n",
      "epoch: 64, batch: 41, loss: 0.559377133846283.\n",
      "epoch: 64, batch: 42, loss: 0.5523568391799927.\n",
      "epoch: 64, batch: 43, loss: 0.59638512134552.\n",
      "epoch: 64, batch: 44, loss: 0.6348462104797363.\n",
      "epoch: 64, batch: 45, loss: 0.6976093649864197.\n",
      "epoch: 64, batch: 46, loss: 0.5524700880050659.\n",
      "epoch: 64, batch: 47, loss: 0.5653598308563232.\n",
      "epoch: 64, batch: 48, loss: 0.5767123103141785.\n",
      "epoch: 64, batch: 49, loss: 0.5554838180541992.\n",
      "epoch: 64, batch: 50, loss: 0.6737674474716187.\n",
      "epoch: 64, batch: 51, loss: 0.555512547492981.\n",
      "epoch: 64, batch: 52, loss: 0.5522148609161377.\n",
      "epoch: 65, batch: 0, loss: 0.7973888516426086.\n",
      "epoch: 65, batch: 1, loss: 0.6443125009536743.\n",
      "epoch: 65, batch: 2, loss: 0.5529249906539917.\n",
      "epoch: 65, batch: 3, loss: 0.6000466346740723.\n",
      "epoch: 65, batch: 4, loss: 0.7207767367362976.\n",
      "epoch: 65, batch: 5, loss: 0.556179404258728.\n",
      "epoch: 65, batch: 6, loss: 0.5622715353965759.\n",
      "epoch: 65, batch: 7, loss: 0.5659040212631226.\n",
      "epoch: 65, batch: 8, loss: 0.5527145862579346.\n",
      "epoch: 65, batch: 9, loss: 0.5523289442062378.\n",
      "epoch: 65, batch: 10, loss: 0.5527912378311157.\n",
      "epoch: 65, batch: 11, loss: 0.753817081451416.\n",
      "epoch: 65, batch: 12, loss: 0.5553479194641113.\n",
      "epoch: 65, batch: 13, loss: 0.5641469955444336.\n",
      "epoch: 65, batch: 14, loss: 0.5688444375991821.\n",
      "epoch: 65, batch: 15, loss: 0.6549572944641113.\n",
      "epoch: 65, batch: 16, loss: 0.5576193332672119.\n",
      "epoch: 65, batch: 17, loss: 0.5564032793045044.\n",
      "epoch: 65, batch: 18, loss: 0.5607476234436035.\n",
      "epoch: 65, batch: 19, loss: 0.5582407712936401.\n",
      "epoch: 65, batch: 20, loss: 0.557185173034668.\n",
      "epoch: 65, batch: 21, loss: 0.5524927973747253.\n",
      "epoch: 65, batch: 22, loss: 0.5530838370323181.\n",
      "epoch: 65, batch: 23, loss: 0.5643801093101501.\n",
      "epoch: 65, batch: 24, loss: 0.5552194118499756.\n",
      "epoch: 65, batch: 25, loss: 0.7011008262634277.\n",
      "epoch: 65, batch: 26, loss: 0.5532413125038147.\n",
      "epoch: 65, batch: 27, loss: 0.5526001453399658.\n",
      "epoch: 65, batch: 28, loss: 0.5522072911262512.\n",
      "epoch: 65, batch: 29, loss: 0.5728785991668701.\n",
      "epoch: 65, batch: 30, loss: 0.5541360378265381.\n",
      "epoch: 65, batch: 31, loss: 0.5736987590789795.\n",
      "epoch: 65, batch: 32, loss: 0.7872183322906494.\n",
      "epoch: 65, batch: 33, loss: 0.5518960356712341.\n",
      "epoch: 65, batch: 34, loss: 0.5527538061141968.\n",
      "epoch: 65, batch: 35, loss: 0.5524172186851501.\n",
      "epoch: 65, batch: 36, loss: 0.5571255683898926.\n",
      "epoch: 65, batch: 37, loss: 0.5530334711074829.\n",
      "epoch: 65, batch: 38, loss: 0.5888752341270447.\n",
      "epoch: 65, batch: 39, loss: 0.5536669492721558.\n",
      "epoch: 65, batch: 40, loss: 0.5538132190704346.\n",
      "epoch: 65, batch: 41, loss: 0.5565881729125977.\n",
      "epoch: 65, batch: 42, loss: 0.6226022243499756.\n",
      "epoch: 65, batch: 43, loss: 0.5528173446655273.\n",
      "epoch: 65, batch: 44, loss: 0.5786473155021667.\n",
      "epoch: 65, batch: 45, loss: 0.5571919679641724.\n",
      "epoch: 65, batch: 46, loss: 0.5745540857315063.\n",
      "epoch: 65, batch: 47, loss: 0.5528020262718201.\n",
      "epoch: 65, batch: 48, loss: 0.5582106113433838.\n",
      "epoch: 65, batch: 49, loss: 0.6585503816604614.\n",
      "epoch: 65, batch: 50, loss: 0.5524020195007324.\n",
      "epoch: 65, batch: 51, loss: 0.6230615377426147.\n",
      "epoch: 65, batch: 52, loss: 0.5558199882507324.\n",
      "epoch: 66, batch: 0, loss: 0.5571324825286865.\n",
      "epoch: 66, batch: 1, loss: 0.5540180802345276.\n",
      "epoch: 66, batch: 2, loss: 0.5538519620895386.\n",
      "epoch: 66, batch: 3, loss: 0.556699275970459.\n",
      "epoch: 66, batch: 4, loss: 0.5533856749534607.\n",
      "epoch: 66, batch: 5, loss: 0.5533864498138428.\n",
      "epoch: 66, batch: 6, loss: 0.5619127154350281.\n",
      "epoch: 66, batch: 7, loss: 0.8686456084251404.\n",
      "epoch: 66, batch: 8, loss: 0.5579547882080078.\n",
      "epoch: 66, batch: 9, loss: 0.5634629130363464.\n",
      "epoch: 66, batch: 10, loss: 0.6161220073699951.\n",
      "epoch: 66, batch: 11, loss: 0.5542384386062622.\n",
      "epoch: 66, batch: 12, loss: 0.5526269674301147.\n",
      "epoch: 66, batch: 13, loss: 0.5813882350921631.\n",
      "epoch: 66, batch: 14, loss: 0.681846022605896.\n",
      "epoch: 66, batch: 15, loss: 0.5519589185714722.\n",
      "epoch: 66, batch: 16, loss: 0.5540648102760315.\n",
      "epoch: 66, batch: 17, loss: 0.555869460105896.\n",
      "epoch: 66, batch: 18, loss: 0.5517500638961792.\n",
      "epoch: 66, batch: 19, loss: 0.5554735660552979.\n",
      "epoch: 66, batch: 20, loss: 0.555870532989502.\n",
      "epoch: 66, batch: 21, loss: 0.5526529550552368.\n",
      "epoch: 66, batch: 22, loss: 0.5983211994171143.\n",
      "epoch: 66, batch: 23, loss: 0.5541751384735107.\n",
      "epoch: 66, batch: 24, loss: 0.5720565319061279.\n",
      "epoch: 66, batch: 25, loss: 0.55282062292099.\n",
      "epoch: 66, batch: 26, loss: 0.5526063442230225.\n",
      "epoch: 66, batch: 27, loss: 0.5753956437110901.\n",
      "epoch: 66, batch: 28, loss: 0.5578700304031372.\n",
      "epoch: 66, batch: 29, loss: 0.5524604916572571.\n",
      "epoch: 66, batch: 30, loss: 0.5533931255340576.\n",
      "epoch: 66, batch: 31, loss: 0.8148234486579895.\n",
      "epoch: 66, batch: 32, loss: 0.7368230819702148.\n",
      "epoch: 66, batch: 33, loss: 0.5529689788818359.\n",
      "epoch: 66, batch: 34, loss: 0.6077157258987427.\n",
      "epoch: 66, batch: 35, loss: 0.5628479719161987.\n",
      "epoch: 66, batch: 36, loss: 0.5667554140090942.\n",
      "epoch: 66, batch: 37, loss: 0.5564928650856018.\n",
      "epoch: 66, batch: 38, loss: 0.6062251329421997.\n",
      "epoch: 66, batch: 39, loss: 0.5548479557037354.\n",
      "epoch: 66, batch: 40, loss: 0.5544111728668213.\n",
      "epoch: 66, batch: 41, loss: 0.5649080872535706.\n",
      "epoch: 66, batch: 42, loss: 0.6908128261566162.\n",
      "epoch: 66, batch: 43, loss: 0.5529605150222778.\n",
      "epoch: 66, batch: 44, loss: 0.5527263283729553.\n",
      "epoch: 66, batch: 45, loss: 0.5734441876411438.\n",
      "epoch: 66, batch: 46, loss: 0.5624386072158813.\n",
      "epoch: 66, batch: 47, loss: 0.5531296133995056.\n",
      "epoch: 66, batch: 48, loss: 0.5576388835906982.\n",
      "epoch: 66, batch: 49, loss: 0.7746645212173462.\n",
      "epoch: 66, batch: 50, loss: 0.5947668552398682.\n",
      "epoch: 66, batch: 51, loss: 0.5755612254142761.\n",
      "epoch: 66, batch: 52, loss: 0.5525065660476685.\n",
      "epoch: 67, batch: 0, loss: 0.5543123483657837.\n",
      "epoch: 67, batch: 1, loss: 0.5587722063064575.\n",
      "epoch: 67, batch: 2, loss: 0.5542341470718384.\n",
      "epoch: 67, batch: 3, loss: 0.5994043946266174.\n",
      "epoch: 67, batch: 4, loss: 0.7878012657165527.\n",
      "epoch: 67, batch: 5, loss: 0.5528227090835571.\n",
      "epoch: 67, batch: 6, loss: 0.5622347593307495.\n",
      "epoch: 67, batch: 7, loss: 0.5653555393218994.\n",
      "epoch: 67, batch: 8, loss: 0.5522404909133911.\n",
      "epoch: 67, batch: 9, loss: 0.5532961487770081.\n",
      "epoch: 67, batch: 10, loss: 0.5528935194015503.\n",
      "epoch: 67, batch: 11, loss: 0.554894208908081.\n",
      "epoch: 67, batch: 12, loss: 0.5538844466209412.\n",
      "epoch: 67, batch: 13, loss: 0.5597870945930481.\n",
      "epoch: 67, batch: 14, loss: 0.5654289722442627.\n",
      "epoch: 67, batch: 15, loss: 0.5524976253509521.\n",
      "epoch: 67, batch: 16, loss: 0.5536884069442749.\n",
      "epoch: 67, batch: 17, loss: 0.5594518184661865.\n",
      "epoch: 67, batch: 18, loss: 0.568253219127655.\n",
      "epoch: 67, batch: 19, loss: 0.5527445077896118.\n",
      "epoch: 67, batch: 20, loss: 0.5536577701568604.\n",
      "epoch: 67, batch: 21, loss: 0.5522441864013672.\n",
      "epoch: 67, batch: 22, loss: 0.5729830861091614.\n",
      "epoch: 67, batch: 23, loss: 0.7735468745231628.\n",
      "epoch: 67, batch: 24, loss: 0.7658699750900269.\n",
      "epoch: 67, batch: 25, loss: 0.552771270275116.\n",
      "epoch: 67, batch: 26, loss: 0.5530308485031128.\n",
      "epoch: 67, batch: 27, loss: 0.5598567724227905.\n",
      "epoch: 67, batch: 28, loss: 0.6036534905433655.\n",
      "epoch: 67, batch: 29, loss: 0.5524829626083374.\n",
      "epoch: 67, batch: 30, loss: 0.5977074503898621.\n",
      "epoch: 67, batch: 31, loss: 0.5556600093841553.\n",
      "epoch: 67, batch: 32, loss: 0.6352200508117676.\n",
      "epoch: 67, batch: 33, loss: 0.5548931360244751.\n",
      "epoch: 67, batch: 34, loss: 0.6075071096420288.\n",
      "epoch: 67, batch: 35, loss: 0.553444504737854.\n",
      "epoch: 67, batch: 36, loss: 0.5658066272735596.\n",
      "epoch: 67, batch: 37, loss: 0.5543229579925537.\n",
      "epoch: 67, batch: 38, loss: 0.6677602529525757.\n",
      "epoch: 67, batch: 39, loss: 0.5532176494598389.\n",
      "epoch: 67, batch: 40, loss: 0.5521990060806274.\n",
      "epoch: 67, batch: 41, loss: 0.5532678365707397.\n",
      "epoch: 67, batch: 42, loss: 0.5674393773078918.\n",
      "epoch: 67, batch: 43, loss: 0.6079300045967102.\n",
      "epoch: 67, batch: 44, loss: 0.5660698413848877.\n",
      "epoch: 67, batch: 45, loss: 0.555030882358551.\n",
      "epoch: 67, batch: 46, loss: 0.5520941019058228.\n",
      "epoch: 67, batch: 47, loss: 0.5519458055496216.\n",
      "epoch: 67, batch: 48, loss: 0.557268500328064.\n",
      "epoch: 67, batch: 49, loss: 0.6537344455718994.\n",
      "epoch: 67, batch: 50, loss: 0.5548973083496094.\n",
      "epoch: 67, batch: 51, loss: 0.5539010167121887.\n",
      "epoch: 67, batch: 52, loss: 1.0784897804260254.\n",
      "epoch: 68, batch: 0, loss: 0.5540159940719604.\n",
      "epoch: 68, batch: 1, loss: 0.5523015260696411.\n",
      "epoch: 68, batch: 2, loss: 0.5625108480453491.\n",
      "epoch: 68, batch: 3, loss: 0.5575453639030457.\n",
      "epoch: 68, batch: 4, loss: 0.5789375305175781.\n",
      "epoch: 68, batch: 5, loss: 0.5525155067443848.\n",
      "epoch: 68, batch: 6, loss: 0.5560425519943237.\n",
      "epoch: 68, batch: 7, loss: 0.5790742635726929.\n",
      "epoch: 68, batch: 8, loss: 0.5593390464782715.\n",
      "epoch: 68, batch: 9, loss: 0.5546333193778992.\n",
      "epoch: 68, batch: 10, loss: 0.554598331451416.\n",
      "epoch: 68, batch: 11, loss: 0.5537446737289429.\n",
      "epoch: 68, batch: 12, loss: 0.5525989532470703.\n",
      "epoch: 68, batch: 13, loss: 0.5526694655418396.\n",
      "epoch: 68, batch: 14, loss: 0.7054322957992554.\n",
      "epoch: 68, batch: 15, loss: 0.5540191531181335.\n",
      "epoch: 68, batch: 16, loss: 0.6998603343963623.\n",
      "epoch: 68, batch: 17, loss: 0.556098461151123.\n",
      "epoch: 68, batch: 18, loss: 0.5539839267730713.\n",
      "epoch: 68, batch: 19, loss: 0.5570127964019775.\n",
      "epoch: 68, batch: 20, loss: 0.5523409843444824.\n",
      "epoch: 68, batch: 21, loss: 0.5574764013290405.\n",
      "epoch: 68, batch: 22, loss: 0.5526584982872009.\n",
      "epoch: 68, batch: 23, loss: 0.5954543948173523.\n",
      "epoch: 68, batch: 24, loss: 0.5522173643112183.\n",
      "epoch: 68, batch: 25, loss: 0.5612776279449463.\n",
      "epoch: 68, batch: 26, loss: 0.5539189577102661.\n",
      "epoch: 68, batch: 27, loss: 0.6330509185791016.\n",
      "epoch: 68, batch: 28, loss: 0.598651647567749.\n",
      "epoch: 68, batch: 29, loss: 0.5719055533409119.\n",
      "epoch: 68, batch: 30, loss: 0.6528964042663574.\n",
      "epoch: 68, batch: 31, loss: 0.5812215805053711.\n",
      "epoch: 68, batch: 32, loss: 0.5794253349304199.\n",
      "epoch: 68, batch: 33, loss: 0.5521267652511597.\n",
      "epoch: 68, batch: 34, loss: 0.5522921085357666.\n",
      "epoch: 68, batch: 35, loss: 0.5517199039459229.\n",
      "epoch: 68, batch: 36, loss: 0.8346794843673706.\n",
      "epoch: 68, batch: 37, loss: 0.5523682832717896.\n",
      "epoch: 68, batch: 38, loss: 0.5555784106254578.\n",
      "epoch: 68, batch: 39, loss: 0.55445396900177.\n",
      "epoch: 68, batch: 40, loss: 0.5526918172836304.\n",
      "epoch: 68, batch: 41, loss: 0.5523483753204346.\n",
      "epoch: 68, batch: 42, loss: 0.6984634399414062.\n",
      "epoch: 68, batch: 43, loss: 0.5584409236907959.\n",
      "epoch: 68, batch: 44, loss: 0.5694674253463745.\n",
      "epoch: 68, batch: 45, loss: 0.5524822473526001.\n",
      "epoch: 68, batch: 46, loss: 0.8002340793609619.\n",
      "epoch: 68, batch: 47, loss: 0.5550864934921265.\n",
      "epoch: 68, batch: 48, loss: 0.6064047813415527.\n",
      "epoch: 68, batch: 49, loss: 0.5641408562660217.\n",
      "epoch: 68, batch: 50, loss: 0.566719651222229.\n",
      "epoch: 68, batch: 51, loss: 0.6438635587692261.\n",
      "epoch: 68, batch: 52, loss: 0.5686789155006409.\n",
      "epoch: 69, batch: 0, loss: 0.6076920032501221.\n",
      "epoch: 69, batch: 1, loss: 0.55413419008255.\n",
      "epoch: 69, batch: 2, loss: 0.584053635597229.\n",
      "epoch: 69, batch: 3, loss: 0.553679883480072.\n",
      "epoch: 69, batch: 4, loss: 0.5527886152267456.\n",
      "epoch: 69, batch: 5, loss: 0.6133716106414795.\n",
      "epoch: 69, batch: 6, loss: 0.552440881729126.\n",
      "epoch: 69, batch: 7, loss: 0.5545875430107117.\n",
      "epoch: 69, batch: 8, loss: 0.5568715929985046.\n",
      "epoch: 69, batch: 9, loss: 0.5596417188644409.\n",
      "epoch: 69, batch: 10, loss: 0.5543205142021179.\n",
      "epoch: 69, batch: 11, loss: 0.5599823594093323.\n",
      "epoch: 69, batch: 12, loss: 0.5762709975242615.\n",
      "epoch: 69, batch: 13, loss: 0.5625333786010742.\n",
      "epoch: 69, batch: 14, loss: 0.7092287540435791.\n",
      "epoch: 69, batch: 15, loss: 0.5594507455825806.\n",
      "epoch: 69, batch: 16, loss: 0.5616586804389954.\n",
      "epoch: 69, batch: 17, loss: 0.8226243257522583.\n",
      "epoch: 69, batch: 18, loss: 0.5551968216896057.\n",
      "epoch: 69, batch: 19, loss: 0.5522795915603638.\n",
      "epoch: 69, batch: 20, loss: 0.7316979169845581.\n",
      "epoch: 69, batch: 21, loss: 0.5524634718894958.\n",
      "epoch: 69, batch: 22, loss: 0.5615179538726807.\n",
      "epoch: 69, batch: 23, loss: 0.5597593188285828.\n",
      "epoch: 69, batch: 24, loss: 0.5525162220001221.\n",
      "epoch: 69, batch: 25, loss: 0.5566458106040955.\n",
      "epoch: 69, batch: 26, loss: 0.5528385639190674.\n",
      "epoch: 69, batch: 27, loss: 0.567058801651001.\n",
      "epoch: 69, batch: 28, loss: 0.5534313321113586.\n",
      "epoch: 69, batch: 29, loss: 0.5522322058677673.\n",
      "epoch: 69, batch: 30, loss: 0.5865750312805176.\n",
      "epoch: 69, batch: 31, loss: 0.608931303024292.\n",
      "epoch: 69, batch: 32, loss: 0.5523445010185242.\n",
      "epoch: 69, batch: 33, loss: 0.554548978805542.\n",
      "epoch: 69, batch: 34, loss: 0.5523617267608643.\n",
      "epoch: 69, batch: 35, loss: 0.5613740682601929.\n",
      "epoch: 69, batch: 36, loss: 0.5546211004257202.\n",
      "epoch: 69, batch: 37, loss: 0.5534602403640747.\n",
      "epoch: 69, batch: 38, loss: 0.5931726694107056.\n",
      "epoch: 69, batch: 39, loss: 0.5525993704795837.\n",
      "epoch: 69, batch: 40, loss: 0.8283716440200806.\n",
      "epoch: 69, batch: 41, loss: 0.5539939999580383.\n",
      "epoch: 69, batch: 42, loss: 0.6482477188110352.\n",
      "epoch: 69, batch: 43, loss: 0.5653472542762756.\n",
      "epoch: 69, batch: 44, loss: 0.5517904758453369.\n",
      "epoch: 69, batch: 45, loss: 0.5542511940002441.\n",
      "epoch: 69, batch: 46, loss: 0.5687568187713623.\n",
      "epoch: 69, batch: 47, loss: 0.5653859376907349.\n",
      "epoch: 69, batch: 48, loss: 0.7149209976196289.\n",
      "epoch: 69, batch: 49, loss: 0.6208394765853882.\n",
      "epoch: 69, batch: 50, loss: 0.5519576072692871.\n",
      "epoch: 69, batch: 51, loss: 0.5527219772338867.\n",
      "epoch: 69, batch: 52, loss: 0.5517740249633789.\n",
      "epoch: 70, batch: 0, loss: 0.55643630027771.\n",
      "epoch: 70, batch: 1, loss: 0.5697311162948608.\n",
      "epoch: 70, batch: 2, loss: 0.5688496828079224.\n",
      "epoch: 70, batch: 3, loss: 0.5690801739692688.\n",
      "epoch: 70, batch: 4, loss: 0.5550746321678162.\n",
      "epoch: 70, batch: 5, loss: 0.5520823001861572.\n",
      "epoch: 70, batch: 6, loss: 0.5560250282287598.\n",
      "epoch: 70, batch: 7, loss: 0.5549363493919373.\n",
      "epoch: 70, batch: 8, loss: 0.5530498027801514.\n",
      "epoch: 70, batch: 9, loss: 0.5524994134902954.\n",
      "epoch: 70, batch: 10, loss: 0.6407966613769531.\n",
      "epoch: 70, batch: 11, loss: 0.7259277105331421.\n",
      "epoch: 70, batch: 12, loss: 0.651130199432373.\n",
      "epoch: 70, batch: 13, loss: 0.5518434047698975.\n",
      "epoch: 70, batch: 14, loss: 0.5560928583145142.\n",
      "epoch: 70, batch: 15, loss: 0.5537802577018738.\n",
      "epoch: 70, batch: 16, loss: 0.5522263050079346.\n",
      "epoch: 70, batch: 17, loss: 0.5567320585250854.\n",
      "epoch: 70, batch: 18, loss: 0.5676448345184326.\n",
      "epoch: 70, batch: 19, loss: 0.5520637035369873.\n",
      "epoch: 70, batch: 20, loss: 0.5527063608169556.\n",
      "epoch: 70, batch: 21, loss: 0.5523903369903564.\n",
      "epoch: 70, batch: 22, loss: 0.5529744625091553.\n",
      "epoch: 70, batch: 23, loss: 0.5527371764183044.\n",
      "epoch: 70, batch: 24, loss: 0.5854732990264893.\n",
      "epoch: 70, batch: 25, loss: 0.5712270736694336.\n",
      "epoch: 70, batch: 26, loss: 0.5524625778198242.\n",
      "epoch: 70, batch: 27, loss: 0.5534645915031433.\n",
      "epoch: 70, batch: 28, loss: 0.5575234889984131.\n",
      "epoch: 70, batch: 29, loss: 0.5794655084609985.\n",
      "epoch: 70, batch: 30, loss: 0.5535475015640259.\n",
      "epoch: 70, batch: 31, loss: 0.5724066495895386.\n",
      "epoch: 70, batch: 32, loss: 0.562260627746582.\n",
      "epoch: 70, batch: 33, loss: 0.56028151512146.\n",
      "epoch: 70, batch: 34, loss: 0.6930455565452576.\n",
      "epoch: 70, batch: 35, loss: 0.8659029603004456.\n",
      "epoch: 70, batch: 36, loss: 0.7019214630126953.\n",
      "epoch: 70, batch: 37, loss: 0.5546501874923706.\n",
      "epoch: 70, batch: 38, loss: 0.6559233665466309.\n",
      "epoch: 70, batch: 39, loss: 0.5608215928077698.\n",
      "epoch: 70, batch: 40, loss: 0.553898811340332.\n",
      "epoch: 70, batch: 41, loss: 0.5572701692581177.\n",
      "epoch: 70, batch: 42, loss: 0.839439868927002.\n",
      "epoch: 70, batch: 43, loss: 0.5520575046539307.\n",
      "epoch: 70, batch: 44, loss: 0.5551968812942505.\n",
      "epoch: 70, batch: 45, loss: 0.5671031475067139.\n",
      "epoch: 70, batch: 46, loss: 0.5525743961334229.\n",
      "epoch: 70, batch: 47, loss: 0.554595410823822.\n",
      "epoch: 70, batch: 48, loss: 0.5535483956336975.\n",
      "epoch: 70, batch: 49, loss: 0.5542740821838379.\n",
      "epoch: 70, batch: 50, loss: 0.5597667694091797.\n",
      "epoch: 70, batch: 51, loss: 0.5517662763595581.\n",
      "epoch: 70, batch: 52, loss: 0.5522199273109436.\n",
      "epoch: 71, batch: 0, loss: 0.5598573684692383.\n",
      "epoch: 71, batch: 1, loss: 0.5752354860305786.\n",
      "epoch: 71, batch: 2, loss: 0.584952712059021.\n",
      "epoch: 71, batch: 3, loss: 0.7229577302932739.\n",
      "epoch: 71, batch: 4, loss: 0.5583587884902954.\n",
      "epoch: 71, batch: 5, loss: 0.789836049079895.\n",
      "epoch: 71, batch: 6, loss: 0.70310378074646.\n",
      "epoch: 71, batch: 7, loss: 0.5528017282485962.\n",
      "epoch: 71, batch: 8, loss: 0.5533745884895325.\n",
      "epoch: 71, batch: 9, loss: 0.5534399747848511.\n",
      "epoch: 71, batch: 10, loss: 0.5550021529197693.\n",
      "epoch: 71, batch: 11, loss: 0.5557214021682739.\n",
      "epoch: 71, batch: 12, loss: 0.5519915819168091.\n",
      "epoch: 71, batch: 13, loss: 0.5741703510284424.\n",
      "epoch: 71, batch: 14, loss: 0.5612579584121704.\n",
      "epoch: 71, batch: 15, loss: 0.5581866502761841.\n",
      "epoch: 71, batch: 16, loss: 0.556620717048645.\n",
      "epoch: 71, batch: 17, loss: 0.5530339479446411.\n",
      "epoch: 71, batch: 18, loss: 0.5561037063598633.\n",
      "epoch: 71, batch: 19, loss: 0.6183798313140869.\n",
      "epoch: 71, batch: 20, loss: 0.555698812007904.\n",
      "epoch: 71, batch: 21, loss: 0.5567406415939331.\n",
      "epoch: 71, batch: 22, loss: 0.6555070877075195.\n",
      "epoch: 71, batch: 23, loss: 0.5537775754928589.\n",
      "epoch: 71, batch: 24, loss: 0.5757540464401245.\n",
      "epoch: 71, batch: 25, loss: 0.552506148815155.\n",
      "epoch: 71, batch: 26, loss: 0.5837105512619019.\n",
      "epoch: 71, batch: 27, loss: 0.5608989000320435.\n",
      "epoch: 71, batch: 28, loss: 0.7683588266372681.\n",
      "epoch: 71, batch: 29, loss: 0.553390383720398.\n",
      "epoch: 71, batch: 30, loss: 0.5541561245918274.\n",
      "epoch: 71, batch: 31, loss: 0.5804169178009033.\n",
      "epoch: 71, batch: 32, loss: 0.5520919561386108.\n",
      "epoch: 71, batch: 33, loss: 0.5517454743385315.\n",
      "epoch: 71, batch: 34, loss: 0.5829952955245972.\n",
      "epoch: 71, batch: 35, loss: 0.5554218292236328.\n",
      "epoch: 71, batch: 36, loss: 0.5532916188240051.\n",
      "epoch: 71, batch: 37, loss: 0.5529458522796631.\n",
      "epoch: 71, batch: 38, loss: 0.5523742437362671.\n",
      "epoch: 71, batch: 39, loss: 0.5517765283584595.\n",
      "epoch: 71, batch: 40, loss: 0.8846534490585327.\n",
      "epoch: 71, batch: 41, loss: 0.5693477392196655.\n",
      "epoch: 71, batch: 42, loss: 0.551906943321228.\n",
      "epoch: 71, batch: 43, loss: 0.5593507885932922.\n",
      "epoch: 71, batch: 44, loss: 0.5564600825309753.\n",
      "epoch: 71, batch: 45, loss: 0.5578402280807495.\n",
      "epoch: 71, batch: 46, loss: 0.5521994829177856.\n",
      "epoch: 71, batch: 47, loss: 0.6114727258682251.\n",
      "epoch: 71, batch: 48, loss: 0.5528469681739807.\n",
      "epoch: 71, batch: 49, loss: 0.5525692701339722.\n",
      "epoch: 71, batch: 50, loss: 0.5532331466674805.\n",
      "epoch: 71, batch: 51, loss: 0.5732970237731934.\n",
      "epoch: 71, batch: 52, loss: 0.6404872536659241.\n",
      "epoch: 72, batch: 0, loss: 0.5525595545768738.\n",
      "epoch: 72, batch: 1, loss: 0.5519171953201294.\n",
      "epoch: 72, batch: 2, loss: 0.841328501701355.\n",
      "epoch: 72, batch: 3, loss: 0.5548505783081055.\n",
      "epoch: 72, batch: 4, loss: 0.5579360723495483.\n",
      "epoch: 72, batch: 5, loss: 0.5642139911651611.\n",
      "epoch: 72, batch: 6, loss: 0.5518895983695984.\n",
      "epoch: 72, batch: 7, loss: 0.5529628992080688.\n",
      "epoch: 72, batch: 8, loss: 0.5643928647041321.\n",
      "epoch: 72, batch: 9, loss: 0.5914293527603149.\n",
      "epoch: 72, batch: 10, loss: 0.553475558757782.\n",
      "epoch: 72, batch: 11, loss: 0.5521112084388733.\n",
      "epoch: 72, batch: 12, loss: 0.5533980131149292.\n",
      "epoch: 72, batch: 13, loss: 0.6096979379653931.\n",
      "epoch: 72, batch: 14, loss: 0.5525070428848267.\n",
      "epoch: 72, batch: 15, loss: 0.5554380416870117.\n",
      "epoch: 72, batch: 16, loss: 0.5574225783348083.\n",
      "epoch: 72, batch: 17, loss: 0.5524057149887085.\n",
      "epoch: 72, batch: 18, loss: 0.5527194738388062.\n",
      "epoch: 72, batch: 19, loss: 0.5539926886558533.\n",
      "epoch: 72, batch: 20, loss: 0.7776134610176086.\n",
      "epoch: 72, batch: 21, loss: 0.552749514579773.\n",
      "epoch: 72, batch: 22, loss: 0.5521552562713623.\n",
      "epoch: 72, batch: 23, loss: 0.5528926849365234.\n",
      "epoch: 72, batch: 24, loss: 0.5553157925605774.\n",
      "epoch: 72, batch: 25, loss: 0.5850309729576111.\n",
      "epoch: 72, batch: 26, loss: 0.5584079027175903.\n",
      "epoch: 72, batch: 27, loss: 0.5524284839630127.\n",
      "epoch: 72, batch: 28, loss: 0.6901639699935913.\n",
      "epoch: 72, batch: 29, loss: 0.6190983057022095.\n",
      "epoch: 72, batch: 30, loss: 0.6205426454544067.\n",
      "epoch: 72, batch: 31, loss: 0.6672357320785522.\n",
      "epoch: 72, batch: 32, loss: 0.559569239616394.\n",
      "epoch: 72, batch: 33, loss: 0.5522875785827637.\n",
      "epoch: 72, batch: 34, loss: 0.5619557499885559.\n",
      "epoch: 72, batch: 35, loss: 0.6942747831344604.\n",
      "epoch: 72, batch: 36, loss: 0.5669803023338318.\n",
      "epoch: 72, batch: 37, loss: 0.554286777973175.\n",
      "epoch: 72, batch: 38, loss: 0.5550321340560913.\n",
      "epoch: 72, batch: 39, loss: 0.5524325966835022.\n",
      "epoch: 72, batch: 40, loss: 0.5700945854187012.\n",
      "epoch: 72, batch: 41, loss: 0.5522642135620117.\n",
      "epoch: 72, batch: 42, loss: 0.5521147847175598.\n",
      "epoch: 72, batch: 43, loss: 0.5529458522796631.\n",
      "epoch: 72, batch: 44, loss: 0.5522171854972839.\n",
      "epoch: 72, batch: 45, loss: 0.5543265342712402.\n",
      "epoch: 72, batch: 46, loss: 0.5707157850265503.\n",
      "epoch: 72, batch: 47, loss: 0.5774388909339905.\n",
      "epoch: 72, batch: 48, loss: 0.5564202070236206.\n",
      "epoch: 72, batch: 49, loss: 0.5624180436134338.\n",
      "epoch: 72, batch: 50, loss: 0.842170238494873.\n",
      "epoch: 72, batch: 51, loss: 0.5704140067100525.\n",
      "epoch: 72, batch: 52, loss: 0.5552302002906799.\n",
      "epoch: 73, batch: 0, loss: 0.5520373582839966.\n",
      "epoch: 73, batch: 1, loss: 0.8597055077552795.\n",
      "epoch: 73, batch: 2, loss: 0.5558710098266602.\n",
      "epoch: 73, batch: 3, loss: 0.8128799796104431.\n",
      "epoch: 73, batch: 4, loss: 0.5718898177146912.\n",
      "epoch: 73, batch: 5, loss: 0.6186352968215942.\n",
      "epoch: 73, batch: 6, loss: 0.5664244890213013.\n",
      "epoch: 73, batch: 7, loss: 0.5635089874267578.\n",
      "epoch: 73, batch: 8, loss: 0.5556018352508545.\n",
      "epoch: 73, batch: 9, loss: 0.5587282180786133.\n",
      "epoch: 73, batch: 10, loss: 0.5533689260482788.\n",
      "epoch: 73, batch: 11, loss: 0.5666025876998901.\n",
      "epoch: 73, batch: 12, loss: 0.5525723695755005.\n",
      "epoch: 73, batch: 13, loss: 0.7801834344863892.\n",
      "epoch: 73, batch: 14, loss: 0.554512619972229.\n",
      "epoch: 73, batch: 15, loss: 0.5523574352264404.\n",
      "epoch: 73, batch: 16, loss: 0.5663868188858032.\n",
      "epoch: 73, batch: 17, loss: 0.5522906184196472.\n",
      "epoch: 73, batch: 18, loss: 0.5573084354400635.\n",
      "epoch: 73, batch: 19, loss: 0.6231924295425415.\n",
      "epoch: 73, batch: 20, loss: 0.5520844459533691.\n",
      "epoch: 73, batch: 21, loss: 0.7574208974838257.\n",
      "epoch: 73, batch: 22, loss: 0.5525007247924805.\n",
      "epoch: 73, batch: 23, loss: 0.5565773248672485.\n",
      "epoch: 73, batch: 24, loss: 0.5555305480957031.\n",
      "epoch: 73, batch: 25, loss: 0.5521482229232788.\n",
      "epoch: 73, batch: 26, loss: 0.6790986061096191.\n",
      "epoch: 73, batch: 27, loss: 0.5612266659736633.\n",
      "epoch: 73, batch: 28, loss: 0.5521775484085083.\n",
      "epoch: 73, batch: 29, loss: 0.5720411539077759.\n",
      "epoch: 73, batch: 30, loss: 0.5966498255729675.\n",
      "epoch: 73, batch: 31, loss: 0.5540323853492737.\n",
      "epoch: 73, batch: 32, loss: 0.5958371162414551.\n",
      "epoch: 73, batch: 33, loss: 0.5519858002662659.\n",
      "epoch: 73, batch: 34, loss: 0.5571540594100952.\n",
      "epoch: 73, batch: 35, loss: 0.5556560754776001.\n",
      "epoch: 73, batch: 36, loss: 0.6236076354980469.\n",
      "epoch: 73, batch: 37, loss: 0.5535064935684204.\n",
      "epoch: 73, batch: 38, loss: 0.6006794571876526.\n",
      "epoch: 73, batch: 39, loss: 0.552115261554718.\n",
      "epoch: 73, batch: 40, loss: 0.5533057451248169.\n",
      "epoch: 73, batch: 41, loss: 0.5541096925735474.\n",
      "epoch: 73, batch: 42, loss: 0.5523110628128052.\n",
      "epoch: 73, batch: 43, loss: 0.5543073415756226.\n",
      "epoch: 73, batch: 44, loss: 0.5635000467300415.\n",
      "epoch: 73, batch: 45, loss: 0.5542240738868713.\n",
      "epoch: 73, batch: 46, loss: 0.5521082878112793.\n",
      "epoch: 73, batch: 47, loss: 0.5524276494979858.\n",
      "epoch: 73, batch: 48, loss: 0.5577115416526794.\n",
      "epoch: 73, batch: 49, loss: 0.5584940314292908.\n",
      "epoch: 73, batch: 50, loss: 0.5530897378921509.\n",
      "epoch: 73, batch: 51, loss: 0.5542793869972229.\n",
      "epoch: 73, batch: 52, loss: 0.5518772602081299.\n",
      "epoch: 74, batch: 0, loss: 0.5532733201980591.\n",
      "epoch: 74, batch: 1, loss: 0.553162157535553.\n",
      "epoch: 74, batch: 2, loss: 0.5520778298377991.\n",
      "epoch: 74, batch: 3, loss: 0.5600151419639587.\n",
      "epoch: 74, batch: 4, loss: 0.552011251449585.\n",
      "epoch: 74, batch: 5, loss: 0.8686702251434326.\n",
      "epoch: 74, batch: 6, loss: 0.5598434209823608.\n",
      "epoch: 74, batch: 7, loss: 0.5600099563598633.\n",
      "epoch: 74, batch: 8, loss: 0.5555354356765747.\n",
      "epoch: 74, batch: 9, loss: 0.5545006394386292.\n",
      "epoch: 74, batch: 10, loss: 0.5692740678787231.\n",
      "epoch: 74, batch: 11, loss: 0.5522967576980591.\n",
      "epoch: 74, batch: 12, loss: 0.5520782470703125.\n",
      "epoch: 74, batch: 13, loss: 0.5548924207687378.\n",
      "epoch: 74, batch: 14, loss: 0.5519928336143494.\n",
      "epoch: 74, batch: 15, loss: 0.5644328594207764.\n",
      "epoch: 74, batch: 16, loss: 0.5660746097564697.\n",
      "epoch: 74, batch: 17, loss: 0.5563897490501404.\n",
      "epoch: 74, batch: 18, loss: 0.5897713899612427.\n",
      "epoch: 74, batch: 19, loss: 0.6370614767074585.\n",
      "epoch: 74, batch: 20, loss: 0.6484813690185547.\n",
      "epoch: 74, batch: 21, loss: 0.5521423816680908.\n",
      "epoch: 74, batch: 22, loss: 0.5581874847412109.\n",
      "epoch: 74, batch: 23, loss: 0.5538942813873291.\n",
      "epoch: 74, batch: 24, loss: 0.5553300380706787.\n",
      "epoch: 74, batch: 25, loss: 0.5530531406402588.\n",
      "epoch: 74, batch: 26, loss: 0.5539649724960327.\n",
      "epoch: 74, batch: 27, loss: 0.5612655878067017.\n",
      "epoch: 74, batch: 28, loss: 0.6667739748954773.\n",
      "epoch: 74, batch: 29, loss: 0.5559291839599609.\n",
      "epoch: 74, batch: 30, loss: 0.7458146810531616.\n",
      "epoch: 74, batch: 31, loss: 0.5527238249778748.\n",
      "epoch: 74, batch: 32, loss: 0.552204430103302.\n",
      "epoch: 74, batch: 33, loss: 0.6006336212158203.\n",
      "epoch: 74, batch: 34, loss: 0.5534746050834656.\n",
      "epoch: 74, batch: 35, loss: 0.5879479050636292.\n",
      "epoch: 74, batch: 36, loss: 0.5529435873031616.\n",
      "epoch: 74, batch: 37, loss: 0.5614007711410522.\n",
      "epoch: 74, batch: 38, loss: 0.5655429363250732.\n",
      "epoch: 74, batch: 39, loss: 0.7942290902137756.\n",
      "epoch: 74, batch: 40, loss: 0.55366051197052.\n",
      "epoch: 74, batch: 41, loss: 0.5547637343406677.\n",
      "epoch: 74, batch: 42, loss: 0.5576267242431641.\n",
      "epoch: 74, batch: 43, loss: 0.5535752773284912.\n",
      "epoch: 74, batch: 44, loss: 0.5919158458709717.\n",
      "epoch: 74, batch: 45, loss: 0.5531908869743347.\n",
      "epoch: 74, batch: 46, loss: 0.5520172119140625.\n",
      "epoch: 74, batch: 47, loss: 0.56231689453125.\n",
      "epoch: 74, batch: 48, loss: 0.5520175695419312.\n",
      "epoch: 74, batch: 49, loss: 0.5570427775382996.\n",
      "epoch: 74, batch: 50, loss: 0.5917958617210388.\n",
      "epoch: 74, batch: 51, loss: 0.5527023077011108.\n",
      "epoch: 74, batch: 52, loss: 0.9896520376205444.\n",
      "epoch: 75, batch: 0, loss: 0.5522795915603638.\n",
      "epoch: 75, batch: 1, loss: 0.5539685487747192.\n",
      "epoch: 75, batch: 2, loss: 0.5521701574325562.\n",
      "epoch: 75, batch: 3, loss: 0.6015142202377319.\n",
      "epoch: 75, batch: 4, loss: 0.8507992625236511.\n",
      "epoch: 75, batch: 5, loss: 0.5519356727600098.\n",
      "epoch: 75, batch: 6, loss: 0.5525749921798706.\n",
      "epoch: 75, batch: 7, loss: 0.5574491024017334.\n",
      "epoch: 75, batch: 8, loss: 0.5761983394622803.\n",
      "epoch: 75, batch: 9, loss: 0.5663954615592957.\n",
      "epoch: 75, batch: 10, loss: 0.5636545419692993.\n",
      "epoch: 75, batch: 11, loss: 0.5521979928016663.\n",
      "epoch: 75, batch: 12, loss: 0.5521788597106934.\n",
      "epoch: 75, batch: 13, loss: 0.5552371144294739.\n",
      "epoch: 75, batch: 14, loss: 0.5524847507476807.\n",
      "epoch: 75, batch: 15, loss: 0.6068559885025024.\n",
      "epoch: 75, batch: 16, loss: 0.5607571601867676.\n",
      "epoch: 75, batch: 17, loss: 0.5554606914520264.\n",
      "epoch: 75, batch: 18, loss: 0.6296790838241577.\n",
      "epoch: 75, batch: 19, loss: 0.5527922511100769.\n",
      "epoch: 75, batch: 20, loss: 0.5700091123580933.\n",
      "epoch: 75, batch: 21, loss: 0.5861282348632812.\n",
      "epoch: 75, batch: 22, loss: 0.5522898435592651.\n",
      "epoch: 75, batch: 23, loss: 0.5536742210388184.\n",
      "epoch: 75, batch: 24, loss: 0.7634495496749878.\n",
      "epoch: 75, batch: 25, loss: 0.5534467697143555.\n",
      "epoch: 75, batch: 26, loss: 0.5521631240844727.\n",
      "epoch: 75, batch: 27, loss: 0.5522031784057617.\n",
      "epoch: 75, batch: 28, loss: 0.5520021915435791.\n",
      "epoch: 75, batch: 29, loss: 0.5718587636947632.\n",
      "epoch: 75, batch: 30, loss: 0.5522604584693909.\n",
      "epoch: 75, batch: 31, loss: 0.8997068405151367.\n",
      "epoch: 75, batch: 32, loss: 0.5586774349212646.\n",
      "epoch: 75, batch: 33, loss: 0.5520057082176208.\n",
      "epoch: 75, batch: 34, loss: 0.5600144267082214.\n",
      "epoch: 75, batch: 35, loss: 0.5703011751174927.\n",
      "epoch: 75, batch: 36, loss: 0.727929949760437.\n",
      "epoch: 75, batch: 37, loss: 0.5551986694335938.\n",
      "epoch: 75, batch: 38, loss: 0.5776538848876953.\n",
      "epoch: 75, batch: 39, loss: 0.55208820104599.\n",
      "epoch: 75, batch: 40, loss: 0.6117795705795288.\n",
      "epoch: 75, batch: 41, loss: 0.5546920299530029.\n",
      "epoch: 75, batch: 42, loss: 0.5555953979492188.\n",
      "epoch: 75, batch: 43, loss: 0.5562899112701416.\n",
      "epoch: 75, batch: 44, loss: 0.7108099460601807.\n",
      "epoch: 75, batch: 45, loss: 0.553098201751709.\n",
      "epoch: 75, batch: 46, loss: 0.5550041198730469.\n",
      "epoch: 75, batch: 47, loss: 0.5533052086830139.\n",
      "epoch: 75, batch: 48, loss: 0.551699161529541.\n",
      "epoch: 75, batch: 49, loss: 0.565436601638794.\n",
      "epoch: 75, batch: 50, loss: 0.5525919198989868.\n",
      "epoch: 75, batch: 51, loss: 0.5565249919891357.\n",
      "epoch: 75, batch: 52, loss: 0.5521814227104187.\n",
      "epoch: 76, batch: 0, loss: 0.5556825399398804.\n",
      "epoch: 76, batch: 1, loss: 0.5607913136482239.\n",
      "epoch: 76, batch: 2, loss: 0.5537358522415161.\n",
      "epoch: 76, batch: 3, loss: 0.5529365539550781.\n",
      "epoch: 76, batch: 4, loss: 0.5563366413116455.\n",
      "epoch: 76, batch: 5, loss: 0.5522936582565308.\n",
      "epoch: 76, batch: 6, loss: 0.5639546513557434.\n",
      "epoch: 76, batch: 7, loss: 0.5530158281326294.\n",
      "epoch: 76, batch: 8, loss: 0.5636038780212402.\n",
      "epoch: 76, batch: 9, loss: 0.5521858930587769.\n",
      "epoch: 76, batch: 10, loss: 0.5539395809173584.\n",
      "epoch: 76, batch: 11, loss: 0.5520714521408081.\n",
      "epoch: 76, batch: 12, loss: 0.5539798736572266.\n",
      "epoch: 76, batch: 13, loss: 0.5884852409362793.\n",
      "epoch: 76, batch: 14, loss: 0.5830305814743042.\n",
      "epoch: 76, batch: 15, loss: 0.5570195913314819.\n",
      "epoch: 76, batch: 16, loss: 0.5523165464401245.\n",
      "epoch: 76, batch: 17, loss: 0.5527924299240112.\n",
      "epoch: 76, batch: 18, loss: 0.5535895228385925.\n",
      "epoch: 76, batch: 19, loss: 0.5595904588699341.\n",
      "epoch: 76, batch: 20, loss: 0.5598865151405334.\n",
      "epoch: 76, batch: 21, loss: 0.55998694896698.\n",
      "epoch: 76, batch: 22, loss: 0.5541229248046875.\n",
      "epoch: 76, batch: 23, loss: 0.6790008544921875.\n",
      "epoch: 76, batch: 24, loss: 0.5543786287307739.\n",
      "epoch: 76, batch: 25, loss: 0.9127272963523865.\n",
      "epoch: 76, batch: 26, loss: 0.6231268644332886.\n",
      "epoch: 76, batch: 27, loss: 0.5537283420562744.\n",
      "epoch: 76, batch: 28, loss: 0.5528632998466492.\n",
      "epoch: 76, batch: 29, loss: 0.673434853553772.\n",
      "epoch: 76, batch: 30, loss: 0.5521231293678284.\n",
      "epoch: 76, batch: 31, loss: 0.5599017143249512.\n",
      "epoch: 76, batch: 32, loss: 0.5520824193954468.\n",
      "epoch: 76, batch: 33, loss: 0.6913560628890991.\n",
      "epoch: 76, batch: 34, loss: 0.7362487316131592.\n",
      "epoch: 76, batch: 35, loss: 0.55270916223526.\n",
      "epoch: 76, batch: 36, loss: 0.5526629090309143.\n",
      "epoch: 76, batch: 37, loss: 0.5522210001945496.\n",
      "epoch: 76, batch: 38, loss: 0.5542440414428711.\n",
      "epoch: 76, batch: 39, loss: 0.554203987121582.\n",
      "epoch: 76, batch: 40, loss: 0.5519336462020874.\n",
      "epoch: 76, batch: 41, loss: 0.5553361177444458.\n",
      "epoch: 76, batch: 42, loss: 0.7120448350906372.\n",
      "epoch: 76, batch: 43, loss: 0.5671789646148682.\n",
      "epoch: 76, batch: 44, loss: 0.560878574848175.\n",
      "epoch: 76, batch: 45, loss: 0.6655099391937256.\n",
      "epoch: 76, batch: 46, loss: 0.577622652053833.\n",
      "epoch: 76, batch: 47, loss: 0.6950284242630005.\n",
      "epoch: 76, batch: 48, loss: 0.5519875288009644.\n",
      "epoch: 76, batch: 49, loss: 0.5554060935974121.\n",
      "epoch: 76, batch: 50, loss: 0.5517807006835938.\n",
      "epoch: 76, batch: 51, loss: 0.5535985231399536.\n",
      "epoch: 76, batch: 52, loss: 0.5602551102638245.\n",
      "epoch: 77, batch: 0, loss: 0.555542528629303.\n",
      "epoch: 77, batch: 1, loss: 0.5553560853004456.\n",
      "epoch: 77, batch: 2, loss: 0.7660620212554932.\n",
      "epoch: 77, batch: 3, loss: 0.5520462989807129.\n",
      "epoch: 77, batch: 4, loss: 0.5523214340209961.\n",
      "epoch: 77, batch: 5, loss: 0.552965521812439.\n",
      "epoch: 77, batch: 6, loss: 0.553049623966217.\n",
      "epoch: 77, batch: 7, loss: 0.5999020338058472.\n",
      "epoch: 77, batch: 8, loss: 0.552978515625.\n",
      "epoch: 77, batch: 9, loss: 0.5529732704162598.\n",
      "epoch: 77, batch: 10, loss: 0.5521456003189087.\n",
      "epoch: 77, batch: 11, loss: 0.5593938231468201.\n",
      "epoch: 77, batch: 12, loss: 0.5521067380905151.\n",
      "epoch: 77, batch: 13, loss: 0.5919613838195801.\n",
      "epoch: 77, batch: 14, loss: 0.573344349861145.\n",
      "epoch: 77, batch: 15, loss: 0.5553256273269653.\n",
      "epoch: 77, batch: 16, loss: 0.5528367757797241.\n",
      "epoch: 77, batch: 17, loss: 0.5938812494277954.\n",
      "epoch: 77, batch: 18, loss: 0.6448227763175964.\n",
      "epoch: 77, batch: 19, loss: 0.5522714853286743.\n",
      "epoch: 77, batch: 20, loss: 0.5535417795181274.\n",
      "epoch: 77, batch: 21, loss: 0.5582296848297119.\n",
      "epoch: 77, batch: 22, loss: 0.5522348880767822.\n",
      "epoch: 77, batch: 23, loss: 0.5756145119667053.\n",
      "epoch: 77, batch: 24, loss: 0.5522282123565674.\n",
      "epoch: 77, batch: 25, loss: 0.5521985292434692.\n",
      "epoch: 77, batch: 26, loss: 0.558718740940094.\n",
      "epoch: 77, batch: 27, loss: 0.5524067878723145.\n",
      "epoch: 77, batch: 28, loss: 0.5537594556808472.\n",
      "epoch: 77, batch: 29, loss: 0.611492395401001.\n",
      "epoch: 77, batch: 30, loss: 0.7610026001930237.\n",
      "epoch: 77, batch: 31, loss: 0.5556676387786865.\n",
      "epoch: 77, batch: 32, loss: 0.5520107746124268.\n",
      "epoch: 77, batch: 33, loss: 0.5604335069656372.\n",
      "epoch: 77, batch: 34, loss: 0.5519495010375977.\n",
      "epoch: 77, batch: 35, loss: 0.5519680976867676.\n",
      "epoch: 77, batch: 36, loss: 0.5518676042556763.\n",
      "epoch: 77, batch: 37, loss: 0.553479790687561.\n",
      "epoch: 77, batch: 38, loss: 0.6871061325073242.\n",
      "epoch: 77, batch: 39, loss: 0.7286403179168701.\n",
      "epoch: 77, batch: 40, loss: 0.8502011299133301.\n",
      "epoch: 77, batch: 41, loss: 0.6075485348701477.\n",
      "epoch: 77, batch: 42, loss: 0.5551415681838989.\n",
      "epoch: 77, batch: 43, loss: 0.5528295040130615.\n",
      "epoch: 77, batch: 44, loss: 0.5520967245101929.\n",
      "epoch: 77, batch: 45, loss: 0.5577876567840576.\n",
      "epoch: 77, batch: 46, loss: 0.551938533782959.\n",
      "epoch: 77, batch: 47, loss: 0.5517476797103882.\n",
      "epoch: 77, batch: 48, loss: 0.5691919326782227.\n",
      "epoch: 77, batch: 49, loss: 0.551701009273529.\n",
      "epoch: 77, batch: 50, loss: 0.6028282642364502.\n",
      "epoch: 77, batch: 51, loss: 0.6821562051773071.\n",
      "epoch: 77, batch: 52, loss: 0.6286605000495911.\n",
      "epoch: 78, batch: 0, loss: 0.5524945855140686.\n",
      "epoch: 78, batch: 1, loss: 0.5524533987045288.\n",
      "epoch: 78, batch: 2, loss: 0.5880683660507202.\n",
      "epoch: 78, batch: 3, loss: 0.554865300655365.\n",
      "epoch: 78, batch: 4, loss: 0.552552342414856.\n",
      "epoch: 78, batch: 5, loss: 0.8560784459114075.\n",
      "epoch: 78, batch: 6, loss: 0.5552029609680176.\n",
      "epoch: 78, batch: 7, loss: 0.552915096282959.\n",
      "epoch: 78, batch: 8, loss: 0.5531584024429321.\n",
      "epoch: 78, batch: 9, loss: 0.5520379543304443.\n",
      "epoch: 78, batch: 10, loss: 0.5533881187438965.\n",
      "epoch: 78, batch: 11, loss: 0.5785309076309204.\n",
      "epoch: 78, batch: 12, loss: 0.5619738101959229.\n",
      "epoch: 78, batch: 13, loss: 0.551774263381958.\n",
      "epoch: 78, batch: 14, loss: 0.5530610680580139.\n",
      "epoch: 78, batch: 15, loss: 0.619013249874115.\n",
      "epoch: 78, batch: 16, loss: 0.5996439456939697.\n",
      "epoch: 78, batch: 17, loss: 0.5532362461090088.\n",
      "epoch: 78, batch: 18, loss: 0.6902264356613159.\n",
      "epoch: 78, batch: 19, loss: 0.5521048307418823.\n",
      "epoch: 78, batch: 20, loss: 0.557384729385376.\n",
      "epoch: 78, batch: 21, loss: 0.5517222881317139.\n",
      "epoch: 78, batch: 22, loss: 0.5523394346237183.\n",
      "epoch: 78, batch: 23, loss: 0.5865516066551208.\n",
      "epoch: 78, batch: 24, loss: 0.5978025197982788.\n",
      "epoch: 78, batch: 25, loss: 0.5551846027374268.\n",
      "epoch: 78, batch: 26, loss: 0.6433537006378174.\n",
      "epoch: 78, batch: 27, loss: 0.7571913003921509.\n",
      "epoch: 78, batch: 28, loss: 0.5530838370323181.\n",
      "epoch: 78, batch: 29, loss: 0.5559878349304199.\n",
      "epoch: 78, batch: 30, loss: 0.5559741258621216.\n",
      "epoch: 78, batch: 31, loss: 0.5539445877075195.\n",
      "epoch: 78, batch: 32, loss: 0.55366051197052.\n",
      "epoch: 78, batch: 33, loss: 0.5518796443939209.\n",
      "epoch: 78, batch: 34, loss: 0.5684140920639038.\n",
      "epoch: 78, batch: 35, loss: 0.5625479221343994.\n",
      "epoch: 78, batch: 36, loss: 0.5525310039520264.\n",
      "epoch: 78, batch: 37, loss: 0.5546362996101379.\n",
      "epoch: 78, batch: 38, loss: 0.5729714632034302.\n",
      "epoch: 78, batch: 39, loss: 0.5721359252929688.\n",
      "epoch: 78, batch: 40, loss: 0.7521277666091919.\n",
      "epoch: 78, batch: 41, loss: 0.6201360821723938.\n",
      "epoch: 78, batch: 42, loss: 0.553424596786499.\n",
      "epoch: 78, batch: 43, loss: 0.560988187789917.\n",
      "epoch: 78, batch: 44, loss: 0.5540674328804016.\n",
      "epoch: 78, batch: 45, loss: 0.5710494518280029.\n",
      "epoch: 78, batch: 46, loss: 0.5537654757499695.\n",
      "epoch: 78, batch: 47, loss: 0.5518126487731934.\n",
      "epoch: 78, batch: 48, loss: 0.5586361885070801.\n",
      "epoch: 78, batch: 49, loss: 0.8526935577392578.\n",
      "epoch: 78, batch: 50, loss: 0.5518523454666138.\n",
      "epoch: 78, batch: 51, loss: 0.5527812242507935.\n",
      "epoch: 78, batch: 52, loss: 0.5527949333190918.\n",
      "epoch: 79, batch: 0, loss: 0.5528738498687744.\n",
      "epoch: 79, batch: 1, loss: 0.5544436573982239.\n",
      "epoch: 79, batch: 2, loss: 0.5842254161834717.\n",
      "epoch: 79, batch: 3, loss: 0.5519444942474365.\n",
      "epoch: 79, batch: 4, loss: 0.5526224970817566.\n",
      "epoch: 79, batch: 5, loss: 0.8017781376838684.\n",
      "epoch: 79, batch: 6, loss: 0.5519630908966064.\n",
      "epoch: 79, batch: 7, loss: 0.5520926713943481.\n",
      "epoch: 79, batch: 8, loss: 0.552949070930481.\n",
      "epoch: 79, batch: 9, loss: 0.5581638813018799.\n",
      "epoch: 79, batch: 10, loss: 0.567064642906189.\n",
      "epoch: 79, batch: 11, loss: 0.5518596172332764.\n",
      "epoch: 79, batch: 12, loss: 0.7112864851951599.\n",
      "epoch: 79, batch: 13, loss: 0.5517957210540771.\n",
      "epoch: 79, batch: 14, loss: 0.5747390985488892.\n",
      "epoch: 79, batch: 15, loss: 0.5519058704376221.\n",
      "epoch: 79, batch: 16, loss: 0.5529145002365112.\n",
      "epoch: 79, batch: 17, loss: 0.5572755932807922.\n",
      "epoch: 79, batch: 18, loss: 0.5562849044799805.\n",
      "epoch: 79, batch: 19, loss: 0.5561367869377136.\n",
      "epoch: 79, batch: 20, loss: 0.5560871362686157.\n",
      "epoch: 79, batch: 21, loss: 0.5562906265258789.\n",
      "epoch: 79, batch: 22, loss: 0.8909546136856079.\n",
      "epoch: 79, batch: 23, loss: 0.6766562461853027.\n",
      "epoch: 79, batch: 24, loss: 0.5519431829452515.\n",
      "epoch: 79, batch: 25, loss: 0.5549441576004028.\n",
      "epoch: 79, batch: 26, loss: 0.5517796277999878.\n",
      "epoch: 79, batch: 27, loss: 0.562828540802002.\n",
      "epoch: 79, batch: 28, loss: 0.551991879940033.\n",
      "epoch: 79, batch: 29, loss: 0.6817511320114136.\n",
      "epoch: 79, batch: 30, loss: 0.5543464422225952.\n",
      "epoch: 79, batch: 31, loss: 0.5999640226364136.\n",
      "epoch: 79, batch: 32, loss: 0.5533578395843506.\n",
      "epoch: 79, batch: 33, loss: 0.6466181874275208.\n",
      "epoch: 79, batch: 34, loss: 0.5633053183555603.\n",
      "epoch: 79, batch: 35, loss: 0.5954402685165405.\n",
      "epoch: 79, batch: 36, loss: 0.5682337880134583.\n",
      "epoch: 79, batch: 37, loss: 0.553195595741272.\n",
      "epoch: 79, batch: 38, loss: 0.5724102258682251.\n",
      "epoch: 79, batch: 39, loss: 0.5617702603340149.\n",
      "epoch: 79, batch: 40, loss: 0.5525096654891968.\n",
      "epoch: 79, batch: 41, loss: 0.5524662137031555.\n",
      "epoch: 79, batch: 42, loss: 0.556523323059082.\n",
      "epoch: 79, batch: 43, loss: 0.6249569654464722.\n",
      "epoch: 79, batch: 44, loss: 0.5519428253173828.\n",
      "epoch: 79, batch: 45, loss: 0.5537987947463989.\n",
      "epoch: 79, batch: 46, loss: 0.5544439554214478.\n",
      "epoch: 79, batch: 47, loss: 0.5734463930130005.\n",
      "epoch: 79, batch: 48, loss: 0.5524556636810303.\n",
      "epoch: 79, batch: 49, loss: 0.5522735118865967.\n",
      "epoch: 79, batch: 50, loss: 0.6915922164916992.\n",
      "epoch: 79, batch: 51, loss: 0.5550858974456787.\n",
      "epoch: 79, batch: 52, loss: 0.5535591840744019.\n",
      "epoch: 80, batch: 0, loss: 0.6250797510147095.\n",
      "epoch: 80, batch: 1, loss: 0.5518491268157959.\n",
      "epoch: 80, batch: 2, loss: 0.8654701709747314.\n",
      "epoch: 80, batch: 3, loss: 0.5517882108688354.\n",
      "epoch: 80, batch: 4, loss: 0.5629628896713257.\n",
      "epoch: 80, batch: 5, loss: 0.5585271120071411.\n",
      "epoch: 80, batch: 6, loss: 0.5523505210876465.\n",
      "epoch: 80, batch: 7, loss: 0.553697943687439.\n",
      "epoch: 80, batch: 8, loss: 0.553839921951294.\n",
      "epoch: 80, batch: 9, loss: 0.5537979006767273.\n",
      "epoch: 80, batch: 10, loss: 0.6527336835861206.\n",
      "epoch: 80, batch: 11, loss: 0.5591840744018555.\n",
      "epoch: 80, batch: 12, loss: 0.7266080379486084.\n",
      "epoch: 80, batch: 13, loss: 0.5528122186660767.\n",
      "epoch: 80, batch: 14, loss: 0.5523577928543091.\n",
      "epoch: 80, batch: 15, loss: 0.6621665954589844.\n",
      "epoch: 80, batch: 16, loss: 0.553392767906189.\n",
      "epoch: 80, batch: 17, loss: 0.5521891713142395.\n",
      "epoch: 80, batch: 18, loss: 0.5539701581001282.\n",
      "epoch: 80, batch: 19, loss: 0.5529791116714478.\n",
      "epoch: 80, batch: 20, loss: 0.5531423091888428.\n",
      "epoch: 80, batch: 21, loss: 0.5897566080093384.\n",
      "epoch: 80, batch: 22, loss: 0.5517227053642273.\n",
      "epoch: 80, batch: 23, loss: 0.559282660484314.\n",
      "epoch: 80, batch: 24, loss: 0.5548667311668396.\n",
      "epoch: 80, batch: 25, loss: 0.5533881187438965.\n",
      "epoch: 80, batch: 26, loss: 0.552425742149353.\n",
      "epoch: 80, batch: 27, loss: 0.5626212954521179.\n",
      "epoch: 80, batch: 28, loss: 0.5532416105270386.\n",
      "epoch: 80, batch: 29, loss: 0.5532755851745605.\n",
      "epoch: 80, batch: 30, loss: 0.5516606569290161.\n",
      "epoch: 80, batch: 31, loss: 0.5523378849029541.\n",
      "epoch: 80, batch: 32, loss: 0.6118090748786926.\n",
      "epoch: 80, batch: 33, loss: 0.7706514000892639.\n",
      "epoch: 80, batch: 34, loss: 0.7014734745025635.\n",
      "epoch: 80, batch: 35, loss: 0.5613654851913452.\n",
      "epoch: 80, batch: 36, loss: 0.5597853660583496.\n",
      "epoch: 80, batch: 37, loss: 0.5547480583190918.\n",
      "epoch: 80, batch: 38, loss: 0.5517661571502686.\n",
      "epoch: 80, batch: 39, loss: 0.559476375579834.\n",
      "epoch: 80, batch: 40, loss: 0.8524295091629028.\n",
      "epoch: 80, batch: 41, loss: 0.5564954876899719.\n",
      "epoch: 80, batch: 42, loss: 0.5633882284164429.\n",
      "epoch: 80, batch: 43, loss: 0.5516537427902222.\n",
      "epoch: 80, batch: 44, loss: 0.5547580122947693.\n",
      "epoch: 80, batch: 45, loss: 0.6276668310165405.\n",
      "epoch: 80, batch: 46, loss: 0.5594673752784729.\n",
      "epoch: 80, batch: 47, loss: 0.5553988218307495.\n",
      "epoch: 80, batch: 48, loss: 0.5519815683364868.\n",
      "epoch: 80, batch: 49, loss: 0.5662763118743896.\n",
      "epoch: 80, batch: 50, loss: 0.5524824261665344.\n",
      "epoch: 80, batch: 51, loss: 0.5520334243774414.\n",
      "epoch: 80, batch: 52, loss: 0.5521625876426697.\n",
      "epoch: 81, batch: 0, loss: 0.9527443051338196.\n",
      "epoch: 81, batch: 1, loss: 0.5956590175628662.\n",
      "epoch: 81, batch: 2, loss: 0.5554935932159424.\n",
      "epoch: 81, batch: 3, loss: 0.5526256561279297.\n",
      "epoch: 81, batch: 4, loss: 0.552013099193573.\n",
      "epoch: 81, batch: 5, loss: 0.5938066840171814.\n",
      "epoch: 81, batch: 6, loss: 0.6057883501052856.\n",
      "epoch: 81, batch: 7, loss: 0.5519300699234009.\n",
      "epoch: 81, batch: 8, loss: 0.553004264831543.\n",
      "epoch: 81, batch: 9, loss: 0.5515379309654236.\n",
      "epoch: 81, batch: 10, loss: 0.5522506237030029.\n",
      "epoch: 81, batch: 11, loss: 0.5563571453094482.\n",
      "epoch: 81, batch: 12, loss: 0.5565100908279419.\n",
      "epoch: 81, batch: 13, loss: 0.5539355278015137.\n",
      "epoch: 81, batch: 14, loss: 0.5518748760223389.\n",
      "epoch: 81, batch: 15, loss: 0.5548198819160461.\n",
      "epoch: 81, batch: 16, loss: 0.5525637865066528.\n",
      "epoch: 81, batch: 17, loss: 0.5523684024810791.\n",
      "epoch: 81, batch: 18, loss: 0.5736410021781921.\n",
      "epoch: 81, batch: 19, loss: 0.55305016040802.\n",
      "epoch: 81, batch: 20, loss: 0.5517342686653137.\n",
      "epoch: 81, batch: 21, loss: 0.5539876222610474.\n",
      "epoch: 81, batch: 22, loss: 0.5694195032119751.\n",
      "epoch: 81, batch: 23, loss: 0.5551586747169495.\n",
      "epoch: 81, batch: 24, loss: 0.5652921795845032.\n",
      "epoch: 81, batch: 25, loss: 0.5527138710021973.\n",
      "epoch: 81, batch: 26, loss: 0.5928798913955688.\n",
      "epoch: 81, batch: 27, loss: 0.5614821314811707.\n",
      "epoch: 81, batch: 28, loss: 0.5533108115196228.\n",
      "epoch: 81, batch: 29, loss: 0.5521857738494873.\n",
      "epoch: 81, batch: 30, loss: 1.0614508390426636.\n",
      "epoch: 81, batch: 31, loss: 0.551796555519104.\n",
      "epoch: 81, batch: 32, loss: 0.5534580945968628.\n",
      "epoch: 81, batch: 33, loss: 0.5577625036239624.\n",
      "epoch: 81, batch: 34, loss: 0.5524425506591797.\n",
      "epoch: 81, batch: 35, loss: 0.5794934034347534.\n",
      "epoch: 81, batch: 36, loss: 0.5521374940872192.\n",
      "epoch: 81, batch: 37, loss: 0.553025484085083.\n",
      "epoch: 81, batch: 38, loss: 0.5587353706359863.\n",
      "epoch: 81, batch: 39, loss: 0.5520837306976318.\n",
      "epoch: 81, batch: 40, loss: 0.5582203269004822.\n",
      "epoch: 81, batch: 41, loss: 0.5561510920524597.\n",
      "epoch: 81, batch: 42, loss: 0.5531094074249268.\n",
      "epoch: 81, batch: 43, loss: 0.7276694774627686.\n",
      "epoch: 81, batch: 44, loss: 0.5518996715545654.\n",
      "epoch: 81, batch: 45, loss: 0.552323579788208.\n",
      "epoch: 81, batch: 46, loss: 0.5519542694091797.\n",
      "epoch: 81, batch: 47, loss: 0.6241053342819214.\n",
      "epoch: 81, batch: 48, loss: 0.732161819934845.\n",
      "epoch: 81, batch: 49, loss: 0.5604375004768372.\n",
      "epoch: 81, batch: 50, loss: 0.5600539445877075.\n",
      "epoch: 81, batch: 51, loss: 0.5550240278244019.\n",
      "epoch: 81, batch: 52, loss: 0.5518296360969543.\n",
      "epoch: 82, batch: 0, loss: 0.5534621477127075.\n",
      "epoch: 82, batch: 1, loss: 0.6083232164382935.\n",
      "epoch: 82, batch: 2, loss: 0.6899869441986084.\n",
      "epoch: 82, batch: 3, loss: 0.5649803876876831.\n",
      "epoch: 82, batch: 4, loss: 0.6405742168426514.\n",
      "epoch: 82, batch: 5, loss: 0.5566503405570984.\n",
      "epoch: 82, batch: 6, loss: 0.8197799921035767.\n",
      "epoch: 82, batch: 7, loss: 0.5580366849899292.\n",
      "epoch: 82, batch: 8, loss: 0.5558310747146606.\n",
      "epoch: 82, batch: 9, loss: 0.7937988638877869.\n",
      "epoch: 82, batch: 10, loss: 0.5537842512130737.\n",
      "epoch: 82, batch: 11, loss: 0.5535914897918701.\n",
      "epoch: 82, batch: 12, loss: 0.552268385887146.\n",
      "epoch: 82, batch: 13, loss: 0.5517494678497314.\n",
      "epoch: 82, batch: 14, loss: 0.5524082183837891.\n",
      "epoch: 82, batch: 15, loss: 0.552658200263977.\n",
      "epoch: 82, batch: 16, loss: 0.55403733253479.\n",
      "epoch: 82, batch: 17, loss: 0.5537606477737427.\n",
      "epoch: 82, batch: 18, loss: 0.5520306825637817.\n",
      "epoch: 82, batch: 19, loss: 0.5595735311508179.\n",
      "epoch: 82, batch: 20, loss: 0.5539037585258484.\n",
      "epoch: 82, batch: 21, loss: 0.5558886528015137.\n",
      "epoch: 82, batch: 22, loss: 0.5850769281387329.\n",
      "epoch: 82, batch: 23, loss: 0.5550225973129272.\n",
      "epoch: 82, batch: 24, loss: 0.5524849891662598.\n",
      "epoch: 82, batch: 25, loss: 0.5671406984329224.\n",
      "epoch: 82, batch: 26, loss: 0.5651861429214478.\n",
      "epoch: 82, batch: 27, loss: 0.5520514845848083.\n",
      "epoch: 82, batch: 28, loss: 0.5547345876693726.\n",
      "epoch: 82, batch: 29, loss: 0.6284462809562683.\n",
      "epoch: 82, batch: 30, loss: 0.5582888126373291.\n",
      "epoch: 82, batch: 31, loss: 0.552128791809082.\n",
      "epoch: 82, batch: 32, loss: 0.5524212121963501.\n",
      "epoch: 82, batch: 33, loss: 0.5520765781402588.\n",
      "epoch: 82, batch: 34, loss: 0.553993284702301.\n",
      "epoch: 82, batch: 35, loss: 0.6691279411315918.\n",
      "epoch: 82, batch: 36, loss: 0.5651026964187622.\n",
      "epoch: 82, batch: 37, loss: 0.5548763871192932.\n",
      "epoch: 82, batch: 38, loss: 0.6031652688980103.\n",
      "epoch: 82, batch: 39, loss: 0.5671143531799316.\n",
      "epoch: 82, batch: 40, loss: 0.5517891645431519.\n",
      "epoch: 82, batch: 41, loss: 0.5526825189590454.\n",
      "epoch: 82, batch: 42, loss: 0.5516942143440247.\n",
      "epoch: 82, batch: 43, loss: 0.5521575212478638.\n",
      "epoch: 82, batch: 44, loss: 0.8477097749710083.\n",
      "epoch: 82, batch: 45, loss: 0.5526046752929688.\n",
      "epoch: 82, batch: 46, loss: 0.5533910989761353.\n",
      "epoch: 82, batch: 47, loss: 0.5541598796844482.\n",
      "epoch: 82, batch: 48, loss: 0.5523780584335327.\n",
      "epoch: 82, batch: 49, loss: 0.560808002948761.\n",
      "epoch: 82, batch: 50, loss: 0.5521726012229919.\n",
      "epoch: 82, batch: 51, loss: 0.5522652268409729.\n",
      "epoch: 82, batch: 52, loss: 0.5515506863594055.\n",
      "epoch: 83, batch: 0, loss: 0.5518522262573242.\n",
      "epoch: 83, batch: 1, loss: 0.5554437041282654.\n",
      "epoch: 83, batch: 2, loss: 0.5567682385444641.\n",
      "epoch: 83, batch: 3, loss: 0.5650852918624878.\n",
      "epoch: 83, batch: 4, loss: 0.5542023777961731.\n",
      "epoch: 83, batch: 5, loss: 0.5534384250640869.\n",
      "epoch: 83, batch: 6, loss: 0.5605477094650269.\n",
      "epoch: 83, batch: 7, loss: 0.5521160960197449.\n",
      "epoch: 83, batch: 8, loss: 0.842425525188446.\n",
      "epoch: 83, batch: 9, loss: 0.5522972345352173.\n",
      "epoch: 83, batch: 10, loss: 0.5848528742790222.\n",
      "epoch: 83, batch: 11, loss: 0.5550550222396851.\n",
      "epoch: 83, batch: 12, loss: 0.5522345304489136.\n",
      "epoch: 83, batch: 13, loss: 0.5523622035980225.\n",
      "epoch: 83, batch: 14, loss: 0.5689299702644348.\n",
      "epoch: 83, batch: 15, loss: 0.5530836582183838.\n",
      "epoch: 83, batch: 16, loss: 0.5518629550933838.\n",
      "epoch: 83, batch: 17, loss: 0.5529085993766785.\n",
      "epoch: 83, batch: 18, loss: 0.5520420074462891.\n",
      "epoch: 83, batch: 19, loss: 0.5544120073318481.\n",
      "epoch: 83, batch: 20, loss: 0.5659215450286865.\n",
      "epoch: 83, batch: 21, loss: 0.5532187223434448.\n",
      "epoch: 83, batch: 22, loss: 0.5524813532829285.\n",
      "epoch: 83, batch: 23, loss: 0.5542687177658081.\n",
      "epoch: 83, batch: 24, loss: 0.5531569123268127.\n",
      "epoch: 83, batch: 25, loss: 0.5522795915603638.\n",
      "epoch: 83, batch: 26, loss: 0.6624565124511719.\n",
      "epoch: 83, batch: 27, loss: 0.5635369420051575.\n",
      "epoch: 83, batch: 28, loss: 0.746909499168396.\n",
      "epoch: 83, batch: 29, loss: 0.5532848834991455.\n",
      "epoch: 83, batch: 30, loss: 0.5519108772277832.\n",
      "epoch: 83, batch: 31, loss: 0.63091641664505.\n",
      "epoch: 83, batch: 32, loss: 0.5534710884094238.\n",
      "epoch: 83, batch: 33, loss: 0.551817774772644.\n",
      "epoch: 83, batch: 34, loss: 0.6499286890029907.\n",
      "epoch: 83, batch: 35, loss: 0.5528347492218018.\n",
      "epoch: 83, batch: 36, loss: 0.5534255504608154.\n",
      "epoch: 83, batch: 37, loss: 0.5522973537445068.\n",
      "epoch: 83, batch: 38, loss: 0.5557220578193665.\n",
      "epoch: 83, batch: 39, loss: 0.5541075468063354.\n",
      "epoch: 83, batch: 40, loss: 0.5637129545211792.\n",
      "epoch: 83, batch: 41, loss: 0.5517430901527405.\n",
      "epoch: 83, batch: 42, loss: 0.5519101619720459.\n",
      "epoch: 83, batch: 43, loss: 0.5564665794372559.\n",
      "epoch: 83, batch: 44, loss: 0.5577898621559143.\n",
      "epoch: 83, batch: 45, loss: 0.5527173280715942.\n",
      "epoch: 83, batch: 46, loss: 0.5517911314964294.\n",
      "epoch: 83, batch: 47, loss: 0.5519506931304932.\n",
      "epoch: 83, batch: 48, loss: 0.6391986608505249.\n",
      "epoch: 83, batch: 49, loss: 0.7253291606903076.\n",
      "epoch: 83, batch: 50, loss: 0.6462318897247314.\n",
      "epoch: 83, batch: 51, loss: 0.7399419546127319.\n",
      "epoch: 83, batch: 52, loss: 0.5522775053977966.\n",
      "epoch: 84, batch: 0, loss: 0.5557827353477478.\n",
      "epoch: 84, batch: 1, loss: 0.5615686178207397.\n",
      "epoch: 84, batch: 2, loss: 0.5520248413085938.\n",
      "epoch: 84, batch: 3, loss: 0.5516943335533142.\n",
      "epoch: 84, batch: 4, loss: 0.634484052658081.\n",
      "epoch: 84, batch: 5, loss: 0.7105445861816406.\n",
      "epoch: 84, batch: 6, loss: 0.5526764392852783.\n",
      "epoch: 84, batch: 7, loss: 0.6008515357971191.\n",
      "epoch: 84, batch: 8, loss: 0.5523092746734619.\n",
      "epoch: 84, batch: 9, loss: 0.5529855489730835.\n",
      "epoch: 84, batch: 10, loss: 0.6281120777130127.\n",
      "epoch: 84, batch: 11, loss: 0.8858850002288818.\n",
      "epoch: 84, batch: 12, loss: 0.5544302463531494.\n",
      "epoch: 84, batch: 13, loss: 0.581684947013855.\n",
      "epoch: 84, batch: 14, loss: 0.5611687302589417.\n",
      "epoch: 84, batch: 15, loss: 0.5612776279449463.\n",
      "epoch: 84, batch: 16, loss: 0.5559208393096924.\n",
      "epoch: 84, batch: 17, loss: 0.5540258884429932.\n",
      "epoch: 84, batch: 18, loss: 0.554060161113739.\n",
      "epoch: 84, batch: 19, loss: 0.5659600496292114.\n",
      "epoch: 84, batch: 20, loss: 0.5525883436203003.\n",
      "epoch: 84, batch: 21, loss: 0.5523862838745117.\n",
      "epoch: 84, batch: 22, loss: 0.5549443960189819.\n",
      "epoch: 84, batch: 23, loss: 0.5979507565498352.\n",
      "epoch: 84, batch: 24, loss: 0.5518779754638672.\n",
      "epoch: 84, batch: 25, loss: 0.5521187782287598.\n",
      "epoch: 84, batch: 26, loss: 0.5585155487060547.\n",
      "epoch: 84, batch: 27, loss: 0.552137017250061.\n",
      "epoch: 84, batch: 28, loss: 0.7486914396286011.\n",
      "epoch: 84, batch: 29, loss: 0.5540038347244263.\n",
      "epoch: 84, batch: 30, loss: 0.5520958304405212.\n",
      "epoch: 84, batch: 31, loss: 0.5678200721740723.\n",
      "epoch: 84, batch: 32, loss: 0.5523031949996948.\n",
      "epoch: 84, batch: 33, loss: 0.5536013841629028.\n",
      "epoch: 84, batch: 34, loss: 0.5534467697143555.\n",
      "epoch: 84, batch: 35, loss: 0.5545909404754639.\n",
      "epoch: 84, batch: 36, loss: 0.553892195224762.\n",
      "epoch: 84, batch: 37, loss: 0.5520506501197815.\n",
      "epoch: 84, batch: 38, loss: 0.5529177188873291.\n",
      "epoch: 84, batch: 39, loss: 0.8198317289352417.\n",
      "epoch: 84, batch: 40, loss: 0.5520786046981812.\n",
      "epoch: 84, batch: 41, loss: 0.5521050691604614.\n",
      "epoch: 84, batch: 42, loss: 0.5516507625579834.\n",
      "epoch: 84, batch: 43, loss: 0.5679227113723755.\n",
      "epoch: 84, batch: 44, loss: 0.5522476434707642.\n",
      "epoch: 84, batch: 45, loss: 0.5521953105926514.\n",
      "epoch: 84, batch: 46, loss: 0.6390985250473022.\n",
      "epoch: 84, batch: 47, loss: 0.5542967319488525.\n",
      "epoch: 84, batch: 48, loss: 0.55350661277771.\n",
      "epoch: 84, batch: 49, loss: 0.5905922055244446.\n",
      "epoch: 84, batch: 50, loss: 0.5618677139282227.\n",
      "epoch: 84, batch: 51, loss: 0.5527931451797485.\n",
      "epoch: 84, batch: 52, loss: 0.5548874139785767.\n",
      "epoch: 85, batch: 0, loss: 0.5538499355316162.\n",
      "epoch: 85, batch: 1, loss: 0.6376597881317139.\n",
      "epoch: 85, batch: 2, loss: 0.5536874532699585.\n",
      "epoch: 85, batch: 3, loss: 0.608229398727417.\n",
      "epoch: 85, batch: 4, loss: 0.5651409029960632.\n",
      "epoch: 85, batch: 5, loss: 0.5520131587982178.\n",
      "epoch: 85, batch: 6, loss: 0.8314136266708374.\n",
      "epoch: 85, batch: 7, loss: 0.5536985397338867.\n",
      "epoch: 85, batch: 8, loss: 0.553051233291626.\n",
      "epoch: 85, batch: 9, loss: 0.5536289215087891.\n",
      "epoch: 85, batch: 10, loss: 0.5519073009490967.\n",
      "epoch: 85, batch: 11, loss: 0.5600206851959229.\n",
      "epoch: 85, batch: 12, loss: 0.5521653890609741.\n",
      "epoch: 85, batch: 13, loss: 0.6500234007835388.\n",
      "epoch: 85, batch: 14, loss: 0.5602555274963379.\n",
      "epoch: 85, batch: 15, loss: 0.563103437423706.\n",
      "epoch: 85, batch: 16, loss: 0.5524200201034546.\n",
      "epoch: 85, batch: 17, loss: 0.5520830154418945.\n",
      "epoch: 85, batch: 18, loss: 0.5518589019775391.\n",
      "epoch: 85, batch: 19, loss: 0.5516117811203003.\n",
      "epoch: 85, batch: 20, loss: 0.55472332239151.\n",
      "epoch: 85, batch: 21, loss: 0.5546542406082153.\n",
      "epoch: 85, batch: 22, loss: 0.5610793828964233.\n",
      "epoch: 85, batch: 23, loss: 0.5529053211212158.\n",
      "epoch: 85, batch: 24, loss: 0.584858775138855.\n",
      "epoch: 85, batch: 25, loss: 0.5781387090682983.\n",
      "epoch: 85, batch: 26, loss: 0.5520460605621338.\n",
      "epoch: 85, batch: 27, loss: 0.5562455058097839.\n",
      "epoch: 85, batch: 28, loss: 0.6034709215164185.\n",
      "epoch: 85, batch: 29, loss: 0.7979270815849304.\n",
      "epoch: 85, batch: 30, loss: 0.5617032051086426.\n",
      "epoch: 85, batch: 31, loss: 0.5527659058570862.\n",
      "epoch: 85, batch: 32, loss: 0.552661120891571.\n",
      "epoch: 85, batch: 33, loss: 0.5531927347183228.\n",
      "epoch: 85, batch: 34, loss: 0.5520955324172974.\n",
      "epoch: 85, batch: 35, loss: 0.5557562112808228.\n",
      "epoch: 85, batch: 36, loss: 0.7503790855407715.\n",
      "epoch: 85, batch: 37, loss: 0.5519644021987915.\n",
      "epoch: 85, batch: 38, loss: 0.5521526336669922.\n",
      "epoch: 85, batch: 39, loss: 0.552594006061554.\n",
      "epoch: 85, batch: 40, loss: 0.5973374843597412.\n",
      "epoch: 85, batch: 41, loss: 0.561470627784729.\n",
      "epoch: 85, batch: 42, loss: 0.5524563193321228.\n",
      "epoch: 85, batch: 43, loss: 0.5606238842010498.\n",
      "epoch: 85, batch: 44, loss: 0.5590413808822632.\n",
      "epoch: 85, batch: 45, loss: 0.5525331497192383.\n",
      "epoch: 85, batch: 46, loss: 0.5520799160003662.\n",
      "epoch: 85, batch: 47, loss: 0.552092432975769.\n",
      "epoch: 85, batch: 48, loss: 0.5544463396072388.\n",
      "epoch: 85, batch: 49, loss: 0.6571928262710571.\n",
      "epoch: 85, batch: 50, loss: 0.5596632361412048.\n",
      "epoch: 85, batch: 51, loss: 0.7160712480545044.\n",
      "epoch: 85, batch: 52, loss: 0.5535480380058289.\n",
      "epoch: 86, batch: 0, loss: 0.5529061555862427.\n",
      "epoch: 86, batch: 1, loss: 0.5527340173721313.\n",
      "epoch: 86, batch: 2, loss: 0.5530601739883423.\n",
      "epoch: 86, batch: 3, loss: 0.5629894733428955.\n",
      "epoch: 86, batch: 4, loss: 0.5518025159835815.\n",
      "epoch: 86, batch: 5, loss: 0.5582388639450073.\n",
      "epoch: 86, batch: 6, loss: 0.5519089102745056.\n",
      "epoch: 86, batch: 7, loss: 0.5521855354309082.\n",
      "epoch: 86, batch: 8, loss: 0.5722459554672241.\n",
      "epoch: 86, batch: 9, loss: 0.5522003173828125.\n",
      "epoch: 86, batch: 10, loss: 0.5529364943504333.\n",
      "epoch: 86, batch: 11, loss: 0.5527111291885376.\n",
      "epoch: 86, batch: 12, loss: 0.5518486499786377.\n",
      "epoch: 86, batch: 13, loss: 0.5540756583213806.\n",
      "epoch: 86, batch: 14, loss: 0.5599600076675415.\n",
      "epoch: 86, batch: 15, loss: 0.6593798995018005.\n",
      "epoch: 86, batch: 16, loss: 0.6701081991195679.\n",
      "epoch: 86, batch: 17, loss: 0.6988785266876221.\n",
      "epoch: 86, batch: 18, loss: 0.5776524543762207.\n",
      "epoch: 86, batch: 19, loss: 0.5520949363708496.\n",
      "epoch: 86, batch: 20, loss: 0.5520187616348267.\n",
      "epoch: 86, batch: 21, loss: 0.6622031927108765.\n",
      "epoch: 86, batch: 22, loss: 0.6130077838897705.\n",
      "epoch: 86, batch: 23, loss: 0.5519403219223022.\n",
      "epoch: 86, batch: 24, loss: 0.7470437288284302.\n",
      "epoch: 86, batch: 25, loss: 0.5531018972396851.\n",
      "epoch: 86, batch: 26, loss: 0.5552891492843628.\n",
      "epoch: 86, batch: 27, loss: 0.552035391330719.\n",
      "epoch: 86, batch: 28, loss: 0.5518941879272461.\n",
      "epoch: 86, batch: 29, loss: 0.5516619682312012.\n",
      "epoch: 86, batch: 30, loss: 0.5520585775375366.\n",
      "epoch: 86, batch: 31, loss: 0.6678246259689331.\n",
      "epoch: 86, batch: 32, loss: 0.551786482334137.\n",
      "epoch: 86, batch: 33, loss: 0.5647069811820984.\n",
      "epoch: 86, batch: 34, loss: 0.5588226318359375.\n",
      "epoch: 86, batch: 35, loss: 0.5521636009216309.\n",
      "epoch: 86, batch: 36, loss: 0.5534517765045166.\n",
      "epoch: 86, batch: 37, loss: 0.7010571956634521.\n",
      "epoch: 86, batch: 38, loss: 0.5547019839286804.\n",
      "epoch: 86, batch: 39, loss: 0.555418848991394.\n",
      "epoch: 86, batch: 40, loss: 0.5521602034568787.\n",
      "epoch: 86, batch: 41, loss: 0.5533746480941772.\n",
      "epoch: 86, batch: 42, loss: 0.5867986679077148.\n",
      "epoch: 86, batch: 43, loss: 0.5519880056381226.\n",
      "epoch: 86, batch: 44, loss: 0.5546689033508301.\n",
      "epoch: 86, batch: 45, loss: 0.552104651927948.\n",
      "epoch: 86, batch: 46, loss: 0.5649865865707397.\n",
      "epoch: 86, batch: 47, loss: 0.5521910190582275.\n",
      "epoch: 86, batch: 48, loss: 0.5540944933891296.\n",
      "epoch: 86, batch: 49, loss: 0.747884213924408.\n",
      "epoch: 86, batch: 50, loss: 0.5541961193084717.\n",
      "epoch: 86, batch: 51, loss: 0.5999096632003784.\n",
      "epoch: 86, batch: 52, loss: 0.5520336627960205.\n",
      "epoch: 87, batch: 0, loss: 0.5573246479034424.\n",
      "epoch: 87, batch: 1, loss: 0.5522792339324951.\n",
      "epoch: 87, batch: 2, loss: 0.5526576638221741.\n",
      "epoch: 87, batch: 3, loss: 0.5523500442504883.\n",
      "epoch: 87, batch: 4, loss: 0.5543873310089111.\n",
      "epoch: 87, batch: 5, loss: 0.5516194701194763.\n",
      "epoch: 87, batch: 6, loss: 0.7040891647338867.\n",
      "epoch: 87, batch: 7, loss: 0.5558979511260986.\n",
      "epoch: 87, batch: 8, loss: 0.5520622730255127.\n",
      "epoch: 87, batch: 9, loss: 0.5698069334030151.\n",
      "epoch: 87, batch: 10, loss: 0.5527312159538269.\n",
      "epoch: 87, batch: 11, loss: 0.5560723543167114.\n",
      "epoch: 87, batch: 12, loss: 0.5524498224258423.\n",
      "epoch: 87, batch: 13, loss: 0.5992985963821411.\n",
      "epoch: 87, batch: 14, loss: 0.5519233345985413.\n",
      "epoch: 87, batch: 15, loss: 0.5688971281051636.\n",
      "epoch: 87, batch: 16, loss: 0.5566611289978027.\n",
      "epoch: 87, batch: 17, loss: 0.5526567697525024.\n",
      "epoch: 87, batch: 18, loss: 0.5520704984664917.\n",
      "epoch: 87, batch: 19, loss: 0.625097393989563.\n",
      "epoch: 87, batch: 20, loss: 0.5520046949386597.\n",
      "epoch: 87, batch: 21, loss: 0.5577573180198669.\n",
      "epoch: 87, batch: 22, loss: 0.552269458770752.\n",
      "epoch: 87, batch: 23, loss: 0.5555359125137329.\n",
      "epoch: 87, batch: 24, loss: 0.670778751373291.\n",
      "epoch: 87, batch: 25, loss: 0.6068521738052368.\n",
      "epoch: 87, batch: 26, loss: 0.5808801054954529.\n",
      "epoch: 87, batch: 27, loss: 0.5524517297744751.\n",
      "epoch: 87, batch: 28, loss: 0.5563515424728394.\n",
      "epoch: 87, batch: 29, loss: 0.5522109270095825.\n",
      "epoch: 87, batch: 30, loss: 0.5522434115409851.\n",
      "epoch: 87, batch: 31, loss: 0.6298458576202393.\n",
      "epoch: 87, batch: 32, loss: 0.5520924925804138.\n",
      "epoch: 87, batch: 33, loss: 0.8076074123382568.\n",
      "epoch: 87, batch: 34, loss: 0.5522112846374512.\n",
      "epoch: 87, batch: 35, loss: 0.552116870880127.\n",
      "epoch: 87, batch: 36, loss: 0.5549683570861816.\n",
      "epoch: 87, batch: 37, loss: 0.5516869425773621.\n",
      "epoch: 87, batch: 38, loss: 0.55214524269104.\n",
      "epoch: 87, batch: 39, loss: 0.5526109933853149.\n",
      "epoch: 87, batch: 40, loss: 0.861687421798706.\n",
      "epoch: 87, batch: 41, loss: 0.5517832040786743.\n",
      "epoch: 87, batch: 42, loss: 0.558455228805542.\n",
      "epoch: 87, batch: 43, loss: 0.5541925430297852.\n",
      "epoch: 87, batch: 44, loss: 0.5517665147781372.\n",
      "epoch: 87, batch: 45, loss: 0.8260974884033203.\n",
      "epoch: 87, batch: 46, loss: 0.5579804182052612.\n",
      "epoch: 87, batch: 47, loss: 0.5519812107086182.\n",
      "epoch: 87, batch: 48, loss: 0.5528501272201538.\n",
      "epoch: 87, batch: 49, loss: 0.5567193031311035.\n",
      "epoch: 87, batch: 50, loss: 0.5573800206184387.\n",
      "epoch: 87, batch: 51, loss: 0.5519922375679016.\n",
      "epoch: 87, batch: 52, loss: 0.5522870421409607.\n",
      "epoch: 88, batch: 0, loss: 0.5517802238464355.\n",
      "epoch: 88, batch: 1, loss: 0.5551509261131287.\n",
      "epoch: 88, batch: 2, loss: 0.5794899463653564.\n",
      "epoch: 88, batch: 3, loss: 0.5516693592071533.\n",
      "epoch: 88, batch: 4, loss: 0.5518326759338379.\n",
      "epoch: 88, batch: 5, loss: 0.5866096615791321.\n",
      "epoch: 88, batch: 6, loss: 0.5759562253952026.\n",
      "epoch: 88, batch: 7, loss: 0.5583248138427734.\n",
      "epoch: 88, batch: 8, loss: 0.5521063804626465.\n",
      "epoch: 88, batch: 9, loss: 0.5568846464157104.\n",
      "epoch: 88, batch: 10, loss: 0.5807380676269531.\n",
      "epoch: 88, batch: 11, loss: 0.5573144555091858.\n",
      "epoch: 88, batch: 12, loss: 0.5523033142089844.\n",
      "epoch: 88, batch: 13, loss: 0.5518066883087158.\n",
      "epoch: 88, batch: 14, loss: 0.5525151491165161.\n",
      "epoch: 88, batch: 15, loss: 0.6690047383308411.\n",
      "epoch: 88, batch: 16, loss: 0.551922619342804.\n",
      "epoch: 88, batch: 17, loss: 0.5811231732368469.\n",
      "epoch: 88, batch: 18, loss: 0.5544940233230591.\n",
      "epoch: 88, batch: 19, loss: 0.5538020133972168.\n",
      "epoch: 88, batch: 20, loss: 0.6497656106948853.\n",
      "epoch: 88, batch: 21, loss: 0.6689951419830322.\n",
      "epoch: 88, batch: 22, loss: 0.5575559139251709.\n",
      "epoch: 88, batch: 23, loss: 0.5534405708312988.\n",
      "epoch: 88, batch: 24, loss: 0.5535268783569336.\n",
      "epoch: 88, batch: 25, loss: 0.5516799688339233.\n",
      "epoch: 88, batch: 26, loss: 0.5522267818450928.\n",
      "epoch: 88, batch: 27, loss: 0.5524414777755737.\n",
      "epoch: 88, batch: 28, loss: 0.552241325378418.\n",
      "epoch: 88, batch: 29, loss: 0.604122519493103.\n",
      "epoch: 88, batch: 30, loss: 0.552716851234436.\n",
      "epoch: 88, batch: 31, loss: 0.5521491765975952.\n",
      "epoch: 88, batch: 32, loss: 0.5593221187591553.\n",
      "epoch: 88, batch: 33, loss: 0.8714696168899536.\n",
      "epoch: 88, batch: 34, loss: 0.5630674362182617.\n",
      "epoch: 88, batch: 35, loss: 0.5520419478416443.\n",
      "epoch: 88, batch: 36, loss: 0.570598840713501.\n",
      "epoch: 88, batch: 37, loss: 0.919033408164978.\n",
      "epoch: 88, batch: 38, loss: 0.5549511909484863.\n",
      "epoch: 88, batch: 39, loss: 0.5519537925720215.\n",
      "epoch: 88, batch: 40, loss: 0.6205651760101318.\n",
      "epoch: 88, batch: 41, loss: 0.5518393516540527.\n",
      "epoch: 88, batch: 42, loss: 0.5522251129150391.\n",
      "epoch: 88, batch: 43, loss: 0.5530917048454285.\n",
      "epoch: 88, batch: 44, loss: 0.5824863314628601.\n",
      "epoch: 88, batch: 45, loss: 0.5894784927368164.\n",
      "epoch: 88, batch: 46, loss: 0.5656217336654663.\n",
      "epoch: 88, batch: 47, loss: 0.551703929901123.\n",
      "epoch: 88, batch: 48, loss: 0.5538785457611084.\n",
      "epoch: 88, batch: 49, loss: 0.6762422919273376.\n",
      "epoch: 88, batch: 50, loss: 0.7780485153198242.\n",
      "epoch: 88, batch: 51, loss: 0.5529100894927979.\n",
      "epoch: 88, batch: 52, loss: 0.5526508092880249.\n",
      "epoch: 89, batch: 0, loss: 0.5529645681381226.\n",
      "epoch: 89, batch: 1, loss: 0.552164614200592.\n",
      "epoch: 89, batch: 2, loss: 0.5518794059753418.\n",
      "epoch: 89, batch: 3, loss: 0.5528907775878906.\n",
      "epoch: 89, batch: 4, loss: 0.5520470142364502.\n",
      "epoch: 89, batch: 5, loss: 0.5826469659805298.\n",
      "epoch: 89, batch: 6, loss: 0.6320775747299194.\n",
      "epoch: 89, batch: 7, loss: 0.5544803142547607.\n",
      "epoch: 89, batch: 8, loss: 0.5549488067626953.\n",
      "epoch: 89, batch: 9, loss: 0.7084953784942627.\n",
      "epoch: 89, batch: 10, loss: 0.553701639175415.\n",
      "epoch: 89, batch: 11, loss: 0.5518289804458618.\n",
      "epoch: 89, batch: 12, loss: 0.6626968383789062.\n",
      "epoch: 89, batch: 13, loss: 0.6364836096763611.\n",
      "epoch: 89, batch: 14, loss: 0.5529175996780396.\n",
      "epoch: 89, batch: 15, loss: 0.5574739575386047.\n",
      "epoch: 89, batch: 16, loss: 0.5526021718978882.\n",
      "epoch: 89, batch: 17, loss: 0.5517138838768005.\n",
      "epoch: 89, batch: 18, loss: 0.5522618889808655.\n",
      "epoch: 89, batch: 19, loss: 0.5523149967193604.\n",
      "epoch: 89, batch: 20, loss: 0.5566568374633789.\n",
      "epoch: 89, batch: 21, loss: 0.5519745349884033.\n",
      "epoch: 89, batch: 22, loss: 0.5520597696304321.\n",
      "epoch: 89, batch: 23, loss: 0.5530462265014648.\n",
      "epoch: 89, batch: 24, loss: 0.5518107414245605.\n",
      "epoch: 89, batch: 25, loss: 0.6795753836631775.\n",
      "epoch: 89, batch: 26, loss: 0.551551342010498.\n",
      "epoch: 89, batch: 27, loss: 0.552664577960968.\n",
      "epoch: 89, batch: 28, loss: 0.5515665411949158.\n",
      "epoch: 89, batch: 29, loss: 0.553222119808197.\n",
      "epoch: 89, batch: 30, loss: 0.551790177822113.\n",
      "epoch: 89, batch: 31, loss: 0.553330659866333.\n",
      "epoch: 89, batch: 32, loss: 0.5521057844161987.\n",
      "epoch: 89, batch: 33, loss: 0.7152544260025024.\n",
      "epoch: 89, batch: 34, loss: 0.5607330203056335.\n",
      "epoch: 89, batch: 35, loss: 0.5533920526504517.\n",
      "epoch: 89, batch: 36, loss: 0.9607246518135071.\n",
      "epoch: 89, batch: 37, loss: 0.5556095838546753.\n",
      "epoch: 89, batch: 38, loss: 0.5545444488525391.\n",
      "epoch: 89, batch: 39, loss: 0.6611700654029846.\n",
      "epoch: 89, batch: 40, loss: 0.7195357084274292.\n",
      "epoch: 89, batch: 41, loss: 0.5645517110824585.\n",
      "epoch: 89, batch: 42, loss: 0.55400151014328.\n",
      "epoch: 89, batch: 43, loss: 0.55311119556427.\n",
      "epoch: 89, batch: 44, loss: 0.5527348518371582.\n",
      "epoch: 89, batch: 45, loss: 0.552107572555542.\n",
      "epoch: 89, batch: 46, loss: 0.5529593229293823.\n",
      "epoch: 89, batch: 47, loss: 0.5659142732620239.\n",
      "epoch: 89, batch: 48, loss: 0.5579543709754944.\n",
      "epoch: 89, batch: 49, loss: 0.6098234057426453.\n",
      "epoch: 89, batch: 50, loss: 0.5519530773162842.\n",
      "epoch: 89, batch: 51, loss: 0.552237868309021.\n",
      "epoch: 89, batch: 52, loss: 0.5528228878974915.\n",
      "epoch: 90, batch: 0, loss: 0.5517830848693848.\n",
      "epoch: 90, batch: 1, loss: 0.5792570114135742.\n",
      "epoch: 90, batch: 2, loss: 0.5537194013595581.\n",
      "epoch: 90, batch: 3, loss: 0.5591939091682434.\n",
      "epoch: 90, batch: 4, loss: 0.5519838929176331.\n",
      "epoch: 90, batch: 5, loss: 0.5595604181289673.\n",
      "epoch: 90, batch: 6, loss: 1.0574121475219727.\n",
      "epoch: 90, batch: 7, loss: 0.6026511192321777.\n",
      "epoch: 90, batch: 8, loss: 0.5570940375328064.\n",
      "epoch: 90, batch: 9, loss: 0.560286819934845.\n",
      "epoch: 90, batch: 10, loss: 0.5725619196891785.\n",
      "epoch: 90, batch: 11, loss: 0.553138792514801.\n",
      "epoch: 90, batch: 12, loss: 0.5521236062049866.\n",
      "epoch: 90, batch: 13, loss: 0.5798076391220093.\n",
      "epoch: 90, batch: 14, loss: 0.5555862784385681.\n",
      "epoch: 90, batch: 15, loss: 0.5867985486984253.\n",
      "epoch: 90, batch: 16, loss: 0.5518332719802856.\n",
      "epoch: 90, batch: 17, loss: 0.5521410703659058.\n",
      "epoch: 90, batch: 18, loss: 0.565001368522644.\n",
      "epoch: 90, batch: 19, loss: 0.5541243553161621.\n",
      "epoch: 90, batch: 20, loss: 0.552274227142334.\n",
      "epoch: 90, batch: 21, loss: 0.5533114671707153.\n",
      "epoch: 90, batch: 22, loss: 0.584069013595581.\n",
      "epoch: 90, batch: 23, loss: 0.5523190498352051.\n",
      "epoch: 90, batch: 24, loss: 0.5520132780075073.\n",
      "epoch: 90, batch: 25, loss: 0.5534766912460327.\n",
      "epoch: 90, batch: 26, loss: 0.551898181438446.\n",
      "epoch: 90, batch: 27, loss: 0.5541078448295593.\n",
      "epoch: 90, batch: 28, loss: 0.55434250831604.\n",
      "epoch: 90, batch: 29, loss: 0.5517430901527405.\n",
      "epoch: 90, batch: 30, loss: 0.5519461631774902.\n",
      "epoch: 90, batch: 31, loss: 0.552080512046814.\n",
      "epoch: 90, batch: 32, loss: 0.5555801391601562.\n",
      "epoch: 90, batch: 33, loss: 0.5521138906478882.\n",
      "epoch: 90, batch: 34, loss: 0.5520034432411194.\n",
      "epoch: 90, batch: 35, loss: 0.8404784202575684.\n",
      "epoch: 90, batch: 36, loss: 0.5523715019226074.\n",
      "epoch: 90, batch: 37, loss: 0.8920078277587891.\n",
      "epoch: 90, batch: 38, loss: 0.7198987007141113.\n",
      "epoch: 90, batch: 39, loss: 0.5556220412254333.\n",
      "epoch: 90, batch: 40, loss: 0.5518314838409424.\n",
      "epoch: 90, batch: 41, loss: 0.5569802522659302.\n",
      "epoch: 90, batch: 42, loss: 0.5556876063346863.\n",
      "epoch: 90, batch: 43, loss: 0.5552881360054016.\n",
      "epoch: 90, batch: 44, loss: 0.5516932010650635.\n",
      "epoch: 90, batch: 45, loss: 0.5823233127593994.\n",
      "epoch: 90, batch: 46, loss: 0.5644724369049072.\n",
      "epoch: 90, batch: 47, loss: 0.5521345138549805.\n",
      "epoch: 90, batch: 48, loss: 0.5583948493003845.\n",
      "epoch: 90, batch: 49, loss: 0.551908016204834.\n",
      "epoch: 90, batch: 50, loss: 0.5527904033660889.\n",
      "epoch: 90, batch: 51, loss: 0.5726279020309448.\n",
      "epoch: 90, batch: 52, loss: 0.5520501732826233.\n",
      "epoch: 91, batch: 0, loss: 0.5533541440963745.\n",
      "epoch: 91, batch: 1, loss: 0.5556857585906982.\n",
      "epoch: 91, batch: 2, loss: 0.5517008304595947.\n",
      "epoch: 91, batch: 3, loss: 0.7281008958816528.\n",
      "epoch: 91, batch: 4, loss: 0.7113265991210938.\n",
      "epoch: 91, batch: 5, loss: 0.5528647899627686.\n",
      "epoch: 91, batch: 6, loss: 0.5522499680519104.\n",
      "epoch: 91, batch: 7, loss: 0.5555614233016968.\n",
      "epoch: 91, batch: 8, loss: 0.6373002529144287.\n",
      "epoch: 91, batch: 9, loss: 0.5518175363540649.\n",
      "epoch: 91, batch: 10, loss: 0.5524390935897827.\n",
      "epoch: 91, batch: 11, loss: 0.5519525408744812.\n",
      "epoch: 91, batch: 12, loss: 0.6458923816680908.\n",
      "epoch: 91, batch: 13, loss: 0.8620353937149048.\n",
      "epoch: 91, batch: 14, loss: 0.5518711805343628.\n",
      "epoch: 91, batch: 15, loss: 0.5558823347091675.\n",
      "epoch: 91, batch: 16, loss: 0.5536803007125854.\n",
      "epoch: 91, batch: 17, loss: 0.609948992729187.\n",
      "epoch: 91, batch: 18, loss: 0.5518410205841064.\n",
      "epoch: 91, batch: 19, loss: 0.6500509977340698.\n",
      "epoch: 91, batch: 20, loss: 0.5717982053756714.\n",
      "epoch: 91, batch: 21, loss: 0.5536389350891113.\n",
      "epoch: 91, batch: 22, loss: 0.5534656047821045.\n",
      "epoch: 91, batch: 23, loss: 0.5526134967803955.\n",
      "epoch: 91, batch: 24, loss: 0.5519264340400696.\n",
      "epoch: 91, batch: 25, loss: 0.55171799659729.\n",
      "epoch: 91, batch: 26, loss: 0.5526541471481323.\n",
      "epoch: 91, batch: 27, loss: 0.5569443702697754.\n",
      "epoch: 91, batch: 28, loss: 0.5528851747512817.\n",
      "epoch: 91, batch: 29, loss: 0.5548198223114014.\n",
      "epoch: 91, batch: 30, loss: 0.552003800868988.\n",
      "epoch: 91, batch: 31, loss: 0.5550092458724976.\n",
      "epoch: 91, batch: 32, loss: 0.5558128952980042.\n",
      "epoch: 91, batch: 33, loss: 0.5525987148284912.\n",
      "epoch: 91, batch: 34, loss: 0.5519956350326538.\n",
      "epoch: 91, batch: 35, loss: 0.5543003082275391.\n",
      "epoch: 91, batch: 36, loss: 0.5536901950836182.\n",
      "epoch: 91, batch: 37, loss: 0.6119289994239807.\n",
      "epoch: 91, batch: 38, loss: 0.5573216676712036.\n",
      "epoch: 91, batch: 39, loss: 0.5816558003425598.\n",
      "epoch: 91, batch: 40, loss: 0.5525351762771606.\n",
      "epoch: 91, batch: 41, loss: 0.5627835392951965.\n",
      "epoch: 91, batch: 42, loss: 0.5736724138259888.\n",
      "epoch: 91, batch: 43, loss: 0.5518701672554016.\n",
      "epoch: 91, batch: 44, loss: 0.5552070140838623.\n",
      "epoch: 91, batch: 45, loss: 0.5560424327850342.\n",
      "epoch: 91, batch: 46, loss: 0.8214715719223022.\n",
      "epoch: 91, batch: 47, loss: 0.5517705678939819.\n",
      "epoch: 91, batch: 48, loss: 0.5970654487609863.\n",
      "epoch: 91, batch: 49, loss: 0.5522115230560303.\n",
      "epoch: 91, batch: 50, loss: 0.5536572933197021.\n",
      "epoch: 91, batch: 51, loss: 0.5563914775848389.\n",
      "epoch: 91, batch: 52, loss: 0.5515794157981873.\n",
      "epoch: 92, batch: 0, loss: 0.5521594285964966.\n",
      "epoch: 92, batch: 1, loss: 0.5589832067489624.\n",
      "epoch: 92, batch: 2, loss: 0.5538308620452881.\n",
      "epoch: 92, batch: 3, loss: 0.5515633821487427.\n",
      "epoch: 92, batch: 4, loss: 0.5521383285522461.\n",
      "epoch: 92, batch: 5, loss: 0.5974938869476318.\n",
      "epoch: 92, batch: 6, loss: 0.5518347024917603.\n",
      "epoch: 92, batch: 7, loss: 0.6187838912010193.\n",
      "epoch: 92, batch: 8, loss: 0.5570745468139648.\n",
      "epoch: 92, batch: 9, loss: 0.5596290230751038.\n",
      "epoch: 92, batch: 10, loss: 0.6553932428359985.\n",
      "epoch: 92, batch: 11, loss: 0.5516067743301392.\n",
      "epoch: 92, batch: 12, loss: 0.5519027709960938.\n",
      "epoch: 92, batch: 13, loss: 0.5518075227737427.\n",
      "epoch: 92, batch: 14, loss: 0.5688838958740234.\n",
      "epoch: 92, batch: 15, loss: 0.5560950636863708.\n",
      "epoch: 92, batch: 16, loss: 0.7177647352218628.\n",
      "epoch: 92, batch: 17, loss: 0.6596859693527222.\n",
      "epoch: 92, batch: 18, loss: 0.7045495510101318.\n",
      "epoch: 92, batch: 19, loss: 0.5738149881362915.\n",
      "epoch: 92, batch: 20, loss: 0.720331609249115.\n",
      "epoch: 92, batch: 21, loss: 0.5701802968978882.\n",
      "epoch: 92, batch: 22, loss: 0.5537368655204773.\n",
      "epoch: 92, batch: 23, loss: 0.5518707036972046.\n",
      "epoch: 92, batch: 24, loss: 0.5523644685745239.\n",
      "epoch: 92, batch: 25, loss: 0.6635522842407227.\n",
      "epoch: 92, batch: 26, loss: 0.5518642663955688.\n",
      "epoch: 92, batch: 27, loss: 0.552839994430542.\n",
      "epoch: 92, batch: 28, loss: 0.5518404245376587.\n",
      "epoch: 92, batch: 29, loss: 0.5720045566558838.\n",
      "epoch: 92, batch: 30, loss: 0.5537519454956055.\n",
      "epoch: 92, batch: 31, loss: 0.8299709558486938.\n",
      "epoch: 92, batch: 32, loss: 0.5545459985733032.\n",
      "epoch: 92, batch: 33, loss: 0.5528546571731567.\n",
      "epoch: 92, batch: 34, loss: 0.5517576932907104.\n",
      "epoch: 92, batch: 35, loss: 0.5550054311752319.\n",
      "epoch: 92, batch: 36, loss: 0.5546011924743652.\n",
      "epoch: 92, batch: 37, loss: 0.5517550110816956.\n",
      "epoch: 92, batch: 38, loss: 0.5525545477867126.\n",
      "epoch: 92, batch: 39, loss: 0.5538655519485474.\n",
      "epoch: 92, batch: 40, loss: 0.554296612739563.\n",
      "epoch: 92, batch: 41, loss: 0.5525509119033813.\n",
      "epoch: 92, batch: 42, loss: 0.5518914461135864.\n",
      "epoch: 92, batch: 43, loss: 0.5563197731971741.\n",
      "epoch: 92, batch: 44, loss: 0.5552603006362915.\n",
      "epoch: 92, batch: 45, loss: 0.5686548948287964.\n",
      "epoch: 92, batch: 46, loss: 0.5521864891052246.\n",
      "epoch: 92, batch: 47, loss: 0.556212306022644.\n",
      "epoch: 92, batch: 48, loss: 0.5518594980239868.\n",
      "epoch: 92, batch: 49, loss: 0.5553399324417114.\n",
      "epoch: 92, batch: 50, loss: 0.5557571649551392.\n",
      "epoch: 92, batch: 51, loss: 0.557161271572113.\n",
      "epoch: 92, batch: 52, loss: 0.5523020029067993.\n",
      "epoch: 93, batch: 0, loss: 0.5550102591514587.\n",
      "epoch: 93, batch: 1, loss: 0.5543608665466309.\n",
      "epoch: 93, batch: 2, loss: 0.5518912076950073.\n",
      "epoch: 93, batch: 3, loss: 0.8211863040924072.\n",
      "epoch: 93, batch: 4, loss: 0.5523241758346558.\n",
      "epoch: 93, batch: 5, loss: 0.8453400135040283.\n",
      "epoch: 93, batch: 6, loss: 0.5522802472114563.\n",
      "epoch: 93, batch: 7, loss: 0.5532500743865967.\n",
      "epoch: 93, batch: 8, loss: 0.5534764528274536.\n",
      "epoch: 93, batch: 9, loss: 0.5906334519386292.\n",
      "epoch: 93, batch: 10, loss: 0.5523484945297241.\n",
      "epoch: 93, batch: 11, loss: 0.5599843859672546.\n",
      "epoch: 93, batch: 12, loss: 0.640181839466095.\n",
      "epoch: 93, batch: 13, loss: 0.5518916249275208.\n",
      "epoch: 93, batch: 14, loss: 0.5584672093391418.\n",
      "epoch: 93, batch: 15, loss: 0.5520508289337158.\n",
      "epoch: 93, batch: 16, loss: 0.5527893304824829.\n",
      "epoch: 93, batch: 17, loss: 0.5516424179077148.\n",
      "epoch: 93, batch: 18, loss: 0.5519707202911377.\n",
      "epoch: 93, batch: 19, loss: 0.5539283752441406.\n",
      "epoch: 93, batch: 20, loss: 0.5661675333976746.\n",
      "epoch: 93, batch: 21, loss: 0.5520714521408081.\n",
      "epoch: 93, batch: 22, loss: 0.55181884765625.\n",
      "epoch: 93, batch: 23, loss: 0.5538597106933594.\n",
      "epoch: 93, batch: 24, loss: 0.5522475242614746.\n",
      "epoch: 93, batch: 25, loss: 0.5526809692382812.\n",
      "epoch: 93, batch: 26, loss: 0.5520108342170715.\n",
      "epoch: 93, batch: 27, loss: 0.5516475439071655.\n",
      "epoch: 93, batch: 28, loss: 0.5581434965133667.\n",
      "epoch: 93, batch: 29, loss: 0.5680698156356812.\n",
      "epoch: 93, batch: 30, loss: 0.5517612099647522.\n",
      "epoch: 93, batch: 31, loss: 0.5528753995895386.\n",
      "epoch: 93, batch: 32, loss: 0.5664292573928833.\n",
      "epoch: 93, batch: 33, loss: 0.627685546875.\n",
      "epoch: 93, batch: 34, loss: 0.5664070844650269.\n",
      "epoch: 93, batch: 35, loss: 0.5543901920318604.\n",
      "epoch: 93, batch: 36, loss: 0.5580236911773682.\n",
      "epoch: 93, batch: 37, loss: 0.56801438331604.\n",
      "epoch: 93, batch: 38, loss: 0.5544964075088501.\n",
      "epoch: 93, batch: 39, loss: 0.6778908371925354.\n",
      "epoch: 93, batch: 40, loss: 0.7491275668144226.\n",
      "epoch: 93, batch: 41, loss: 0.5538676381111145.\n",
      "epoch: 93, batch: 42, loss: 0.5534493923187256.\n",
      "epoch: 93, batch: 43, loss: 0.5518157482147217.\n",
      "epoch: 93, batch: 44, loss: 0.5517401099205017.\n",
      "epoch: 93, batch: 45, loss: 0.604853630065918.\n",
      "epoch: 93, batch: 46, loss: 0.5533052682876587.\n",
      "epoch: 93, batch: 47, loss: 0.5525901317596436.\n",
      "epoch: 93, batch: 48, loss: 0.7989321947097778.\n",
      "epoch: 93, batch: 49, loss: 0.5517563819885254.\n",
      "epoch: 93, batch: 50, loss: 0.5718053579330444.\n",
      "epoch: 93, batch: 51, loss: 0.5526115894317627.\n",
      "epoch: 93, batch: 52, loss: 0.5521283149719238.\n",
      "epoch: 94, batch: 0, loss: 0.5575579404830933.\n",
      "epoch: 94, batch: 1, loss: 0.561812162399292.\n",
      "epoch: 94, batch: 2, loss: 0.5980727076530457.\n",
      "epoch: 94, batch: 3, loss: 0.5527924299240112.\n",
      "epoch: 94, batch: 4, loss: 0.5776503682136536.\n",
      "epoch: 94, batch: 5, loss: 0.55764240026474.\n",
      "epoch: 94, batch: 6, loss: 0.55584317445755.\n",
      "epoch: 94, batch: 7, loss: 0.8079686164855957.\n",
      "epoch: 94, batch: 8, loss: 1.0378235578536987.\n",
      "epoch: 94, batch: 9, loss: 0.5732485055923462.\n",
      "epoch: 94, batch: 10, loss: 0.5518798828125.\n",
      "epoch: 94, batch: 11, loss: 0.5558357238769531.\n",
      "epoch: 94, batch: 12, loss: 0.5524863600730896.\n",
      "epoch: 94, batch: 13, loss: 0.7879521250724792.\n",
      "epoch: 94, batch: 14, loss: 0.551683783531189.\n",
      "epoch: 94, batch: 15, loss: 0.5520889759063721.\n",
      "epoch: 94, batch: 16, loss: 0.5516824722290039.\n",
      "epoch: 94, batch: 17, loss: 0.5520288944244385.\n",
      "epoch: 94, batch: 18, loss: 0.552209734916687.\n",
      "epoch: 94, batch: 19, loss: 0.5528606176376343.\n",
      "epoch: 94, batch: 20, loss: 0.5669587850570679.\n",
      "epoch: 94, batch: 21, loss: 0.5528160333633423.\n",
      "epoch: 94, batch: 22, loss: 0.5519962906837463.\n",
      "epoch: 94, batch: 23, loss: 0.5540292263031006.\n",
      "epoch: 94, batch: 24, loss: 0.5522774457931519.\n",
      "epoch: 94, batch: 25, loss: 0.551619291305542.\n",
      "epoch: 94, batch: 26, loss: 0.5555436015129089.\n",
      "epoch: 94, batch: 27, loss: 0.5517998933792114.\n",
      "epoch: 94, batch: 28, loss: 0.598635196685791.\n",
      "epoch: 94, batch: 29, loss: 0.5519905090332031.\n",
      "epoch: 94, batch: 30, loss: 0.552087128162384.\n",
      "epoch: 94, batch: 31, loss: 0.5517855286598206.\n",
      "epoch: 94, batch: 32, loss: 0.5534116625785828.\n",
      "epoch: 94, batch: 33, loss: 0.9591938853263855.\n",
      "epoch: 94, batch: 34, loss: 0.5523600578308105.\n",
      "epoch: 94, batch: 35, loss: 0.5521062016487122.\n",
      "epoch: 94, batch: 36, loss: 0.551838755607605.\n",
      "epoch: 94, batch: 37, loss: 0.7553194165229797.\n",
      "epoch: 94, batch: 38, loss: 0.5528067350387573.\n",
      "epoch: 94, batch: 39, loss: 0.5568464994430542.\n",
      "epoch: 94, batch: 40, loss: 0.5554180145263672.\n",
      "epoch: 94, batch: 41, loss: 0.6570981740951538.\n",
      "epoch: 94, batch: 42, loss: 0.5530551671981812.\n",
      "epoch: 94, batch: 43, loss: 0.5534583926200867.\n",
      "epoch: 94, batch: 44, loss: 0.5542503595352173.\n",
      "epoch: 94, batch: 45, loss: 0.5597621202468872.\n",
      "epoch: 94, batch: 46, loss: 0.5518678426742554.\n",
      "epoch: 94, batch: 47, loss: 0.551906943321228.\n",
      "epoch: 94, batch: 48, loss: 0.5583254098892212.\n",
      "epoch: 94, batch: 49, loss: 0.551721453666687.\n",
      "epoch: 94, batch: 50, loss: 0.5545703768730164.\n",
      "epoch: 94, batch: 51, loss: 0.5583724975585938.\n",
      "epoch: 94, batch: 52, loss: 0.552469789981842.\n",
      "epoch: 95, batch: 0, loss: 0.7938427925109863.\n",
      "epoch: 95, batch: 1, loss: 0.5581455230712891.\n",
      "epoch: 95, batch: 2, loss: 0.5524548292160034.\n",
      "epoch: 95, batch: 3, loss: 0.5552312135696411.\n",
      "epoch: 95, batch: 4, loss: 0.5518351793289185.\n",
      "epoch: 95, batch: 5, loss: 0.5533432960510254.\n",
      "epoch: 95, batch: 6, loss: 0.5522547960281372.\n",
      "epoch: 95, batch: 7, loss: 0.6183561086654663.\n",
      "epoch: 95, batch: 8, loss: 0.5527843236923218.\n",
      "epoch: 95, batch: 9, loss: 0.5518525838851929.\n",
      "epoch: 95, batch: 10, loss: 0.5586837530136108.\n",
      "epoch: 95, batch: 11, loss: 0.5523764491081238.\n",
      "epoch: 95, batch: 12, loss: 0.5538226962089539.\n",
      "epoch: 95, batch: 13, loss: 0.5935013294219971.\n",
      "epoch: 95, batch: 14, loss: 0.553242027759552.\n",
      "epoch: 95, batch: 15, loss: 0.5874365568161011.\n",
      "epoch: 95, batch: 16, loss: 0.5629671812057495.\n",
      "epoch: 95, batch: 17, loss: 0.5537335276603699.\n",
      "epoch: 95, batch: 18, loss: 0.5521458387374878.\n",
      "epoch: 95, batch: 19, loss: 0.5527833700180054.\n",
      "epoch: 95, batch: 20, loss: 0.5550499558448792.\n",
      "epoch: 95, batch: 21, loss: 0.5528564453125.\n",
      "epoch: 95, batch: 22, loss: 0.5524660348892212.\n",
      "epoch: 95, batch: 23, loss: 0.6407619714736938.\n",
      "epoch: 95, batch: 24, loss: 0.6860423684120178.\n",
      "epoch: 95, batch: 25, loss: 0.8086298704147339.\n",
      "epoch: 95, batch: 26, loss: 0.5530760288238525.\n",
      "epoch: 95, batch: 27, loss: 0.5517899990081787.\n",
      "epoch: 95, batch: 28, loss: 0.5701005458831787.\n",
      "epoch: 95, batch: 29, loss: 0.5554406642913818.\n",
      "epoch: 95, batch: 30, loss: 0.6396135091781616.\n",
      "epoch: 95, batch: 31, loss: 0.5548725128173828.\n",
      "epoch: 95, batch: 32, loss: 0.5541974306106567.\n",
      "epoch: 95, batch: 33, loss: 0.6156195998191833.\n",
      "epoch: 95, batch: 34, loss: 0.6073448061943054.\n",
      "epoch: 95, batch: 35, loss: 0.5518815517425537.\n",
      "epoch: 95, batch: 36, loss: 0.56333988904953.\n",
      "epoch: 95, batch: 37, loss: 0.5524196028709412.\n",
      "epoch: 95, batch: 38, loss: 0.7769254446029663.\n",
      "epoch: 95, batch: 39, loss: 0.5557923913002014.\n",
      "epoch: 95, batch: 40, loss: 0.5520519018173218.\n",
      "epoch: 95, batch: 41, loss: 0.5582777857780457.\n",
      "epoch: 95, batch: 42, loss: 0.6543422937393188.\n",
      "epoch: 95, batch: 43, loss: 0.5517950057983398.\n",
      "epoch: 95, batch: 44, loss: 0.5518678426742554.\n",
      "epoch: 95, batch: 45, loss: 0.5516122579574585.\n",
      "epoch: 95, batch: 46, loss: 0.5620386004447937.\n",
      "epoch: 95, batch: 47, loss: 0.552332878112793.\n",
      "epoch: 95, batch: 48, loss: 0.5515749454498291.\n",
      "epoch: 95, batch: 49, loss: 0.5629273653030396.\n",
      "epoch: 95, batch: 50, loss: 0.5520028471946716.\n",
      "epoch: 95, batch: 51, loss: 0.5522160530090332.\n",
      "epoch: 95, batch: 52, loss: 0.5536422729492188.\n",
      "epoch: 96, batch: 0, loss: 0.5518937706947327.\n",
      "epoch: 96, batch: 1, loss: 0.7655866742134094.\n",
      "epoch: 96, batch: 2, loss: 0.900304913520813.\n",
      "epoch: 96, batch: 3, loss: 0.557188093662262.\n",
      "epoch: 96, batch: 4, loss: 0.551837682723999.\n",
      "epoch: 96, batch: 5, loss: 0.5530214905738831.\n",
      "epoch: 96, batch: 6, loss: 0.5519490242004395.\n",
      "epoch: 96, batch: 7, loss: 0.5518089532852173.\n",
      "epoch: 96, batch: 8, loss: 0.5524766445159912.\n",
      "epoch: 96, batch: 9, loss: 0.5518031120300293.\n",
      "epoch: 96, batch: 10, loss: 0.6175854206085205.\n",
      "epoch: 96, batch: 11, loss: 0.5518456697463989.\n",
      "epoch: 96, batch: 12, loss: 0.5519998073577881.\n",
      "epoch: 96, batch: 13, loss: 0.5538107752799988.\n",
      "epoch: 96, batch: 14, loss: 0.6853674650192261.\n",
      "epoch: 96, batch: 15, loss: 0.5560508966445923.\n",
      "epoch: 96, batch: 16, loss: 0.5731995701789856.\n",
      "epoch: 96, batch: 17, loss: 0.5574716329574585.\n",
      "epoch: 96, batch: 18, loss: 0.5519276857376099.\n",
      "epoch: 96, batch: 19, loss: 0.5535441637039185.\n",
      "epoch: 96, batch: 20, loss: 0.5554369688034058.\n",
      "epoch: 96, batch: 21, loss: 0.5708492994308472.\n",
      "epoch: 96, batch: 22, loss: 0.5515519976615906.\n",
      "epoch: 96, batch: 23, loss: 0.5518542528152466.\n",
      "epoch: 96, batch: 24, loss: 0.6817630529403687.\n",
      "epoch: 96, batch: 25, loss: 0.5526289939880371.\n",
      "epoch: 96, batch: 26, loss: 0.6834977865219116.\n",
      "epoch: 96, batch: 27, loss: 0.6443647742271423.\n",
      "epoch: 96, batch: 28, loss: 0.5521054267883301.\n",
      "epoch: 96, batch: 29, loss: 0.5519681572914124.\n",
      "epoch: 96, batch: 30, loss: 0.6057958602905273.\n",
      "epoch: 96, batch: 31, loss: 0.5559356212615967.\n",
      "epoch: 96, batch: 32, loss: 0.5519379377365112.\n",
      "epoch: 96, batch: 33, loss: 0.5526422262191772.\n",
      "epoch: 96, batch: 34, loss: 0.5532258749008179.\n",
      "epoch: 96, batch: 35, loss: 0.7716858983039856.\n",
      "epoch: 96, batch: 36, loss: 0.5587761402130127.\n",
      "epoch: 96, batch: 37, loss: 0.5531636476516724.\n",
      "epoch: 96, batch: 38, loss: 0.5546268224716187.\n",
      "epoch: 96, batch: 39, loss: 0.5550501346588135.\n",
      "epoch: 96, batch: 40, loss: 0.5526676177978516.\n",
      "epoch: 96, batch: 41, loss: 0.5521012544631958.\n",
      "epoch: 96, batch: 42, loss: 0.5518530011177063.\n",
      "epoch: 96, batch: 43, loss: 0.5515879392623901.\n",
      "epoch: 96, batch: 44, loss: 0.5593301057815552.\n",
      "epoch: 96, batch: 45, loss: 0.6381840705871582.\n",
      "epoch: 96, batch: 46, loss: 0.5545667409896851.\n",
      "epoch: 96, batch: 47, loss: 0.5651395320892334.\n",
      "epoch: 96, batch: 48, loss: 0.5518456101417542.\n",
      "epoch: 96, batch: 49, loss: 0.5600879788398743.\n",
      "epoch: 96, batch: 50, loss: 0.5524201393127441.\n",
      "epoch: 96, batch: 51, loss: 0.5536092519760132.\n",
      "epoch: 96, batch: 52, loss: 0.5530656576156616.\n",
      "epoch: 97, batch: 0, loss: 0.5534114837646484.\n",
      "epoch: 97, batch: 1, loss: 0.6403813362121582.\n",
      "epoch: 97, batch: 2, loss: 0.5735160708427429.\n",
      "epoch: 97, batch: 3, loss: 0.5525995492935181.\n",
      "epoch: 97, batch: 4, loss: 0.5525262951850891.\n",
      "epoch: 97, batch: 5, loss: 0.5522228479385376.\n",
      "epoch: 97, batch: 6, loss: 0.5517255067825317.\n",
      "epoch: 97, batch: 7, loss: 0.554461658000946.\n",
      "epoch: 97, batch: 8, loss: 0.5517567992210388.\n",
      "epoch: 97, batch: 9, loss: 0.5517082214355469.\n",
      "epoch: 97, batch: 10, loss: 0.5519237518310547.\n",
      "epoch: 97, batch: 11, loss: 0.551750898361206.\n",
      "epoch: 97, batch: 12, loss: 0.7714271545410156.\n",
      "epoch: 97, batch: 13, loss: 0.5528457164764404.\n",
      "epoch: 97, batch: 14, loss: 0.5561451315879822.\n",
      "epoch: 97, batch: 15, loss: 0.5530718564987183.\n",
      "epoch: 97, batch: 16, loss: 0.5523420572280884.\n",
      "epoch: 97, batch: 17, loss: 0.5526236295700073.\n",
      "epoch: 97, batch: 18, loss: 0.5518770217895508.\n",
      "epoch: 97, batch: 19, loss: 0.5540307760238647.\n",
      "epoch: 97, batch: 20, loss: 0.5515960454940796.\n",
      "epoch: 97, batch: 21, loss: 0.5544561147689819.\n",
      "epoch: 97, batch: 22, loss: 0.5516737699508667.\n",
      "epoch: 97, batch: 23, loss: 0.5517948865890503.\n",
      "epoch: 97, batch: 24, loss: 0.6268402338027954.\n",
      "epoch: 97, batch: 25, loss: 0.5518264770507812.\n",
      "epoch: 97, batch: 26, loss: 0.5575871467590332.\n",
      "epoch: 97, batch: 27, loss: 0.5993256568908691.\n",
      "epoch: 97, batch: 28, loss: 0.7203191518783569.\n",
      "epoch: 97, batch: 29, loss: 0.580094575881958.\n",
      "epoch: 97, batch: 30, loss: 0.5517858266830444.\n",
      "epoch: 97, batch: 31, loss: 0.5535993576049805.\n",
      "epoch: 97, batch: 32, loss: 0.5543782711029053.\n",
      "epoch: 97, batch: 33, loss: 0.5569705963134766.\n",
      "epoch: 97, batch: 34, loss: 0.5542300939559937.\n",
      "epoch: 97, batch: 35, loss: 0.6337073445320129.\n",
      "epoch: 97, batch: 36, loss: 0.8677823543548584.\n",
      "epoch: 97, batch: 37, loss: 0.5519360303878784.\n",
      "epoch: 97, batch: 38, loss: 0.5650008320808411.\n",
      "epoch: 97, batch: 39, loss: 0.5704352855682373.\n",
      "epoch: 97, batch: 40, loss: 0.9067986011505127.\n",
      "epoch: 97, batch: 41, loss: 0.5661162734031677.\n",
      "epoch: 97, batch: 42, loss: 0.553939700126648.\n",
      "epoch: 97, batch: 43, loss: 0.653282880783081.\n",
      "epoch: 97, batch: 44, loss: 0.5517714023590088.\n",
      "epoch: 97, batch: 45, loss: 0.5523929595947266.\n",
      "epoch: 97, batch: 46, loss: 0.553500771522522.\n",
      "epoch: 97, batch: 47, loss: 0.5562386512756348.\n",
      "epoch: 97, batch: 48, loss: 0.5634369850158691.\n",
      "epoch: 97, batch: 49, loss: 0.5639432668685913.\n",
      "epoch: 97, batch: 50, loss: 0.67070472240448.\n",
      "epoch: 97, batch: 51, loss: 0.5535549521446228.\n",
      "epoch: 97, batch: 52, loss: 0.5524834394454956.\n",
      "epoch: 98, batch: 0, loss: 0.6990560293197632.\n",
      "epoch: 98, batch: 1, loss: 0.5527663230895996.\n",
      "epoch: 98, batch: 2, loss: 0.552503228187561.\n",
      "epoch: 98, batch: 3, loss: 0.5524598956108093.\n",
      "epoch: 98, batch: 4, loss: 0.8290418386459351.\n",
      "epoch: 98, batch: 5, loss: 0.5534652471542358.\n",
      "epoch: 98, batch: 6, loss: 0.552519679069519.\n",
      "epoch: 98, batch: 7, loss: 0.6632434129714966.\n",
      "epoch: 98, batch: 8, loss: 0.5529470443725586.\n",
      "epoch: 98, batch: 9, loss: 0.5522881746292114.\n",
      "epoch: 98, batch: 10, loss: 0.5519551038742065.\n",
      "epoch: 98, batch: 11, loss: 0.5608371496200562.\n",
      "epoch: 98, batch: 12, loss: 0.5521807670593262.\n",
      "epoch: 98, batch: 13, loss: 0.5518985390663147.\n",
      "epoch: 98, batch: 14, loss: 0.5518175363540649.\n",
      "epoch: 98, batch: 15, loss: 0.5537313222885132.\n",
      "epoch: 98, batch: 16, loss: 0.5560399293899536.\n",
      "epoch: 98, batch: 17, loss: 0.5522986054420471.\n",
      "epoch: 98, batch: 18, loss: 0.5519260168075562.\n",
      "epoch: 98, batch: 19, loss: 0.5524582862854004.\n",
      "epoch: 98, batch: 20, loss: 0.561603307723999.\n",
      "epoch: 98, batch: 21, loss: 0.5560123920440674.\n",
      "epoch: 98, batch: 22, loss: 0.5523688793182373.\n",
      "epoch: 98, batch: 23, loss: 0.602213978767395.\n",
      "epoch: 98, batch: 24, loss: 0.5517354607582092.\n",
      "epoch: 98, batch: 25, loss: 0.5519415736198425.\n",
      "epoch: 98, batch: 26, loss: 0.5684220194816589.\n",
      "epoch: 98, batch: 27, loss: 0.6088705658912659.\n",
      "epoch: 98, batch: 28, loss: 0.5553241968154907.\n",
      "epoch: 98, batch: 29, loss: 0.6214598417282104.\n",
      "epoch: 98, batch: 30, loss: 0.5550835728645325.\n",
      "epoch: 98, batch: 31, loss: 0.5529894828796387.\n",
      "epoch: 98, batch: 32, loss: 0.5516105890274048.\n",
      "epoch: 98, batch: 33, loss: 0.5584237575531006.\n",
      "epoch: 98, batch: 34, loss: 0.5516128540039062.\n",
      "epoch: 98, batch: 35, loss: 0.555609941482544.\n",
      "epoch: 98, batch: 36, loss: 0.5662440657615662.\n",
      "epoch: 98, batch: 37, loss: 0.5523505210876465.\n",
      "epoch: 98, batch: 38, loss: 0.8835738897323608.\n",
      "epoch: 98, batch: 39, loss: 0.5529749989509583.\n",
      "epoch: 98, batch: 40, loss: 0.6562864780426025.\n",
      "epoch: 98, batch: 41, loss: 0.5518140196800232.\n",
      "epoch: 98, batch: 42, loss: 0.5787806510925293.\n",
      "epoch: 98, batch: 43, loss: 0.6721682548522949.\n",
      "epoch: 98, batch: 44, loss: 0.5528639554977417.\n",
      "epoch: 98, batch: 45, loss: 0.5517987012863159.\n",
      "epoch: 98, batch: 46, loss: 0.5529301166534424.\n",
      "epoch: 98, batch: 47, loss: 0.6370864510536194.\n",
      "epoch: 98, batch: 48, loss: 0.5517129898071289.\n",
      "epoch: 98, batch: 49, loss: 0.555199146270752.\n",
      "epoch: 98, batch: 50, loss: 0.5519058704376221.\n",
      "epoch: 98, batch: 51, loss: 0.5706861615180969.\n",
      "epoch: 98, batch: 52, loss: 0.5516695976257324.\n",
      "epoch: 99, batch: 0, loss: 0.5545029640197754.\n",
      "epoch: 99, batch: 1, loss: 0.553009569644928.\n",
      "epoch: 99, batch: 2, loss: 0.5523089170455933.\n",
      "epoch: 99, batch: 3, loss: 0.5518785119056702.\n",
      "epoch: 99, batch: 4, loss: 0.5516207218170166.\n",
      "epoch: 99, batch: 5, loss: 0.554609477519989.\n",
      "epoch: 99, batch: 6, loss: 0.5520713329315186.\n",
      "epoch: 99, batch: 7, loss: 0.5516852736473083.\n",
      "epoch: 99, batch: 8, loss: 0.5517375469207764.\n",
      "epoch: 99, batch: 9, loss: 0.5517147779464722.\n",
      "epoch: 99, batch: 10, loss: 0.5665843486785889.\n",
      "epoch: 99, batch: 11, loss: 0.5522729158401489.\n",
      "epoch: 99, batch: 12, loss: 0.5525246858596802.\n",
      "epoch: 99, batch: 13, loss: 0.5518972873687744.\n",
      "epoch: 99, batch: 14, loss: 0.5545386075973511.\n",
      "epoch: 99, batch: 15, loss: 0.5932063460350037.\n",
      "epoch: 99, batch: 16, loss: 0.6653441190719604.\n",
      "epoch: 99, batch: 17, loss: 0.5542550086975098.\n",
      "epoch: 99, batch: 18, loss: 0.5631583333015442.\n",
      "epoch: 99, batch: 19, loss: 0.5768060684204102.\n",
      "epoch: 99, batch: 20, loss: 0.7260737419128418.\n",
      "epoch: 99, batch: 21, loss: 0.5546327829360962.\n",
      "epoch: 99, batch: 22, loss: 0.5517332553863525.\n",
      "epoch: 99, batch: 23, loss: 0.5544315576553345.\n",
      "epoch: 99, batch: 24, loss: 0.5526002645492554.\n",
      "epoch: 99, batch: 25, loss: 0.554686427116394.\n",
      "epoch: 99, batch: 26, loss: 0.5525758862495422.\n",
      "epoch: 99, batch: 27, loss: 0.7088053226470947.\n",
      "epoch: 99, batch: 28, loss: 0.5700804591178894.\n",
      "epoch: 99, batch: 29, loss: 0.5541965961456299.\n",
      "epoch: 99, batch: 30, loss: 0.5520170331001282.\n",
      "epoch: 99, batch: 31, loss: 0.5516164898872375.\n",
      "epoch: 99, batch: 32, loss: 0.5651096105575562.\n",
      "epoch: 99, batch: 33, loss: 0.5518922209739685.\n",
      "epoch: 99, batch: 34, loss: 0.6031004190444946.\n",
      "epoch: 99, batch: 35, loss: 0.5526284575462341.\n",
      "epoch: 99, batch: 36, loss: 0.5519101023674011.\n",
      "epoch: 99, batch: 37, loss: 0.5516725778579712.\n",
      "epoch: 99, batch: 38, loss: 0.5574897527694702.\n",
      "epoch: 99, batch: 39, loss: 0.5521478652954102.\n",
      "epoch: 99, batch: 40, loss: 0.6860325336456299.\n",
      "epoch: 99, batch: 41, loss: 0.5800544023513794.\n",
      "epoch: 99, batch: 42, loss: 0.5518521666526794.\n",
      "epoch: 99, batch: 43, loss: 0.5540090799331665.\n",
      "epoch: 99, batch: 44, loss: 0.5541020035743713.\n",
      "epoch: 99, batch: 45, loss: 0.5526480674743652.\n",
      "epoch: 99, batch: 46, loss: 0.8204934597015381.\n",
      "epoch: 99, batch: 47, loss: 0.5520709156990051.\n",
      "epoch: 99, batch: 48, loss: 0.9562340378761292.\n",
      "epoch: 99, batch: 49, loss: 0.5516738891601562.\n",
      "epoch: 99, batch: 50, loss: 0.5525768995285034.\n",
      "epoch: 99, batch: 51, loss: 0.551826000213623.\n",
      "epoch: 99, batch: 52, loss: 0.6874426603317261.\n"
     ]
    }
   ],
   "source": [
    "train_dl, test_dl = prepare_data()\n",
    "print(len(train_dl), len(test_dl))\n",
    "model = MLP(4, 5, 6, 3)\n",
    "loss = CrossEntropyLoss()\n",
    "trainer = Adam(model.parameters())\n",
    "train_model(train_dl, model, loss, trainer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\1\\Desktop\\Study\\PyStudy\\Coding\\DeepLearning\\MLP.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/1/Desktop/Study/PyStudy/Coding/DeepLearning/MLP.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m acc \u001b[39m=\u001b[39m evaluate_model(test_dl, model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/1/Desktop/Study/PyStudy/Coding/DeepLearning/MLP.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m acc\n",
      "\u001b[1;32mc:\\Users\\1\\Desktop\\Study\\PyStudy\\Coding\\DeepLearning\\MLP.ipynb Cell 10\u001b[0m in \u001b[0;36mevaluate_model\u001b[1;34m(test_dl, model)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/1/Desktop/Study/PyStudy/Coding/DeepLearning/MLP.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_hat \u001b[39m=\u001b[39m argmax(y_hat)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/1/Desktop/Study/PyStudy/Coding/DeepLearning/MLP.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mreshape((\u001b[39mlen\u001b[39m(y), \u001b[39m1\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/1/Desktop/Study/PyStudy/Coding/DeepLearning/MLP.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39mreshape((\u001b[39mlen\u001b[39;49m(y_hat), \u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/1/Desktop/Study/PyStudy/Coding/DeepLearning/MLP.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m predictions\u001b[39m.\u001b[39mappend(y_hat)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/1/Desktop/Study/PyStudy/Coding/DeepLearning/MLP.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m actuals\u001b[39m.\u001b[39mappend(y)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "acc = evaluate_model(test_dl, model)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden1, n_hidden2, n_output):\n",
    "        super().__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_inputs, n_hidden1)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        self.hidden3 = torch.nn.Linear(n_hidden2, n_output)\n",
    "        self.act3 = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "# 初始化MLP\n",
    "n_inputs = 4   # 假设输入特征数量为4\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 5\n",
    "n_output = 3\n",
    "model = MLP(n_inputs, n_hidden1, n_hidden2, n_output)\n",
    "\n",
    "# # 生成测试数据\n",
    "# test_input = torch.randn(1, n_inputs)  # 假设我们只生成一个数据点来测试\n",
    "\n",
    "# # 通过模型传递测试数据\n",
    "# output = model(test_input)\n",
    "# print(\"Output:\", output.shape)\n",
    "\n",
    "# x, y = next(iter(enumerate(train_dl)))\n",
    "dataset = IRISDataSet()\n",
    "train, test = dataset.get_splits()\n",
    "# train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "# test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "# idx, (x, y) = next(enumerate(train_dl))\n",
    "model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3248, -0.1896, -0.0985],\n",
      "        [-0.3486, -0.1853, -0.1203],\n",
      "        [-0.2957, -0.2077, -0.0680],\n",
      "        [-0.2322, -0.1958, -0.0174],\n",
      "        [-0.3106, -0.2028, -0.0866],\n",
      "        [-0.2308, -0.2088,  0.0064],\n",
      "        [-0.2473, -0.2335,  0.0477],\n",
      "        [-0.2898, -0.2113, -0.0525],\n",
      "        [-0.2712, -0.2105, -0.0379],\n",
      "        [-0.3516, -0.1929, -0.1244],\n",
      "        [-0.2299, -0.1767, -0.0507],\n",
      "        [-0.2962, -0.2039, -0.0653],\n",
      "        [-0.2307, -0.2030, -0.0076],\n",
      "        [-0.3206, -0.2021, -0.0951],\n",
      "        [-0.2353, -0.2026,  0.0161],\n",
      "        [-0.3477, -0.1798, -0.1199]], grad_fn=<AddmmBackward0>) tensor([1, 1, 1, 0, 1, 0, 0, 2, 2, 2, 0, 1, 0, 1, 0, 1])\n",
      "Epoch 1, Loss: 1.0230727195739746\n",
      "Epoch 2, Loss: 0.8724119067192078\n",
      "Epoch 3, Loss: 0.5938277244567871\n",
      "Epoch 4, Loss: 0.6242708563804626\n",
      "Epoch 5, Loss: 0.3450513482093811\n",
      "Epoch 6, Loss: 0.36853355169296265\n",
      "Epoch 7, Loss: 0.22045762836933136\n",
      "Epoch 8, Loss: 0.04926508292555809\n",
      "Epoch 9, Loss: 0.21979288756847382\n",
      "Epoch 10, Loss: 0.09072853624820709\n",
      "Epoch 11, Loss: 0.21187245845794678\n",
      "Epoch 12, Loss: 0.18594522774219513\n",
      "Epoch 13, Loss: 0.33080604672431946\n",
      "Epoch 14, Loss: 0.23808009922504425\n",
      "Epoch 15, Loss: 0.04454762861132622\n",
      "Epoch 16, Loss: 0.061496373265981674\n",
      "Epoch 17, Loss: 0.05354919657111168\n",
      "Epoch 18, Loss: 0.13213858008384705\n",
      "Epoch 19, Loss: 0.008206169120967388\n",
      "Epoch 20, Loss: 0.04204858839511871\n",
      "Epoch 21, Loss: 0.01759333163499832\n",
      "Epoch 22, Loss: 0.22917383909225464\n",
      "Epoch 23, Loss: 0.3946303725242615\n",
      "Epoch 24, Loss: 0.0022459072060883045\n",
      "Epoch 25, Loss: 0.005864690523594618\n",
      "Epoch 26, Loss: 0.003105414565652609\n",
      "Epoch 27, Loss: 0.028069978579878807\n",
      "Epoch 28, Loss: 0.004164825193583965\n",
      "Epoch 29, Loss: 0.0017649043584242463\n",
      "Epoch 30, Loss: 0.04161309078335762\n",
      "Epoch 31, Loss: 0.19024932384490967\n",
      "Epoch 32, Loss: 0.04796423763036728\n",
      "Epoch 33, Loss: 0.0035569528117775917\n",
      "Epoch 34, Loss: 0.13039372861385345\n",
      "Epoch 35, Loss: 0.008844973519444466\n",
      "Epoch 36, Loss: 0.2436060756444931\n",
      "Epoch 37, Loss: 0.0018033288652077317\n",
      "Epoch 38, Loss: 0.0005312967114150524\n",
      "Epoch 39, Loss: 0.0657232478260994\n",
      "Epoch 40, Loss: 0.015690194442868233\n",
      "Epoch 41, Loss: 0.1504981368780136\n",
      "Epoch 42, Loss: 0.16972334682941437\n",
      "Epoch 43, Loss: 0.000706010963767767\n",
      "Epoch 44, Loss: 0.08043885976076126\n",
      "Epoch 45, Loss: 0.2553888261318207\n",
      "Epoch 46, Loss: 0.03723100200295448\n",
      "Epoch 47, Loss: 0.22768379747867584\n",
      "Epoch 48, Loss: 0.032166194170713425\n",
      "Epoch 49, Loss: 0.010598020628094673\n",
      "Epoch 50, Loss: 0.1284445971250534\n",
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 数据加载和预处理\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 标准化特征以帮助训练\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 转换为torch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)  # CrossEntropyLoss要求目标是长整型\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 定义 MLP 模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden1 = nn.Linear(4, 10)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(10, 10)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型、损失函数和优化器\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练模型\n",
    "def train_model(num_epochs):\n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            if i == 0:\n",
    "                print(output, target)\n",
    "                i += 1\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# 测试模型\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "\n",
    "# 运行训练和评估\n",
    "train_model(50)\n",
    "evaluate_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7679)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1, y2 = torch.tensor([0.2, 0.7, 0.1]), torch.tensor(1)\n",
    "criterion(y1, y2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
