{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单头自注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_k, dim_v):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self.linear_q = nn.Linear(dim_in, dim_k, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_in, dim_k, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_in, dim_v, bias=False)\n",
    "        self._norm_fact = 1 / sqrt(dim_k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x.shape: (batch, n, dim_in)\n",
    "        batch, n, dim_in = x.shape\n",
    "        assert dim_in == self.dim_in\n",
    "        q = self.linear_q(x)\n",
    "        k = self.linear_k(x)\n",
    "        v = self.linear_v(x)\n",
    "        dist = torch.bmm(q, k.transpose(1, 2)) * self._norm_fact\n",
    "        dist = torch.softmax(dist, dim=-1)\n",
    "        att = torch.bmm(dist, v)\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in, dim_k, dim_v = 4, 5, 6\n",
    "batch, n, dim_in = 2, 3, 4\n",
    "_norm_fact = 1 / sqrt(dim_k)\n",
    "x = torch.randn(batch, n, dim_in)\n",
    "linear_q = nn.Linear(dim_in, dim_k, bias=False)\n",
    "linear_k = nn.Linear(dim_in, dim_k, bias=False)\n",
    "linear_v = nn.Linear(dim_in, dim_v, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1170, -0.0199, -0.4891,  0.1782, -0.0399,  0.1063],\n",
       "         [-0.1161, -0.0436, -0.4582,  0.1723, -0.0123,  0.0957],\n",
       "         [-0.1250, -0.0564, -0.4577,  0.1804,  0.0045,  0.0994]],\n",
       "\n",
       "        [[ 0.4761, -0.1290, -0.3365, -0.3308, -0.0623, -0.3062],\n",
       "         [ 0.4869, -0.0480, -0.2797, -0.3377, -0.0965, -0.2591],\n",
       "         [ 0.5008,  0.0143, -0.2248, -0.3498, -0.1231, -0.2280]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = linear_q(x)\n",
    "k = linear_k(x)\n",
    "v = linear_v(x)\n",
    "dist = torch.bmm(q, k.transpose(1, 2)) * _norm_fact\n",
    "dist = torch.softmax(dist, dim=-1)\n",
    "att = torch.bmm(dist, v)\n",
    "att\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多头自注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_k, dim_v, num_heads=8):\n",
    "        super().__init__()\n",
    "        assert dim_k % num_heads == 0 and dim_v % num_heads == 0,\\\n",
    "            \"dim_k and dim_v must be multiple of num_heads\"\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_q = nn.Linear(dim_in, dim_k, bias=False)\n",
    "        self.linear_k = nn.Linear(dim_in, dim_k, bias=False)\n",
    "        self.linear_v = nn.Linear(dim_in, dim_v, bias=False)\n",
    "        self._norm_fact = 1 / sqrt(dim_k // num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (batch, n, dim_in)\n",
    "        batch, n, dim_in = x.shape\n",
    "        assert dim_in == self.dim_in\n",
    "\n",
    "        nh = self.num_heads\n",
    "        dk = self.dim_k // nh # dim_k of each head\n",
    "        dv = self.dim_v // nh # dim_v of each head\n",
    "\n",
    "        q = self.linear_q(x).reshape(batch, n, nh, dk).transpose(1, 2) # (batch, nh, n, dk)\n",
    "        k = self.linear_k(x).reshape(batch, n, nh, dk).transpose(1, 2) # (batch, nh, n, dk)\n",
    "        v = self.linear_v(x).reshape(batch, n, nh, dv).transpose(1, 2) # (batch, nh, n, dv)\n",
    "\n",
    "        dist = torch.matmul(q, k.transpose(2, 3)) * self._norm_fact # (batch, nh, n, n)\n",
    "        dist = torch.softmax(dist, dim=-1)\n",
    "\n",
    "        att = torch.matmul(dist, v)\n",
    "        att = att.transpose(1, 2).reshape(batch, n, self.dim_v) # batch, n, dim_v\n",
    "\n",
    "        return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in, dim_k, dim_v, nh = 4, 6, 8, 2\n",
    "batch, n, dim_in = 2, 3, 4\n",
    "_norm_fact = 1 / sqrt(dim_k // nh)\n",
    "dk, dv = dim_k // nh, dim_v // nh\n",
    "x = torch.randn(batch, n, dim_in)\n",
    "linear_q = nn.Linear(dim_in, dim_k, bias=False)\n",
    "linear_k = nn.Linear(dim_in, dim_k, bias=False)\n",
    "linear_v = nn.Linear(dim_in, dim_v, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = linear_q(x).reshape(batch, n, nh, dk).transpose(1, 2)\n",
    "k = linear_k(x).reshape(batch, n, nh, dk).transpose(1, 2)\n",
    "v = linear_v(x).reshape(batch, n, nh, dv).transpose(1, 2)\n",
    "dist = torch.matmul(q, k.transpose(2, 3)) * _norm_fact\n",
    "dist = torch.softmax(dist, dim=-1)\n",
    "att = torch.matmul(dist, v)\n",
    "att = att.transpose(1, 2).reshape(batch, n, dim_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7564, -0.4551, -0.2517, -0.0053, -0.3878,  0.0794,  0.5259,\n",
       "           0.4184],\n",
       "         [ 0.6843, -0.4034, -0.2421, -0.0107, -0.3764,  0.1330,  0.4599,\n",
       "           0.4155],\n",
       "         [ 0.9905, -0.6450, -0.4833, -0.2000, -0.4009,  0.1281,  0.5068,\n",
       "           0.4333]],\n",
       "\n",
       "        [[ 0.6283, -0.3416, -0.0734,  0.2018, -0.0785, -0.2899,  0.3741,\n",
       "           0.2465],\n",
       "         [ 0.6967, -0.3385,  0.0030,  0.3277, -0.0423, -0.1433,  0.3717,\n",
       "           0.0984],\n",
       "         [ 0.6710, -0.3422, -0.0310,  0.2727, -0.0586, -0.2714,  0.3703,\n",
       "           0.2078]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHA = MultiHeadSelfAttention(dim_in, dim_k, dim_v, num_heads=nh)\n",
    "MHA.forward(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43e4d72c8ad61331984bc36f87d846bb73452d1c2c0cb0d0679bd7cf00b5eb21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
